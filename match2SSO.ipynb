{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# match2SSO\n",
    "* Running instructions are given at the start of function run_match2SSO\n",
    "* Originally written as a Jupyter Notebook using Python 3.8.10\n",
    "* Compatible with MeerLICHT / BlackGEM observations\n",
    "* Compatible with BlackBOX / ZOGY version 1.0.0 and up (image processing pipeline of MeerLICHT & BlackGEM)\n",
    "* Compatible with Unix file systems and Google cloud storage systems\n",
    "\n",
    "Output (SSO) catalogue columns and header keywords are listed here:\n",
    "https://www.overleaf.com/read/zrhqwcbkfqns\n",
    "\n",
    "<i>match2SSO</i> makes grateful use of the <i>lunar</i> and \n",
    "<i>jpl_eph</i> repositories that were written by Bill Gray under \n",
    "Project Pluto. The core of <i>match2SSO</i> is <i>astcheck</i>: a C++\n",
    "script in the <i>lunar</i> repository that matches detections to known\n",
    "solar system objects. More information on <i>astcheck</i> can be found\n",
    "at: https://www.projectpluto.com/astcheck.htm\n",
    "\n",
    "<b>Dependencies on scripts and files</b>\n",
    "* <i>lunar</i> package (https://github.com/Bill-Gray/lunar)\n",
    "* <i>jpl_eph</i> package (https://github.com/Bill-Gray/jpl_eph)\n",
    "* JPL DE ephemeris file (ftp://ssd.jpl.nasa.gov/pub/eph/planets/Linux/)\n",
    "* MPC's Observatory codes list\n",
    "  (https://www.minorplanetcenter.net/iau/lists/ObsCodes.html)\n",
    "\n",
    "In addition, <i>match2SSO</i> uses MPCORB.DAT (MPC's asteroid database) and\n",
    "COMET.ELEMENTS (JPL's comet database), but these are downloaded when\n",
    "running the script and hence do not need to be pre-downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python packages and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__version__ = \"1.2.0\"\n",
    "__author__ = \"Danielle Pieterse\"\n",
    "KEYWORDS_VERSION = \"1.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import resource\n",
    "import platform\n",
    "import shutil\n",
    "import subprocess\n",
    "from math import sqrt\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "import psutil\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Third party imports\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table, Column\n",
    "from astropy.time import Time\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "\n",
    "# In case a Google cloud storage system is being used\n",
    "from google.cloud import storage\n",
    "\n",
    "# Local imports\n",
    "import set_match2SSO as settingsFile # Load match2SSO settings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set log format and global logging constants\n",
    "LOG_FORMAT = (\"%(asctime)s.%(msecs)03d [%(levelname)s, %(process)s] \"\n",
    "              \"%(message)s [%(funcName)s, line %(lineno)d]\")\n",
    "DATE_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
    "logging.basicConfig(level=\"INFO\", format=LOG_FORMAT, datefmt=DATE_FORMAT)\n",
    "LOG_FORMATTER = logging.Formatter(LOG_FORMAT, DATE_FORMAT)\n",
    "logging.Formatter.converter = time.gmtime # Convert time in logger to UTC\n",
    "LOG = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global constants\n",
    "\n",
    "# Relevant data columns\n",
    "NUMBER_COLUMN = settingsFile.colNumber\n",
    "RA_COLUMN = settingsFile.colRA\n",
    "DEC_COLUMN = settingsFile.colDec\n",
    "MAG_COLUMN = settingsFile.colMag\n",
    "SNR_COLUMN = settingsFile.colSNR\n",
    "\n",
    "# Relevant header keywords of detection catalogue\n",
    "DUMMY_KEYWORD = settingsFile.keyDummy\n",
    "DATE_KEYWORD = settingsFile.keyDate\n",
    "MPC_CODE_KEYWORD = settingsFile.keyMPCcode\n",
    "CENTRAL_RA_KEYWORD = settingsFile.keyRACentre\n",
    "CENTRAL_DEC_KEYWORD = settingsFile.keyDecCentre\n",
    "LIMMAG_KEYWORD = settingsFile.keyLimmag\n",
    "\n",
    "# Load switches from settings file\n",
    "INCLUDE_COMETS = bool(settingsFile.include_comets)\n",
    "KEEP_TMP = bool(settingsFile.keep_tmp)\n",
    "TIME_FUNCTIONS = bool(settingsFile.time_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main routines to call match2SSO functions in the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_match2SSO(tel, mode, cat2process, date2process, list2process,\n",
    "                  logname, redownload, overwrite):\n",
    "    \"\"\"\n",
    "    Run match2SSO on the input catalogue(s)/date. match2SSO can be run in\n",
    "    different mode / date2process / cat2process / list2process combinations.\n",
    "    Allowed combinations are: (if not mentioned, the variable is None)\n",
    "      * Day mode\n",
    "      * Day mode + date2process\n",
    "      * Night mode + cat2process\n",
    "      * Historic mode + cat2process\n",
    "      * Historic mode + date2process\n",
    "      * Historic mode + list2process\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tel: string\n",
    "        Abbreviated telescope name. Can be either ML1, BG2, BG3 or BG4.\n",
    "    mode: string\n",
    "        Mode in which match2SSO is run. This can be 'day', 'night' or \n",
    "        'historic'.\n",
    "    cat2process: string\n",
    "        Path to and name of the transient catalogue that is to be processed.\n",
    "    date2process: string\n",
    "        Formatted as yyyymmdd, yyyy-mm-dd, yyyy/mm/dd or yyyy.mm.dd. When used\n",
    "        with day mode: date for which the known objects database needs to be\n",
    "        prepared. When used with historic mode: date for which all (light)\n",
    "        transient catalogues need to be processed.\n",
    "    list2process: string\n",
    "        Path to and name of the text file that contains the paths to and names\n",
    "        of transient catalogues (one per line) that need to be processed.\n",
    "    logname: string\n",
    "        Path to and name of the log file in which comments about the run are\n",
    "        stored.\n",
    "    redownload: boolean\n",
    "        Boolean to indicate whether the asteroid (and comet) databases should be\n",
    "        redownloaded. Alternatively the most recently downloaded databases will\n",
    "        be used. Parameter is not relevant for the night mode.\n",
    "    overwrite: boolean\n",
    "        Boolean to indicate whether files will be remade and overwritten.\n",
    "        Alternatively existing files will be  used.\n",
    "    \"\"\"\n",
    "    t_glob = time.time()\n",
    "    \n",
    "    # Format input parameters\n",
    "    mode = mode.lower()\n",
    "    if date2process:\n",
    "        date2process = date2process.replace(\".\", \"\").replace(\n",
    "            \"/\", \"\").replace(\"-\", \"\")\n",
    "    \n",
    "    # Set global variables\n",
    "    global FILE_JPLEPH\n",
    "    FILE_JPLEPH = get_par(settingsFile.JPL_ephemerisFile, tel)\n",
    "    global OVERWRITE_FILES\n",
    "    OVERWRITE_FILES = overwrite\n",
    "    \n",
    "    # Perform checks on input parameter combinations and setting file\n",
    "    # parameters\n",
    "    if not check_input_parameters(mode, cat2process, date2process,\n",
    "                                  list2process):\n",
    "        return\n",
    "    if not check_settings():\n",
    "        return\n",
    "    folders = load_and_check_folders(tel)\n",
    "    if not folders: # Empty tuple\n",
    "        return\n",
    "    input_folder, tmp_folder, log_folder, report_folder = folders\n",
    "    \n",
    "    # Logging\n",
    "    setup_logfile(logname, log_folder)\n",
    "    LOG.info(\"Mode: {}\".format(mode))\n",
    "    #mem_use(label=\"at start of run_match2SSO\")\n",
    "    \n",
    "    # Get local noon corresponding to the night start in case date2process or\n",
    "    # cat2process are specified.\n",
    "    night_start = None\n",
    "    local_timezone = timezone(get_par(settingsFile.timeZoneTelescope, tel))\n",
    "    if cat2process:\n",
    "        night_start = get_night_start_from_date(cat2process, tel)\n",
    "    elif date2process:\n",
    "        night_start = local_timezone.localize(datetime.strptime(\" \".join([\n",
    "            date2process, \"120000\"]), \"%Y%m%d %H%M%S\"))\n",
    "    \n",
    "    # Run match2SSO\n",
    "    if mode == \"day\":\n",
    "        day_mode(night_start, tel, tmp_folder, redownload)\n",
    "        \n",
    "    elif mode == \"night\":\n",
    "        night_mode(cat2process, night_start, tel, tmp_folder, report_folder)\n",
    "    \n",
    "    elif mode == \"historic\" or \"hist\":\n",
    "        hist_mode(cat2process, date2process, list2process, night_start, tel,\n",
    "                  input_folder, tmp_folder, report_folder)\n",
    "    \n",
    "    LOG.info(\"Finished running match2SSO.\")\n",
    "    log_timing_memory(t_glob, label=\"run_match2SSO\")\n",
    "    logging.shutdown()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_mode(night_start, tel, tmp_folder, redownload_db):\n",
    "    \n",
    "    \"\"\"\n",
    "    Run match2SSO in day mode to prepare for the night mode. The day mode\n",
    "    creates the known objects database & CHK files for the upcoming night, or\n",
    "    - in case [date] is specified - for the specified night.\n",
    "      0) Creates a run directory in preparation of the nightly processing.\n",
    "      1) Downloads asteroid and comet databases to the database folder.\n",
    "      2) Integrates the asteroid database to midnight of the observation night.\n",
    "      3) Combines the comet and integrated asteroid databases into a SOF-\n",
    "         formatted known objects database.\n",
    "      4) Creates symbolic links to the used databases and the observatory codes\n",
    "         list in the run directory.\n",
    "      5) Runs astcheck on a fake detection in order to create the CHK files\n",
    "         that astcheck will need for faster processing when running on\n",
    "         observations.\n",
    "      6) Removes the fake detection in- & output, excluding the CHK files.\n",
    "    Products of steps 1-2 are saved to the database folder, those of steps 3-5\n",
    "    to the run directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    night_start: datetime object (incl timezone) or None\n",
    "        Noon corresponding to the start of the observation night.\n",
    "    tel: string\n",
    "        Abbreviated telescope name. Can be either ML1, BG2, BG3 or BG4.\n",
    "    tmp_folder: string\n",
    "        Name of the folder where the known objects databases will be downloaded\n",
    "        to and in which the run directory will be placed.\n",
    "    redownload_db: boolean\n",
    "        Boolean to indicate whether the asteroid (and comet) databases should be\n",
    "        redownloaded. Alternatively the most recently downloaded databases will\n",
    "        be used.\n",
    "    \"\"\"\n",
    "    \n",
    "    LOG.info(\"Running the day mode.\")\n",
    "    \n",
    "    # Use the upcoming local night if no observation night is specified\n",
    "    if not night_start:\n",
    "        local_timezone = timezone(get_par(settingsFile.timeZoneTelescope, tel))\n",
    "        night_start = (datetime.now(local_timezone)).strftime(\"%Y%m%d 120000\")\n",
    "        night_start = local_timezone.localize(datetime.strptime(\n",
    "            night_start, \"%Y%m%d %H%M%S\"))\n",
    "    \n",
    "    # Delete the temporary products made in the night mode of the previous\n",
    "    # night.\n",
    "    #if not KEEP_TMP:\n",
    "    #    night_previous = (night_start - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "    #    rundir_previous = \"{}{}/\".format(tmp_folder, night_previous)\n",
    "    #    \n",
    "    #    if isfile(\"{}MPCORB.DAT\".format(rundir_previous)):\n",
    "    #        asteroid_database = os.readlink(\"{}MPCORB.DAT\"\n",
    "    #                                        .format(rundir_previous))\n",
    "    #        if \"epoch\" in asteroid_database:\n",
    "    #            os.remove(asteroid_database)\n",
    "    #            LOG.info(\"Removed {}\".format(asteroid_database))\n",
    "    #    remove_tmp_folder(rundir_previous)\n",
    "    \n",
    "    # Create a run directory corresponding to the observation night\n",
    "    rundir = (\"{}{}/\".format(tmp_folder, night_start.strftime(\"%Y%m%d\")))\n",
    "    LOG.info(\"Run directory: {}\".format(rundir))\n",
    "    if not os.path.isdir(rundir):\n",
    "        os.makedirs(rundir)\n",
    "    \n",
    "    # Create symbolic link to the observatory codes list\n",
    "    if not isfile(\"{}ObsCodes.html\".format(rundir)):\n",
    "        LOG.info(\"Creating symbolic link to ObsCodes.html\")\n",
    "        os.symlink(settingsFile.obsCodesFile,\n",
    "                   \"{}ObsCodes.html\".format(rundir))\n",
    "    \n",
    "    # Download and integrate known object databases\n",
    "    midnight = night_start + timedelta(days=0.5)\n",
    "    create_known_objects_database(midnight, rundir, tmp_folder, redownload_db)\n",
    "    \n",
    "    # Create CHK files that astcheck needs in advance, to allow parallelisation\n",
    "    # in the night mode\n",
    "    create_chk_files(night_start, \"night_start\", rundir)\n",
    "    create_chk_files(night_start + timedelta(days=1), \"night_end\", rundir)\n",
    "    \n",
    "    # Check for known object database products\n",
    "    if not find_database_products(rundir):\n",
    "        LOG.error(\"No database products found...\")\n",
    "        return\n",
    "        \n",
    "    LOG.info(\"Day mode finished.\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def night_mode(cat_name, night_start, tel, tmp_folder, report_folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Run match2SSO on a single transient catalogue. The day mode should have been\n",
    "    run once before the night mode. This allows the night mode to run in\n",
    "    parallel on multiple transient catalogues of the same night, as steps that\n",
    "    cannot be parallelised (making known objects database and CHK files) have\n",
    "    already been executed in the day mode. The night mode:\n",
    "      1) Converts the transient catalogue into an MPC-formatted text file.\n",
    "      2) Runs astcheck on that file, to find matches between the transient\n",
    "         detections and known solar system objects.\n",
    "      3) Makes an SSO catalogue containing the matches.\n",
    "      4) Makes an MPC report of the matches.\n",
    "    Running the night mode in parallel on multiple catalogues can be done by\n",
    "    calling match2SSO (with --mode night) multiple times in parallel.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cat_name: string\n",
    "        Path to and name of the transient catalogue that is to be processed.\n",
    "    night_start: datetime object (with timezone)\n",
    "        Noon corresponding to the start of the observation night.\n",
    "    tel: string\n",
    "        Abbreviated telescope name. Can be either ML1, BG2, BG3 or BG4.\n",
    "    tmp_folder: string\n",
    "        Name of the folder that contains the known objects databases and the run\n",
    "        directory.\n",
    "    report_folder: string\n",
    "        Name of the folder in which the MPC reports will be stored.\n",
    "    \"\"\"\n",
    "    \n",
    "    LOG.info(\"Running the night mode on transient catalogue: \\n{}\"\n",
    "             .format(cat_name))\n",
    "    \n",
    "    # Get name of run directory\n",
    "    rundir = \"{}{}/\".format(tmp_folder, night_start.strftime(\"%Y%m%d\"))\n",
    "    LOG.info(\"Run directory: {}\".format(rundir))\n",
    "    \n",
    "    # Check for known object database products. Stop processing if it doesn't\n",
    "    # exist.\n",
    "    if not find_database_products(rundir):\n",
    "        logging.shutdown()\n",
    "        return\n",
    "    \n",
    "    # Check for CHK files\n",
    "    night_start_utc = get_night_start_from_date(cat_name, tel, \"utc\")\n",
    "    night_end_utc = night_start_utc + timedelta(days=1)\n",
    "    night_start_utc = night_start_utc.strftime(\"%Y%m%d\")\n",
    "    night_end_utc = night_end_utc.strftime(\"%Y%m%d\")\n",
    "    if not isfile(\"{}{}.chk\".format(rundir, night_start_utc)):\n",
    "        LOG.critical(\"Missing {}.chk!\".format(night_start_utc))\n",
    "        logging.shutdown()\n",
    "        return\n",
    "    if not isfile(\"{}{}.chk\".format(rundir, night_end_utc)):\n",
    "        LOG.critical(\"Missing {}.chk!\".format(night_end_utc))\n",
    "        logging.shutdown()\n",
    "        return\n",
    "    del night_start_utc\n",
    "    del night_end_utc\n",
    "    \n",
    "    # Check for observatory codes list. Stop processing if it doesn't exist\n",
    "    if not isfile(\"{}ObsCodes.html\".format(rundir)):\n",
    "        LOG.critical(\"{}ObsCodes.html doesn't exist.\".format(rundir))\n",
    "        logging.shutdown()\n",
    "        return\n",
    "    \n",
    "    _ = match_single_catalogue(cat_name, rundir, tmp_folder, report_folder,\n",
    "                               night_start, make_kod=False, redownload_db=False)\n",
    "    \n",
    "    # Beware that the run directory created for the processing of the\n",
    "    # catalogue is not removed. This is the case because a single parallel\n",
    "    # process does not know about the rest. A cleaning function can be run\n",
    "    # in the day mode.\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_mode(cat_name, date, catlist, night_start, tel, input_folder,\n",
    "              tmp_folder, report_folder, redownload_db):\n",
    "    \n",
    "    \"\"\"\n",
    "    The historic mode does the entire processing of match2SSO, including the\n",
    "    preparation of the known objects databases. The historic mode can be run on\n",
    "    a single transient catalogue, an entire night of observations or a list of\n",
    "    transient catalogues (possibly spanning multiple nights).\n",
    "    \n",
    "    For the first catalogue that is processed of each observation night, the\n",
    "    code:\n",
    "      0) Creates a run directory in preparation of the processing.\n",
    "      1) Downloads asteroid and comet databases to the database folder.\n",
    "      2) Integrates the asteroid database to midnight of the observation night.\n",
    "      3) Combines the comet and integrated asteroid databases into a SOF-\n",
    "         formatted known objects database.\n",
    "      4) Creates symbolic links to the used databases and the observatory codes\n",
    "         list in the run directory.\n",
    "    The asteroid and comet databases used for this are only downloaded once per\n",
    "    historic mode run (and only if [redownload_db] is True). For the\n",
    "    remaining files (or if [redownload_db] is False), the most recently\n",
    "    downloaded versions are used.\n",
    "    \n",
    "    For all transient catalogues, the code then:\n",
    "      5) Converts the transient catalogue into an MPC-formatted text file.\n",
    "      6) Runs astcheck on that file, to find matches between the transient\n",
    "         detections and known solar system objects.\n",
    "      7) Makes an SSO catalogue containing the matches.\n",
    "      8) Makes an MPC report of the matches.\n",
    "    \n",
    "    The historic mode can only be parallelised if there is no overlap in \n",
    "    observing nights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cat_name: string or None\n",
    "        Path to and name of the transient catalogue that is to be processed.\n",
    "    date: string or None\n",
    "        Date for which the known objects database needs to be prepared.\n",
    "        Formatted as yyyymmdd.\n",
    "    catlist: string or None\n",
    "        Path to and name of the text file that contains the paths to and names\n",
    "        of transient catalogues (one per line) that need to be processed.\n",
    "    night_start: datetime object (with timezone) or None\n",
    "        Noon corresponding to the start of the observation night.\n",
    "    tel: string\n",
    "        Abbreviated telescope name. Can be either ML1, BG2, BG3 or BG4.\n",
    "    input_folder: string\n",
    "        Folder which contains the yyyy/mm/dd/ folders in which the transient\n",
    "        catalogues are stored.\n",
    "    tmp_folder: string\n",
    "        Name of the folder that contains the known objects databases and the run\n",
    "        directory.\n",
    "    report_folder: string\n",
    "        Name of the folder in which the MPC reports will be stored.\n",
    "    redownload_db: boolean\n",
    "        Boolean to indicate whether the asteroid (and comet) databases should be\n",
    "        redownloaded. Alternatively the most recently downloaded databases will\n",
    "        be used.\n",
    "    Beware that exactly two of the variables (cat_name, date, catlist) need to\n",
    "    be None!\n",
    "    \"\"\"\n",
    "    \n",
    "    if catlist:\n",
    "        LOG.info(\"Running historic mode on catalogue list: \\n{}\"\n",
    "                 .format(catlist))\n",
    "        \n",
    "        # Open catalogue list\n",
    "        with open(catlist, \"r\") as catalogue_list:\n",
    "            catalogues2process = [name.strip() for name in catalogue_list \\\n",
    "                                  if name[0] != \"#\"]\n",
    "        \n",
    "        # Order by observation date (noon that equals the start of the\n",
    "        # observation day)\n",
    "        noons = [get_night_start_from_date(catname, tel).strftime(\n",
    "            \"%Y%m%d %H%M%S\") for catname in catalogues2process]\n",
    "        \n",
    "        # Process files per night\n",
    "        LOG.info(\"Catalogue list spans {} nights\".format(len(np.unique(noons))))\n",
    "        first_night = True\n",
    "        for noon in np.unique(noons):\n",
    "            LOG.info(\"Processing night that starts at {}\".format(noon))\n",
    "            night_index = np.where(np.array(noons) == noon)[0]\n",
    "            catalogues2process_1night = np.array(\n",
    "                catalogues2process)[night_index]\n",
    "            \n",
    "            local_timezone = timezone(get_par(settingsFile.timeZoneTelescope,\n",
    "                                              tel))\n",
    "            noon = local_timezone.localize(datetime.strptime(noon,\n",
    "                                                             \"%Y%m%d %H%M%S\"))\n",
    "            if first_night:\n",
    "                match_catalogues_single_night(\n",
    "                    catalogues2process_1night, noon, redownload_db, tmp_folder,\n",
    "                    report_folder)\n",
    "            else:\n",
    "                match_catalogues_single_night(catalogues2process_1night, noon,\n",
    "                                              False, tmp_folder, report_folder)\n",
    "            first_night = False\n",
    "        return\n",
    "    \n",
    "    if cat_name:\n",
    "        LOG.info(\"Running historic mode on transient catalogue: \\n{}\"\n",
    "                 .format(cat_name))\n",
    "        catalogues2process = (cat_name)\n",
    "    \n",
    "    elif date:\n",
    "        LOG.info(\"Running historic mode on night {}\".format(date))\n",
    "        catalogues2process = get_transient_filenames(\n",
    "            input_folder, night_start,\n",
    "            night_start+timedelta(days=1)-timedelta(minutes=1), tel)\n",
    "        \n",
    "        if not catalogues2process:\n",
    "            LOG.critical(\"No (light) transient catalogues exist for night {}\"\n",
    "                         .format(date))\n",
    "            return\n",
    "    \n",
    "    match_catalogues_single_night(catalogues2process, night_start,\n",
    "                                  redownload_db, tmp_folder, report_folder)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_catalogues_single_night(catalogues_single_night, night_start,\n",
    "                                  redownload_db, tmp_folder, report_folder):\n",
    "    \"\"\"\n",
    "    Wrapper function that calls the match_single_catalogue function for\n",
    "    each catalogue in a list that contains catalogues corresponding to\n",
    "    observations taken on the same night.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    catalogues_single_night: list of strings\n",
    "        Names of the catalogues that will need to be processed by match2SSO.\n",
    "        The catalogues are expected to correspond to observations taken on the\n",
    "        same night.\n",
    "    night_start: datetime object\n",
    "        Start of the observing night.\n",
    "    redownload_db: boolean\n",
    "        Boolean indicating whether the known object databases need to be\n",
    "        redownloaded, or alternatively if existing downloaded versions of the\n",
    "        databases can be used for the matching.\n",
    "    tmp_folder: string\n",
    "        Name of the folder containing the known objects databases, or where\n",
    "        these databases can be downloaded.\n",
    "    report_folder: string\n",
    "        Name of the folder in which the MPC reports will be stored.\n",
    "    \"\"\"\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    LOG.info(\"{} catalogues to process for the night around {}.\".format(\n",
    "        len(catalogues_single_night),\n",
    "        night_start.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    \n",
    "    #Create run directory\n",
    "    rundir = \"{}{}/\".format(tmp_folder, night_start.strftime(\"%Y%m%d\"))\n",
    "    LOG.info(\"Run directory: {}\".format(rundir))\n",
    "    if not os.path.isdir(rundir):\n",
    "        os.makedirs(rundir)\n",
    "    \n",
    "    #Run matching per catalogue\n",
    "    make_kod = True\n",
    "    for cat_name in catalogues_single_night:\n",
    "        made_kod = match_single_catalogue(\n",
    "            cat_name, rundir, tmp_folder, report_folder, night_start,\n",
    "            make_kod, redownload_db)\n",
    "        if made_kod:\n",
    "            make_kod = False #Only make known objects database once\n",
    "    \n",
    "    #Remove the run directory after processing the last catalogue of the night\n",
    "    if not KEEP_TMP:\n",
    "        #Remove integrated database made for this night\n",
    "        if isfile(\"{}MPCORB.DAT\".format(rundir)):\n",
    "            asteroid_database = os.readlink(\"{}MPCORB.DAT\".format(rundir))\n",
    "            if \"epoch\" in asteroid_database:\n",
    "                os.remove(asteroid_database)\n",
    "                LOG.info(\"Removed {}\".format(asteroid_database))\n",
    "        \n",
    "        #Remove temporary folder made for this night\n",
    "        remove_tmp_folder(rundir)\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"match_catalogues_single_night\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_single_catalogue(cat_name, rundir, tmp_folder, report_folder,\n",
    "                           night_start, make_kod, redownload_db):\n",
    "    \"\"\"\n",
    "    Run matching routine on a single transient catalogue. Optionally, a new\n",
    "    known objects database is created where the reference epoch corresponds to\n",
    "    midnight on the observation night. The detections in the transient \n",
    "    catalogue are then matched to the positions of the solar system bodies in\n",
    "    the known objects catalogue. Matches are saved to an SSO catalogue.\n",
    "    Function returns a boolean indicating whether a known objects catalogue was\n",
    "    (successfully) made.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cat_name: string\n",
    "        Name of the transient catalogue of which the detections are to be\n",
    "        matched to known solar system objects.\n",
    "    rundir: string\n",
    "        Directory corresponding to observation night where all temporary\n",
    "        products will be stored during the running of match2SSO.\n",
    "    tmp_folder: string\n",
    "        Name of the folder containing the known objects databases, or where\n",
    "        these databases can be downloaded.\n",
    "    report_folder: string\n",
    "        Name of the folder in which the MPC reports will be stored.\n",
    "    night_start: datetime object, including time zone\n",
    "        Noon corresponding to the start of the local night during which the\n",
    "        observation corresponding to the transient catalogue was made.\n",
    "    make_kod: boolean\n",
    "        Boolean indicating whether a new known objects database needs to be\n",
    "        made. In night mode, this should be False. In historic mode, this is\n",
    "        only True for the first catalogue that is processed per observation\n",
    "        night, as the database will need to be integrated to that observation\n",
    "        night.\n",
    "    redownload_db: boolean\n",
    "        Only used when make_kod is True. This boolean indicates whether the\n",
    "        asteroid and comet databases will need to be redownloaded before making\n",
    "        the known objects database. Alternatively, the most recent, previously\n",
    "        downloaded version of the databases are used.\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of match_single_catalogue\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    LOG.info(\"Running match2SSO on {}\".format(cat_name))\n",
    "    \n",
    "    # Check if input catalogue exists and is not flagged red\n",
    "    is_existing, is_dummy, cat_name = check_input_catalogue(cat_name)\n",
    "    if not is_existing:\n",
    "        return made_kod\n",
    "    del is_existing\n",
    "    \n",
    "    # File names\n",
    "    mpcformat_file = \"{}{}\".format(rundir, os.path.basename(cat_name).replace(\n",
    "        \"_light.fits\", \".fits\").replace(\".fits\", \"_MPCformat.txt\"))\n",
    "    sso_cat = cat_name.replace(\"_light.fits\", \".fits\").replace(\".fits\",\n",
    "                                                               \"_sso.fits\")\n",
    "    predict_cat = sso_cat.replace(\"_trans\", \"\").replace(\"_sso.fits\",\n",
    "                                                        \"_sso_predict.fits\")\n",
    "    reportname = \"{}{}.txt\".format(\n",
    "        report_folder, os.path.basename(sso_cat).replace(\".fits\", \"_report\"))\n",
    "    \n",
    "    # Keep track of whether known object database has been made\n",
    "    made_kod = False\n",
    "    \n",
    "    # Create predictions and SSO catalogues in case of a dummy (empty) detection\n",
    "    # catalogue\n",
    "    if is_dummy:\n",
    "        _ = predictions(None, rundir, predict_cat, \"\")\n",
    "        create_sso_catalogue(None, rundir, sso_cat, 0)\n",
    "        return made_kod\n",
    "    \n",
    "    # Check if output catalogues exist, in which case the known objects database\n",
    "    # will not need to be made\n",
    "    if isfile(predict_cat) and isfile(sso_cat) and not OVERWRITE_FILES:\n",
    "        LOG.info(\"Prediction and SSO catalogues exist and won't be remade.\\n\")\n",
    "        \n",
    "        # Check for any version of an MPC report for this observation\n",
    "        reportnames = list_files(reportname.replace(\".txt\", \"\"), end_str=\".txt\")\n",
    "        if reportnames:\n",
    "            LOG.info(\"There is at least one version of an MPC report for this \"\n",
    "                     \"observation. No new one will be made.\\n\")\n",
    "            return made_kod\n",
    "        \n",
    "        # Check if MPC-formatted file that the create_MPC_report function needs\n",
    "        # exists and get the MPC code. If it doesn't exist yet / anymore, remake\n",
    "        # this file first.\n",
    "        mpc_code, create_dummy = convert_fits2mpc(cat_name, mpcformat_file)\n",
    "        if mpc_code is None:\n",
    "            LOG.critical(\"Unknown MPC code - MPC report will not be made.\")\n",
    "            return made_kod\n",
    "        \n",
    "        if create_dummy:\n",
    "            # No MPC report will need to be made, as the SSO catalogue is a\n",
    "            # dummy (empty) catalogue.\n",
    "            return made_kod\n",
    "        \n",
    "        # Create an report that can be used to submit the detections that were\n",
    "        # matched to known solar system objects to the MPC\n",
    "        create_MPC_report(sso_cat, mpcformat_file, reportname, rundir, mpc_code)\n",
    "        \n",
    "        if not KEEP_TMP:\n",
    "            os.remove(mpcformat_file)\n",
    "            LOG.info(\"Removed {}\".format(mpcformat_file))\n",
    "        return made_kod\n",
    "    \n",
    "    # Convert the transient catalogue to an MPC-formatted text file\n",
    "    mpc_code, create_dummy = convert_fits2mpc(cat_name, mpcformat_file)\n",
    "    if mpc_code is None:\n",
    "        LOG.critical(\"Matching cannot be done because of unknown MPC code.\")\n",
    "        LOG.info(\"Creating dummy catalogues.\")\n",
    "        _ = predictions(None, rundir, predict_cat, \"\")\n",
    "        create_sso_catalogue(None, rundir, sso_cat, 0)\n",
    "        return made_kod\n",
    "    \n",
    "    if create_dummy:\n",
    "        _ = predictions(None, rundir, predict_cat, \"\")\n",
    "        create_sso_catalogue(None, rundir, sso_cat, 0)\n",
    "        return made_kod\n",
    "    \n",
    "    # If make_kod, create a new known objects database with a reference epoch\n",
    "    # corresponding to midnight of the observation night. Also create a\n",
    "    # symbolic link to the MPC observatory codes list.\n",
    "    if make_kod:\n",
    "        midnight = night_start + timedelta(days=0.5)\n",
    "        create_known_objects_database(midnight, rundir, tmp_folder,\n",
    "                                      redownload_db)\n",
    "        made_kod = True\n",
    "        \n",
    "        # Make symbolic link to observatory codes list if it doesn't exist yet\n",
    "        if not isfile(\"{}ObsCodes.html\".format(rundir)):\n",
    "            LOG.info(\"Creating symbolic link to ObsCodes.html\")\n",
    "            os.symlink(settingsFile.obsCodesFile,\n",
    "                       \"{}ObsCodes.html\".format(rundir))\n",
    "    \n",
    "    # Make predictions catalogue\n",
    "    N_sso = predictions(cat_name, rundir, predict_cat, mpc_code)\n",
    "    \n",
    "    # Check if the SSO catalogue and MPC report already exist\n",
    "    reportnames = list_files(reportname.replace(\".txt\", \"\"), end_str=\".txt\")\n",
    "    if reportnames and isfile(sso_cat) and not OVERWRITE_FILES:\n",
    "        LOG.info(\"SSO catalogue and MPC report exist and won't be remade.\\n\")\n",
    "        if not KEEP_TMP:\n",
    "            os.remove(mpcformat_file)\n",
    "            LOG.info(\"Removed {}\".format(mpcformat_file))\n",
    "        return made_kod\n",
    "    \n",
    "    # Run astcheck on the MPC-formatted transient file\n",
    "    astcheck_file = mpcformat_file.replace(\"_MPCformat.txt\",\n",
    "                                           \"_astcheckMatches.txt\")\n",
    "    run_astcheck(mpcformat_file, rundir, astcheck_file)\n",
    "    \n",
    "    # Save matches found by astcheck to an SSO catalogue\n",
    "    create_sso_catalogue(astcheck_file, rundir, sso_cat, N_sso)\n",
    "    \n",
    "    # Create a report that can be used to submit the detections that were\n",
    "    # matched to known solar system objects to the MPC\n",
    "    create_MPC_report(sso_cat, mpcformat_file, reportname, rundir, mpc_code)\n",
    "    \n",
    "    # Delete temporary files corresponding to the processed transient\n",
    "    # catalogue. The other temporary files (the CHK files, the SOF file and the\n",
    "    # symbolic links) in the run directory are not (yet) removed, as they\n",
    "    # might be needed for processing of other data from the same night.\n",
    "    if not KEEP_TMP:\n",
    "        os.remove(mpcformat_file)\n",
    "        LOG.info(\"Removed {}\".format(mpcformat_file))\n",
    "        os.remove(astcheck_file)\n",
    "        LOG.info(\"Removed {}\".format(astcheck_file))\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"match_single_catalogue\")\n",
    "    \n",
    "    return made_kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core routines in match2SSO\n",
    "* Create known objects catalogue (with asteroids with a max. orbital\n",
    "  uncertainty)\n",
    "* Create catalogue with predictions of asteroids in the FOV\n",
    "* Convert transient catalogue to MPC format\n",
    "* Run matching algorithm (astcheck) on file made in previous step\n",
    "* Save solar system object detections (matches) to SSO catalogue\n",
    "* Create MPC report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chk_files(noon, noon_type, rundir):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that creates the CHK files that astcheck uses (and produces if\n",
    "    they don't exist yet) when matching, by running astcheck on a fake\n",
    "    detection and subsequently removing the fake data products excluding the\n",
    "    CHK files. These files describe the positions of all asteroids at the start\n",
    "    and end of the night (both at noon) in UTC. This function is only needed\n",
    "    when running match2SSO in the day mode, as this will then allow\n",
    "    parallelisation in the night mode.\n",
    "    \n",
    "    As the 24-hour local observing night (between local noons) can overlap with\n",
    "    two UTC nights (between UTC noons) if there is a large time difference\n",
    "    between the timezone of the telescope and UTC, we will have to try\n",
    "    producing CHK files both for a time close to the start of the local night\n",
    "    (we take 1 min after) as well as for a time close to the end of it (1 min\n",
    "    before). This function should hence be run twice. This will produce a total\n",
    "    of 2 or 3 CHK files that astcheck will subsequently use to match any\n",
    "    observation taken during the local observing night to the known solar\n",
    "    system objects.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    noon: datetime object\n",
    "        Noon that corresponds to either the start or the end of the observing\n",
    "        night.\n",
    "    noon_type: string\n",
    "        Should be either \"night_start\" or \"night_end\", depending on which noon\n",
    "        was given as input.\n",
    "    rundir: string\n",
    "        Directory in which astcheck is run. This directory should contain\n",
    "        the mpc2sof catalogue that contains the known solar system objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check noon_type parameter and set observation time for fake\n",
    "    # detection\n",
    "    noon_type = noon_type.lower()\n",
    "    if noon_type not in [\"night_start\", \"night_end\"]:\n",
    "        LOG.error(\"Unknown noon type!\")\n",
    "        return\n",
    "    if noon_type == \"night_start\":\n",
    "        obstime = noon + timedelta(minutes=1)\n",
    "    elif noon_type == \"night_end\":\n",
    "        obstime = noon - timedelta(minutes=1)\n",
    "    \n",
    "    # Convert observation time of fake detection to UTC\n",
    "    obstime = obstime.astimezone(pytz.utc)\n",
    "    \n",
    "    # Create MPC-formatted file with fake detection\n",
    "    mpcformat_file_fake = \"{}fakedetection_{}_MPCformat.txt\".format(rundir,\n",
    "                                                                    noon_type)\n",
    "    LOG.info(\"Creating fake detection: {}\".format(mpcformat_file_fake))\n",
    "    mpcformat_file_fake_content = open(mpcformat_file_fake, \"w\")\n",
    "    fake_detection = \"\".join([\n",
    "        \"     0000001  C{} {:0>2} {:08.5f} \"\n",
    "        .format(obstime.year, obstime.month, obstime.day),\n",
    "        \"00 00 00.00 +00 00 00.0          0.00 G      L66\"\n",
    "        ])\n",
    "    mpcformat_file_fake_content.write(fake_detection)\n",
    "    mpcformat_file_fake_content.close()\n",
    "    \n",
    "    # Run astcheck on fake observation to create CHK files\n",
    "    LOG.info(\"Running astcheck on fake detection\")\n",
    "    astcheck_file_fake = mpcformat_file_fake.replace(\"_MPCformat.txt\",\n",
    "                                                     \"_astcheckMatches.txt\")\n",
    "    run_astcheck(mpcformat_file_fake, rundir, astcheck_file_fake,\n",
    "                 matching_radius=0)\n",
    "    \n",
    "    # Remove MPC-formatted file and astcheck output related to the fake\n",
    "    # detection\n",
    "    os.remove(mpcformat_file_fake)\n",
    "    LOG.info(\"Removed {}\".format(mpcformat_file_fake))\n",
    "    os.remove(astcheck_file_fake)\n",
    "    LOG.info(\"Removed {}\".format(astcheck_file_fake))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_database(sso_type, redownload_db, tmp_folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function downloads the asteroid or comet database if desired. After\n",
    "    downloading, asteroids with too large uncertainties will be removed from\n",
    "    the downloaded asteroid database copy.\n",
    "    Alternatively, the function will load the latest downloaded version of the\n",
    "    database. The function returns the database name and version number.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sso_type: string\n",
    "        Solar system object type that is in the database. Can be either\n",
    "        'asteroid' or 'comet'. Capitals are allowed as well.\n",
    "    redownload_db: boolean\n",
    "        If False, the databases will not be redownloaded. Instead, the name and\n",
    "        version number of the latest downloaded database version are returned.\n",
    "    tmp_folder: string\n",
    "        Folder to save the asteroid and comet databases to that are downloaded\n",
    "        in this function.\n",
    "    \"\"\"\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_subfunc = time.time()\n",
    "        \n",
    "    sso_type = sso_type.lower()\n",
    "    if sso_type == \"asteroid\":\n",
    "        database_url = settingsFile.URL_asteroidDatabase\n",
    "    elif sso_type == \"comet\":\n",
    "        database_url = settingsFile.URL_cometDatabase\n",
    "    else:\n",
    "        error_string = \"Database type unknown. Cannot be downloaded.\"\n",
    "        LOG.critical(error_string)\n",
    "        raise ValueError(error_string)\n",
    "    \n",
    "    # Determine whether database needs to be downloaded\n",
    "    existing_databases = list_files(\"{}{}DB_\".format(tmp_folder, sso_type),\n",
    "                                    end_str=\".dat\")\n",
    "    existing_unintegrated_databases = [DB for DB in existing_databases \\\n",
    "                                       if \"epoch\" not in DB]\n",
    "    download = True\n",
    "    if not redownload_db and len(existing_databases) > 0:\n",
    "        download = False\n",
    "        \n",
    "    # Download database if desired and get database version\n",
    "    if download:\n",
    "        LOG.info(\"Downloading {} database...\".format(sso_type))\n",
    "        database_version = datetime.utcnow().strftime(\"%Y%m%dT%H%M\")\n",
    "        database_name = \"{}{}DB_version{}.dat\".format(tmp_folder, sso_type,\n",
    "                                                      database_version)\n",
    "        req = requests.get(database_url, allow_redirects=True)\n",
    "        open(database_name, \"wb\").write(req.content)\n",
    "        LOG.info(\"{} database version: {}\".format(sso_type, database_version))\n",
    "        \n",
    "        # Remove asteroids with large orbital uncertainties from database\n",
    "        if sso_type == \"asteroid\":\n",
    "            select_asteroids_on_uncertainty(database_name)\n",
    "            \n",
    "        #if not KEEP_TMP and len(existing_databases) > 0:\n",
    "        #    LOG.info(\"Removing older {} database versions.\".format(sso_type))\n",
    "        #    for old_database in existing_databases:\n",
    "        #        os.remove(old_database)\n",
    "        #        LOG.info(\"Removed {}.\".format(old_database))\n",
    "    else:\n",
    "        # Retrieve most recent (unintegrated) database version. If there is no\n",
    "        # unintegrated database, retrieve the most recent integrated one. Empty\n",
    "        # databases (created when INCLUDE_COMETS = False) are not taken into\n",
    "        # account, as these are in a different folder.\n",
    "        databases_sorted = sorted(existing_unintegrated_databases)\n",
    "        if not databases_sorted:\n",
    "            databases_sorted = sorted(existing_databases)\n",
    "        database_name = databases_sorted[-1]\n",
    "        database_version = os.path.splitext(\n",
    "            os.path.basename(database_name))[0].split(\"_\")[1].replace(\n",
    "                \"version\", \"\")\n",
    "        LOG.info(\"{} database version {}\".format(sso_type, database_version))\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_subfunc, label=\"downloadDatabase ({})\"\n",
    "                          .format(sso_type))\n",
    "    \n",
    "    return database_name, database_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_known_objects_database(midnight, rundir, tmp_folder, redownload_db):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function downloads the most recent versions of the asteroid database and\n",
    "    the comet database. It then uses integrat.cpp from the lunar repository to\n",
    "    integrate the asteroid orbits to midnight of the observation night, in\n",
    "    order to optimize the predicted positions of known objects.\n",
    "    \n",
    "    Beware: the current version of integrat.cpp cannot be used on JPL's comet\n",
    "    file, as it is not compatible with its format. As a consequence, there\n",
    "    might be an offset in the predictions of the comet positions, perhaps\n",
    "    causing us to miss these objects in the linking routine. Note that the\n",
    "    MPC's comet file could be integrated to the right epoch using integrat.cpp,\n",
    "    but as this file is not compatible with astcheck, it cannot be used here.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    midnight: datetime object, including time zone\n",
    "        Local midnight during the observation night.\n",
    "    rundir: string\n",
    "        Directory in which mpc2sof is run and which the known objects catalogue\n",
    "        is saved to.\n",
    "    tmp_folder: string\n",
    "        Folder to save the known objects databases to that are created in this\n",
    "        function.\n",
    "    redownload_db: boolean\n",
    "        If False, the databases will not be redownloaded. They will only be\n",
    "        integrated to the observation epoch (midnight on the observation\n",
    "        night).\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of create_known_objects_database\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    # Download asteroid database\n",
    "    asteroid_database, asteroid_database_version = download_database(\n",
    "        \"asteroid\", redownload_db, tmp_folder)\n",
    "    \n",
    "    # Download comet database if requested\n",
    "    if INCLUDE_COMETS:\n",
    "        _, comet_database_version = download_database(\"comet\", redownload_db,\n",
    "                                                      tmp_folder)\n",
    "    else:\n",
    "        LOG.info(\"Do not download comet database. Instead, create an empty \"\n",
    "                 \"comet database so that there's no matching to comets.\")\n",
    "        open(\"{}ELEMENTS.COMET\".format(rundir), \"w\").write(\"\".join([\n",
    "            \"Num  Name                                     Epoch      q      \",\n",
    "            \"     e        i         w        Node          Tp       Ref\\n---\",\n",
    "            \"---------------------------------------- ------- ----------- ---\",\n",
    "            \"------- --------- --------- --------- -------------- ------------\"\n",
    "            ]))\n",
    "    \n",
    "    # Integrat only accepts UTC midnights. Choose the one closest to local\n",
    "    # midnight.\n",
    "    date_midnight = midnight.date()\n",
    "    if midnight.hour >= 12.:\n",
    "        date_midnight = date_midnight + timedelta(days=1)\n",
    "    midnight_utc = pytz.utc.localize(datetime.strptime(\" \".join([\n",
    "        date_midnight.strftime(\"%Y%m%d\"), \"000000\"]), \"%Y%m%d %H%M%S\"))\n",
    "    midnight_utc_str = midnight_utc.strftime(\"%Y%m%dT%H%M\")\n",
    "    \n",
    "    # Integrate the asteroid database to the observation date\n",
    "    integrated_asteroid_database = (\n",
    "        \"{}asteroidDB_version{}_epoch{}.dat\"\n",
    "        .format(tmp_folder, asteroid_database_version, midnight_utc_str))\n",
    "    if not isfile(integrated_asteroid_database):\n",
    "        LOG.info(\"Integrating asteroid database to epoch {}...\"\n",
    "                 .format(midnight_utc_str))\n",
    "        if TIME_FUNCTIONS:\n",
    "            t_subtiming = time.time()\n",
    "        subprocess.run([\"integrat\", asteroid_database,\n",
    "                        integrated_asteroid_database,\n",
    "                        str(Time(midnight_utc).jd), \"-f{}\".format(FILE_JPLEPH)],\n",
    "                       cwd=tmp_folder, stdout=subprocess.DEVNULL, check=True)\n",
    "        if TIME_FUNCTIONS:\n",
    "            log_timing_memory(t_subtiming, label=\"integrat\")\n",
    "        \n",
    "        # Remove temporary file created by integrat\n",
    "        if isfile(\"{}ickywax.ugh\".format(tmp_folder)):\n",
    "            os.remove(\"{}ickywax.ugh\".format(tmp_folder))\n",
    "    \n",
    "    # Create the symbolic links in the run directory that mpc2sof needs\n",
    "    symlink_asteroid_database = \"{}MPCORB.DAT\".format(rundir)\n",
    "    if isfile(symlink_asteroid_database):\n",
    "        LOG.info(\"Removing the old MPCORB.DAT symbolic link\")\n",
    "        os.unlink(symlink_asteroid_database)\n",
    "    os.symlink(integrated_asteroid_database, symlink_asteroid_database)\n",
    "    LOG.info(\"Created symbolic link {}\".format(symlink_asteroid_database))\n",
    "    \n",
    "    if INCLUDE_COMETS:\n",
    "        symlink_comet_database = \"{}ELEMENTS.COMET\".format(rundir)\n",
    "        if isfile(symlink_comet_database):\n",
    "            LOG.info(\"Removing the old ELEMENTS.COMET symbolic link\")\n",
    "            os.unlink(symlink_comet_database)\n",
    "        os.symlink(\"{}cometDB_version{}.dat\".format(\n",
    "            tmp_folder, comet_database_version), symlink_comet_database)\n",
    "        LOG.info(\"Created symbolic link {}\".format(symlink_comet_database))\n",
    "    #mem_use(label=\"after creating symbolic links to the databases\")\n",
    "    \n",
    "    # Combine the known comets and asteroids into a SOF file, which astcheck\n",
    "    # will then use as input\n",
    "    LOG.info(\"Combining asteroids and comets into SOF file.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_subtiming = time.time()\n",
    "    \n",
    "    subprocess.run(\"mpc2sof\", cwd=rundir, check=True)\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_subtiming, label=\"mpc2sof\")\n",
    "        log_timing_memory(t_func, label=\"create_known_objects_database\")\n",
    "    LOG.info(\"Finished loading and formatting external databases.\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_asteroids_on_uncertainty(asteroid_database):\n",
    "    \n",
    "    \"\"\"\n",
    "    Go through the asteroid database (MPCORB format) and select the asteroids\n",
    "    which have orbital uncertainty parameters smaller than maxUncertainty.\n",
    "    The MPC uncertainty parameters that we consider are explained here:\n",
    "    https://www.minorplanetcenter.net/iau/info/UValue.html\n",
    "    \n",
    "    Overwrite the database with just the asteroids selected on their orbital\n",
    "    uncertainties, so that there will be no matching with poorly known objects.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    asteroid_database: string\n",
    "        Name of the full asteroid database (MPCORB-formatted text-file).\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of select_asteroids_on_uncertainty\")\n",
    "    \n",
    "    if settingsFile.maxUncertainty is None:\n",
    "        LOG.info(\"All known solar system bodies are used in the matching, \"\n",
    "                 \"irrespective of their uncertainty parameter.\")\n",
    "        return\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    LOG.info(\"Removing asteroids with too large uncertainties...\\n\")\n",
    "    \n",
    "    #Open asteroid database\n",
    "    asteroid_database_content = open(asteroid_database, \"r\").readlines()\n",
    "    \n",
    "    #Find the size of the header of the asteroid database, assuming that the\n",
    "    #header ends with a line of dashes.\n",
    "    header_end_index = 0\n",
    "    line_index = 0\n",
    "    for line in asteroid_database_content:\n",
    "        if line.startswith(\"-----\"):\n",
    "            header_end_index = line_index\n",
    "            break\n",
    "        line_index += 1\n",
    "    \n",
    "    #Re-write the asteroid database file, including only the header and the\n",
    "    #lines corresponding to asteroids that have small orbital uncertainties.\n",
    "    number_asteroids_pre_selection = 0\n",
    "    number_asteroids_post_selection = 0\n",
    "    with open(asteroid_database, \"w\") as asteroid_database_content_new:\n",
    "        for line_index in range(len(asteroid_database_content)-1):\n",
    "            \n",
    "            #Copy header to file\n",
    "            if line_index <= header_end_index:\n",
    "                asteroid_database_content_new.write(\n",
    "                    asteroid_database_content[line_index])\n",
    "                continue\n",
    "            \n",
    "            line = asteroid_database_content[line_index]\n",
    "            \n",
    "            #Copy empty lines\n",
    "            if line == \"\\n\":\n",
    "                asteroid_database_content_new.write(line)\n",
    "                continue\n",
    "            \n",
    "            number_asteroids_pre_selection += 1\n",
    "            \n",
    "            #Filter on uncertainty parameter. Copy lines of asteroids for\n",
    "            #which orbits are determined reasonably well.\n",
    "            uncertainty = line[105]\n",
    "            if uncertainty.isdigit():\n",
    "                if float(uncertainty) <= settingsFile.maxUncertainty:\n",
    "                    asteroid_database_content_new.write(line)\n",
    "                    number_asteroids_post_selection += 1\n",
    "    \n",
    "    LOG.info(\"{} out of {} asteroids have U <= {}\".format(\n",
    "        number_asteroids_post_selection, number_asteroids_pre_selection,\n",
    "        settingsFile.maxUncertainty))\n",
    "    LOG.info(\"Asteroid database now only includes sources with U <= {}\"\n",
    "             .format(settingsFile.maxUncertainty))\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"select_asteroids_on_uncertainty\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(transient_cat, rundir, predict_cat, mpc_code,\n",
    "                is_FOV_circle=False):\n",
    "    \"\"\"\n",
    "    Use astcheck to predict which asteroids are in the FOV during the\n",
    "    observation. Predictions can be made for a circular or square FOV. The\n",
    "    function returns the number of asteroids that are estimated to be bright\n",
    "    enough to be detected (V mag <= limiting AB magnitude). This is a crude\n",
    "    estimate, as no correction for the different type of magnitudes is taken\n",
    "    into account.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    transient_cat: string or None\n",
    "        Path to and name of the transient catalogue. If None, we will make a\n",
    "        dummy (empty) predictions catalogue.\n",
    "    rundir: string\n",
    "        Directory corresponding to observation night where the temporary output\n",
    "        file that astcheck makes will be stored.\n",
    "    predict_cat: string\n",
    "        Path to and name of the output catalogue to which the predictions will\n",
    "        be saved.\n",
    "    mpc_code: string\n",
    "        MPC code corresponding to the telescope.\n",
    "    is_FOV_circle: boolean\n",
    "        Boolean indicating if the FOV is circular. If False, a square FOV is\n",
    "        assumed where the sides are aligned with RA & Dec.\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of predictions\")\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    if not OVERWRITE_FILES and isfile(predict_cat):\n",
    "        LOG.info(\"Prediction catalogue already exists and won't be re-made.\")\n",
    "        with fits.open(predict_cat) as hdu:\n",
    "            hdr = hdu[1].header\n",
    "        return hdr[\"N-SSO\"]\n",
    "    \n",
    "    # If the transient catalogue was red-flagged, we will not make a predictions\n",
    "    # catalogue as there is little use for it.\n",
    "    if transient_cat is None:\n",
    "        LOG.info(\"Creating a dummy predictions catalogue.\")\n",
    "        sso_header = create_sso_header(rundir, 0, 0, True, False)\n",
    "        fitstable = format_cat(Table(), start_header=sso_header)\n",
    "        save_fits(fitstable, predict_cat, rundir=rundir)\n",
    "        if TIME_FUNCTIONS:\n",
    "            log_timing_memory(t_func, label=\"predictions\")\n",
    "        return 0\n",
    "    \n",
    "    LOG.info(\"Making predictions...\")\n",
    "    \n",
    "    # Open header of transient catalogue\n",
    "    with fits.open(transient_cat) as hdu:\n",
    "        hdr = hdu[1].header\n",
    "    \n",
    "    # Get the observation date (isot format) and central field coordinates (in\n",
    "    # deg) from the header\n",
    "    date = hdr[DATE_KEYWORD]\n",
    "    if CENTRAL_RA_KEYWORD in hdr.keys() and CENTRAL_DEC_KEYWORD in hdr.keys():\n",
    "        ra_field = hdr[CENTRAL_RA_KEYWORD]\n",
    "        dec_field = hdr[CENTRAL_DEC_KEYWORD]\n",
    "    else:\n",
    "        ra_field = hdr[\"RA\"]\n",
    "        dec_field = hdr[\"DEC\"]\n",
    "    limmag = hdr[LIMMAG_KEYWORD]\n",
    "    \n",
    "    # Create temporary output file for astcheck results\n",
    "    output_file = \"{}{}\".format(rundir, os.path.basename(transient_cat).replace(\n",
    "        \"_light\", \"\").replace(\"_trans.fits\", \"_sso_predictions.txt\"))\n",
    "    output_file_content = open(output_file, \"w\")\n",
    "    \n",
    "    if is_FOV_circle:\n",
    "        field_radius = 3600.*settingsFile.FOV_width/2.\n",
    "    else:\n",
    "        #Square FOV. Use the half diagonal of the FOV as the radius.\n",
    "        field_radius = 3600.*sqrt(2)*settingsFile.FOV_width/2.\n",
    "    \n",
    "    # Run astcheck from folder containing .sof-file\n",
    "    subprocess.call([\"astcheck\", \"-c\", str(date), str(ra_field), str(dec_field),\n",
    "                     str(mpc_code), \"-r{}\".format(field_radius), \"-h\",\n",
    "                     \"-m{}\".format(settingsFile.limitingMagnitude),\n",
    "                     \"-M{}\".format(settingsFile.maximalNumberOfAsteroids)],\n",
    "                    stdout=output_file_content, cwd=rundir)\n",
    "    output_file_content.close()\n",
    "    \n",
    "    # Read in astcheck output\n",
    "    astcheck_file_content = open(output_file, \"r\").readlines()\n",
    "    astcheck_file_content = remove_astcheck_header_and_footer(\n",
    "        astcheck_file_content)\n",
    "    \n",
    "    # Create table to store solar system bodies and their properties\n",
    "    output_columns = {\n",
    "        \"ID_SSO\":       [\"12a\", \"\", \"\"],\n",
    "        \"RA_SSO\":       [\"f4\", \"deg\", \"%.6f\"],\n",
    "        \"DEC_SSO\":      [\"f4\", \"deg\", \"%.6f\"],\n",
    "        \"V_RA_SSO\":     [\"f4\", \"arcsec/hour\", \"%.4f\"],\n",
    "        \"V_DEC_SSO\":    [\"f4\", \"arcsec/hour\", \"%.4f\"],\n",
    "        \"MAG_V_SSO\":    [\"f4\", \"\", \"%.2f\"]\n",
    "        }\n",
    "    output_table = Table()\n",
    "    for key in output_columns.keys():\n",
    "        output_table.add_column(Column(name=key, dtype=output_columns[key][0],\n",
    "                                       unit=output_columns[key][1]))\n",
    "        if output_columns[key][2]:\n",
    "            output_table[key].format = output_columns[key][2]\n",
    "        \n",
    "    # Loop through SSOs and store their properties in the output table\n",
    "    for source in astcheck_file_content:\n",
    "        source = re.sub(\"\\n$\", \"\", source) # Remove line end character\n",
    "        source_properties = re.split(\" +\", source)\n",
    "        if source_properties[0]:\n",
    "            identifier = \" \".join([source_properties[0], source_properties[1]])\n",
    "        else:\n",
    "            identifier = source_properties[1]\n",
    "        ra_source, dec_source, magnitude, v_ra, v_dec = source_properties[2:]\n",
    "        \n",
    "        output_row = (identifier, float(ra_source), float(dec_source),\n",
    "                      float(v_ra), float(v_dec), float(magnitude))\n",
    "        output_table.add_row(output_row)\n",
    "    \n",
    "    # Remove temporary astcheck output file\n",
    "    if not KEEP_TMP:\n",
    "        os.remove(output_file)\n",
    "        LOG.info(\"Removed {}\".format(output_file))\n",
    "    \n",
    "    # In case of a square FOV, disregard SSOs outside the FOV\n",
    "    if not is_FOV_circle and len(output_table)>0:\n",
    "        center = SkyCoord(ra_field, dec_field, unit=\"deg\", frame=\"icrs\")\n",
    "        sources = SkyCoord(output_table[\"RA_SSO\"],\n",
    "                           output_table[\"DEC_SSO\"], unit=\"deg\", frame=\"icrs\")\n",
    "        dra, ddec = center.spherical_offsets_to(sources)\n",
    "        mask_dist = ((abs(dra.deg) <= settingsFile.FOV_width/2.) &\n",
    "                     (abs(ddec.deg) <= settingsFile.FOV_width/2.))\n",
    "        output_table = output_table[mask_dist]\n",
    "    \n",
    "    # Create header for predicted asteroids in FOV\n",
    "    i_bright = np.where(output_table[\"MAG_V_SSO\"] <= limmag)[0]\n",
    "    N_sso = len(i_bright)\n",
    "    if N_sso > 0:\n",
    "        dummy = False\n",
    "    else:\n",
    "        dummy = True\n",
    "    sso_header = create_sso_header(rundir, 0, N_sso, dummy, False)\n",
    "    \n",
    "    # Save to table\n",
    "    fitstable = format_cat(output_table, start_header=sso_header)\n",
    "    save_fits(fitstable, predict_cat, rundir=rundir)\n",
    "    \n",
    "    LOG.info(\"Predictions saved to {}.\".format(predict_cat))\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"predictions\")\n",
    "    \n",
    "    return N_sso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sso_header(rundir, N_det, N_sso, dummy, incl_detections):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function creates the headers for the SSO catalogue and the SSO predictions\n",
    "    catalogue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rundir: string\n",
    "        Name of the folder in which the symbolic links to the databases are\n",
    "        stored. These are used to get the version numbers of the databases.\n",
    "    N_det: int\n",
    "        Number of detected solar system objects. Only one matched object is\n",
    "        counted per transient detection and if an object is matched to multiple\n",
    "        transients, it is also only counted once.\n",
    "    N_sso: int\n",
    "        Number of solar system objects in the FOV that are supposedly bright\n",
    "        enough for a detection (V magnitude < T-LMAG). The difference between V\n",
    "        and AB magnitudes is ignored here. This number is therefore a rough\n",
    "        prediction for the number of detections.\n",
    "    dummy: boolean\n",
    "        Boolean indicating whether the catalogue is a dummy catalogue without\n",
    "        sources (dummy=True). If False, there are sources in the catalogue.\n",
    "    incl_detections: boolean\n",
    "        Boolean indicating whether the catalogue is the SSO catalogue -\n",
    "        corresponding to detected solar system objects - or not. If not, it is\n",
    "        the catalogue containing the predicted objects in the FOV and some of\n",
    "        the header keywords will not be included in the header.\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of create_sso_header\")\n",
    "    LOG.info(\"Creating SSO header.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    # Create empty SSO header\n",
    "    header = fits.Header()\n",
    "    \n",
    "    # Add Python version to SSO header\n",
    "    header[\"PYTHON-V\"] = (platform.python_version(), \"Python version used\")\n",
    "    \n",
    "    # Get C++ version and add to the SSO header. Based on \n",
    "    # [https://stackoverflow.com/questions/44734397/which-c-standard-is-the-\n",
    "    # default-when-compiling-with-g/44735016#44735016]\n",
    "    proc = subprocess.run(\"g++ -dM -E -x c++  /dev/null | grep -F __cplusplus\",\n",
    "                          capture_output=True, shell=True, check=True)\n",
    "    cpp_macro = proc.stdout.decode(\"utf-8\").replace(\"\\n\", \"\").split()[-1]\n",
    "    \n",
    "    if cpp_macro not in settingsFile.CPPmacro2version.keys():\n",
    "        LOG.error(\"C++ macro unknown: {}\".format(cpp_macro))\n",
    "        cpp_version = \"None\"\n",
    "    else:\n",
    "        cpp_version = settingsFile.CPPmacro2version[cpp_macro]\n",
    "    header[\"CPP-V\"] = (cpp_version, \"C++ version used\")\n",
    "    \n",
    "    # Get G++ version and add to SSO header\n",
    "    proc = subprocess.run(\"g++ --version\", capture_output=True, shell=True,\n",
    "                          check=True)\n",
    "    gpp_version = proc.stdout.decode(\"utf-8\").split(\"\\n\")[0].split()[-1]\n",
    "    header[\"GPP-V\"] = (gpp_version, \"G++ version used\")\n",
    "    \n",
    "    # Add match2SSO & header keyword versions to the SSO header\n",
    "    header[\"SSO-V\"] = (__version__, \"match2SSO version used\")\n",
    "    header[\"SSOKW-V\"] = (KEYWORDS_VERSION,\n",
    "                         \"SSO header keywords version used\")\n",
    "    \n",
    "    # Get the versions of the lunar & jpl_eph repositories. The versions are\n",
    "    # given by unique strings signifying the latest commit that was made to the\n",
    "    # repositories. Save the strings to the SSO header.\n",
    "    lunar_version = retrieve_version(\"lunar\")\n",
    "    header[\"LUNAR-V\"] = (lunar_version, \"lunar repository version used\")\n",
    "    \n",
    "    jpl_eph_version = retrieve_version(\"jpl_eph\")\n",
    "    header[\"JPLEPH-V\"] = (jpl_eph_version, \"jpl_eph repository version used\")\n",
    "    \n",
    "    # Add version of JPL lunar & planetary ephemerides file to SSO header\n",
    "    header[\"JPLDE-V\"] = (\"DE{}\".format(FILE_JPLEPH.split(\".\")[-1]),\n",
    "                         \"JPL ephemeris file version used\")\n",
    "    \n",
    "    # Add asteroid database version & reference epoch to the SSO header.\n",
    "    # The MPCORB.DAT symbolic link in the run directory refers to the\n",
    "    # asteroid database version that was used. The name structure of this\n",
    "    # database is: \n",
    "    # asteroid_database_version[yyyymmddThhmm]_epoch[yyyymmddThhmm].dat\n",
    "    asteroid_database_version, asteroid_database_epoch = \"None\", \"None\"\n",
    "    if isfile(\"{}MPCORB.DAT\".format(rundir)):\n",
    "        asteroid_database = os.readlink(\"{}MPCORB.DAT\".format(rundir))\n",
    "        asteroid_database_date = os.path.basename(\n",
    "            asteroid_database).split(\"_\")[1].replace(\"version\", \"\")\n",
    "        asteroid_database_version = \"{}-{}-{}T{}:{}\".format(\n",
    "            asteroid_database_date[0:4], asteroid_database_date[4:6],\n",
    "            asteroid_database_date[6:8], asteroid_database_date[9:11],\n",
    "            asteroid_database_date[11:13])\n",
    "        reference_epoch = os.path.basename(asteroid_database).split(\n",
    "            \"_\")[2].replace(\"epoch\", \"\")\n",
    "        asteroid_database_epoch = (\"{}-{}-{}T{}:{}\".format(\n",
    "            reference_epoch[0:4], reference_epoch[4:6], reference_epoch[6:8],\n",
    "            reference_epoch[9:11], reference_epoch[11:13]))\n",
    "    \n",
    "    header[\"ASTDB-V\"] = (asteroid_database_version,\n",
    "                         \"asteroid database version (date in UTC)\")\n",
    "    header[\"ASTDB-EP\"] = (asteroid_database_epoch,\n",
    "                          \"asteroid database epoch in UTC\")\n",
    "    \n",
    "    # Add comet database version to the SSO header.\n",
    "    # The ELEMENTS.COMET symbolic link in the run directory refers to the\n",
    "    # comet database version that was used. The name structure of this\n",
    "    # database is: cometDB_version[yyyymmddThhmm].dat\n",
    "    comet_database_version = \"None\"\n",
    "    if INCLUDE_COMETS:\n",
    "        comet_database = os.readlink(\"{}ELEMENTS.COMET\".format(rundir))\n",
    "        comet_database_date = os.path.basename(comet_database).split(\n",
    "            \"_\")[1].replace(\"version\", \"\")\n",
    "        comet_database_version = \"{}-{}-{}T{}:{}\".format(\n",
    "            comet_database_date[0:4], comet_database_date[4:6],\n",
    "            comet_database_date[6:8], comet_database_date[9:11],\n",
    "            comet_database_date[11:13])\n",
    "    \n",
    "    header[\"COMDB-V\"] = (comet_database_version,\n",
    "                         \"comet database version (date in UTC)\")\n",
    "    \n",
    "    # Add matching radius and maximum orbital uncertainty parameter to header\n",
    "    if incl_detections:\n",
    "        header[\"RADIUS\"] = (float(settingsFile.matchingRadius),\n",
    "                            \"matching radius in arcsec\")\n",
    "    header[\"U-MAX\"] = (settingsFile.maxUncertainty,\n",
    "                       \"maximum orbital uncertainty parameter\")\n",
    "    \n",
    "    # Add number of (predicted) detections to header\n",
    "    header[\"N-SSO\"] = (N_sso, \"predicted number of bright SSOs in FOV\")\n",
    "    if incl_detections:\n",
    "        header[\"N-SSODET\"] = (N_det, \"number of detected solar system objects\")\n",
    "    \n",
    "    # Add keyword indicating whether there is\n",
    "    header[\"SDUMCAT\"] = (bool(dummy), \"dummy SSO catalogue without sources?\")\n",
    "    \n",
    "    LOG.info(\"SSO header complete.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"create_sso_header\")\n",
    "    \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fits2mpc(transient_cat, mpcformat_file):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function converts the transient catalogue to a text file of the MPC\n",
    "    80-column format, so that astcheck can run on it. For the asteroid / comet\n",
    "    identifier used in the MPC file, the transient number is used. This\n",
    "    transient number cannot be used for MPC reports as it is not all-time unique\n",
    "    (per telescope). But it is a straight-forward way to link detections to\n",
    "    known solar system objects within match2SSO.\n",
    "    \n",
    "    Negative transients are excluded from the MPC-formatted file. In the case of\n",
    "    moving objects, these are sources that are present in the reference image\n",
    "    but not in the new image. They can be recognised as having a negative\n",
    "    signal-to-noise ratio value in the significance (Scorr) image for MeerLICHT\n",
    "    and BlackGEM. As reference images can be stacked images (that are not\n",
    "    centered on the asteroid) for which the observation date and time is\n",
    "    unclear, and as we use the date of the new image as the observation date,\n",
    "    asteroids in the reference images are not useful.\n",
    "    \n",
    "    Function returns the MPC Observatory code of the telescope with which the\n",
    "    observation was made, and a boolean indicating whether there were detections\n",
    "    to be converted (dummy = False) or not (dummy = True).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    transient_cat: string\n",
    "        Path to and name of the transient catalogue.\n",
    "    mpcformat_file: string\n",
    "        Path to and name of the MPC-formatted text file that is made in this\n",
    "        function.\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of convert_fits2mpc\")\n",
    "    LOG.info(\"Converting transient catalogue to MPC-format.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    # Load transient catalogue header\n",
    "    with fits.open(transient_cat) as hdu:\n",
    "        transient_header = hdu[1].header\n",
    "    \n",
    "    # Get the MPC observatory code from the header\n",
    "    mpc_code = transient_header[MPC_CODE_KEYWORD].strip()\n",
    "    if mpc_code not in list(pd.read_fwf(settingsFile.obsCodesFile,\n",
    "                                        widths=[4, 2000],\n",
    "                                        skiprows=1)[\"Code\"])[:-1]:\n",
    "        LOG.critical(\"MPC code {} is not in the MPC list of observatory codes\"\n",
    "                     .format(mpc_code))\n",
    "        return None, True\n",
    "    \n",
    "    # Check if MPC-formatted file exists and if it should be overwritten or not\n",
    "    if not OVERWRITE_FILES and isfile(mpcformat_file):\n",
    "        LOG.info(\"MPC-formatted file already exists and will not re-made.\")\n",
    "        return mpc_code, False\n",
    "    \n",
    "    # Get observation date in the right format\n",
    "    date_obs = Time(transient_header[DATE_KEYWORD], format=\"isot\").datetime\n",
    "    decimal_day = date_obs.day + 1./24.*(\n",
    "        date_obs.hour + 1./60.*(date_obs.minute + 1./60.*(\n",
    "            date_obs.second + date_obs.microsecond/10.**6)))\n",
    "    mpc_char16to32 = \"{} {:0>2} {:08.5f} \".format(date_obs.year, \n",
    "                                                  date_obs.month, decimal_day)\n",
    "    # Load transient catalogue data\n",
    "    with fits.open(transient_cat) as hdu:\n",
    "        detections = Table(hdu[1].data)\n",
    "    \n",
    "    # Remove negative transients\n",
    "    index_positives = np.where(detections[SNR_COLUMN]>=0)[0]\n",
    "    detections = detections[index_positives]\n",
    "    \n",
    "    # Check if there are positive transients to include\n",
    "    if len(detections) == 0:\n",
    "        dummy = True\n",
    "        LOG.info(\"No (positive) sources available for linking.\")\n",
    "        return mpc_code, dummy\n",
    "    dummy = False\n",
    "    \n",
    "    # Create output file\n",
    "    mpcformat_file_content = open(mpcformat_file, \"w\")\n",
    "    \n",
    "    # Loop over the detections and add data to the MPC-formatted file\n",
    "    for detection_index in range(len(detections)):\n",
    "        # Use the source numbers as \"temporary designations\" in the MPC format.\n",
    "        # In this way, we will be able to link the known objects to the right\n",
    "        # source.\n",
    "        line = (\"     {:0>7}  C\"\n",
    "                .format(detections[NUMBER_COLUMN][detection_index]))\n",
    "        line = \"\".join([line, mpc_char16to32])\n",
    "        \n",
    "        # Get the coordinates and magnitude of the source\n",
    "        coord = SkyCoord(detections[RA_COLUMN][detection_index],\n",
    "                         detections[DEC_COLUMN][detection_index],\n",
    "                         unit=\"deg\", frame=\"icrs\") \n",
    "        mag = \"{:.1f}\".format(detections[MAG_COLUMN][detection_index])\n",
    "        \n",
    "        line = \"\".join([line, \"{} {}          {} G      {}\"\n",
    "                        .format(coord.to_string(\"hmsdms\", sep=\" \",\n",
    "                                                precision=2)[:11],\n",
    "                                coord.to_string(\"hmsdms\", sep=\" \",\n",
    "                                                precision=1)[-11:],\n",
    "                                mag.rjust(4), mpc_code)])\n",
    "        \n",
    "        # Write the data to the MPC-formatted file\n",
    "        mpcformat_file_content.write(\"{}\\n\".format(line))\n",
    "    \n",
    "    mpcformat_file_content.close()\n",
    "    \n",
    "    LOG.info(\"MPC-formatted file saved to {}.\".format(mpcformat_file))\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"convert_fits2mpc\")\n",
    "    \n",
    "    return mpc_code, dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_astcheck(mpcformat_file, rundir, output_file,\n",
    "                 matching_radius=settingsFile.matchingRadius):\n",
    "    \"\"\"\n",
    "    Run astcheck on the input transient catalogue to find matches between\n",
    "    transient detections and known solar system objects. Per detection, all\n",
    "    matches within the matching_radius are selected and saved to the output text\n",
    "    file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mpcformat_file: string\n",
    "        Name of the input text-file that is formatted according to the MPC's\n",
    "        80-column MPC format. This file can list detections / tracklets for\n",
    "        astcheck to match, but it can also contain just a single row\n",
    "        representing the observation. In the latter use case, the specified\n",
    "        coordinates should correspond to the centre of the observation.\n",
    "    rundir: string\n",
    "        Directory in which astcheck is run. This directory should contain\n",
    "        the mpc2sof catalogue that contains the known solar system objects.\n",
    "    output_file: string\n",
    "        Path to and name of the output text file in which the matches found\n",
    "        by astcheck are stored.\n",
    "    matching_radius: int or float\n",
    "        Matching radius in arcsec. The default value is the one specified in\n",
    "        the settings file [set_match2SSO.py].\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of run_astcheck\")\n",
    "    LOG.info(\"Running astcheck: matching detections to known solar system \"\n",
    "             \"bodies.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    if not OVERWRITE_FILES and isfile(output_file):\n",
    "        LOG.info(\"Astcheck output file already exists and won't be re-made.\")\n",
    "        return\n",
    "    \n",
    "    # Create a file for storing the output of the astcheck run\n",
    "    output_file_content = open(output_file, \"w\")\n",
    "    \n",
    "    # Run astcheck from folder containing .sof-file\n",
    "    subprocess.call([\"astcheck\", mpcformat_file, \"-h\",\n",
    "                     \"-r{}\".format(matching_radius),\n",
    "                     \"-m{}\".format(settingsFile.limitingMagnitude),\n",
    "                     \"-M{}\".format(settingsFile.maximalNumberOfAsteroids)],\n",
    "                    stdout=output_file_content, cwd=rundir)\n",
    "    output_file_content.close()\n",
    "    \n",
    "    LOG.info(\"Matches saved to {}.\".format(output_file))\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"run_astcheck\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sso_catalogue(astcheck_file, rundir, sso_cat, N_sso):\n",
    "    \n",
    "    \"\"\"\n",
    "    Open the text-file that was produced when running astcheck [astcheck_file]\n",
    "    and save the information to an SSO catalogue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    astcheck_file: string\n",
    "        Name of the file containing astcheck's output (the matches). This can\n",
    "        be None, in which case we will create a dummy SSO catalogue without\n",
    "        matches.\n",
    "    rundir: string\n",
    "        Directory in which astcheck was run. This directory also contains\n",
    "        symbolic links to the asteroid and comet databases that were used to\n",
    "        create the known objects catalogue that astcheck used.\n",
    "    sso_cat: string\n",
    "        Name of the SSO catalogue to be created in which the matches\n",
    "        are stored.\n",
    "    N_sso: int\n",
    "        Number of solar system objects in the FOV that are supposedly bright\n",
    "        enough for a detection (V magnitude < T-LMAG). The difference between V\n",
    "        and AB magnitudes is ignored here. This number is therefore a rough\n",
    "        prediction for the number of detections.\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of create_sso_catalogue\")\n",
    "    LOG.info(\"Converting astcheck output into an SSO catalogue.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    if not OVERWRITE_FILES and isfile(sso_cat):\n",
    "        LOG.info(\"SSO catalogue already exists and will not be re-made.\")\n",
    "        return\n",
    "    \n",
    "    # If the transient catalogue was red-flagged, matching was not performed\n",
    "    # and an empty SSO catalogue needs to be created.\n",
    "    if astcheck_file is None:\n",
    "        LOG.info(\"Creating a dummy SSO catalogue.\")\n",
    "        sso_header = create_sso_header(rundir, 0, N_sso, True, True)\n",
    "        fitstable = format_cat(Table(), start_header=sso_header)\n",
    "        save_fits(fitstable, sso_cat, rundir=rundir)\n",
    "        if TIME_FUNCTIONS:\n",
    "            log_timing_memory(t_func, label=\"create_sso_catalogue\")\n",
    "        return\n",
    "    \n",
    "    # Remove astcheck header and footer if needed\n",
    "    astcheck_file_content = open(astcheck_file, \"r\").readlines()\n",
    "    astcheck_file_content = remove_astcheck_header_and_footer(\n",
    "        astcheck_file_content)\n",
    "    \n",
    "    # Find empty lines\n",
    "    separator = \"\\n\"\n",
    "    indices_separator = np.where(np.array(astcheck_file_content)\n",
    "                                 == separator)[0]\n",
    "    indices_separator = np.append(-1, indices_separator)\n",
    "    indices_separator = np.append(indices_separator,\n",
    "                                  len(astcheck_file_content))\n",
    "    \n",
    "    # Create table to store match information in\n",
    "    output_columns = {\n",
    "        NUMBER_COLUMN:  [\"i4\", \"\"],\n",
    "        \"ID_SSO\":       [\"12a\", \"\"],\n",
    "        \"DIST_RA_SSO\":  [\"i2\", \"arcsec\"],\n",
    "        \"DIST_DEC_SSO\": [\"i2\", \"arcsec\"],\n",
    "        \"DIST_SSO\":     [\"i2\", \"arcsec\"],\n",
    "        \"MAG_V_SSO\":    [\"f4\", \"\"],\n",
    "        \"FLAGS_SSO\":    [\"i2\", \"\"]\n",
    "        }\n",
    "    output_table = Table()\n",
    "    for key in output_columns.keys():\n",
    "        output_table.add_column(Column(name=key, dtype=output_columns[key][0],\n",
    "                                       unit=output_columns[key][1]))\n",
    "    # Loop over sources\n",
    "    N_det = 0\n",
    "    for index in range(len(indices_separator)-1):\n",
    "        minimal_index = indices_separator[index]+1\n",
    "        maximal_index = indices_separator[index+1]\n",
    "        \n",
    "        if minimal_index == maximal_index:\n",
    "            continue\n",
    "        \n",
    "        # Name of the source in the MPC-formatted input file (transient number)\n",
    "        transient_number = astcheck_file_content[minimal_index:maximal_index][\n",
    "            0].split(\":\")[0].split()\n",
    "        # Lines corresponding to matches in the astcheck output file\n",
    "        matches = astcheck_file_content[minimal_index:maximal_index][1:]\n",
    "        \n",
    "        if not matches: #Empty list\n",
    "            continue\n",
    "        \n",
    "        N_det += 1\n",
    "        \n",
    "        # If a source is matched to multiple solar system objects, assign the\n",
    "        # matches a flag of 1\n",
    "        if len(matches)>1:\n",
    "            initial_flag = 1\n",
    "        else:\n",
    "            initial_flag = 0\n",
    "        \n",
    "        # Get properties of matches\n",
    "        for i_match in range(len(matches)):\n",
    "            match_properties = re.split(\" +\", matches[i_match])\n",
    "            match_properties = [x for x in match_properties if len(x) > 0]\n",
    "            identifier = match_properties[0]\n",
    "            try:\n",
    "                int(match_properties[1])\n",
    "                offset_ra, offset_dec, offset, magnitude = match_properties[1:5]\n",
    "            except ValueError:\n",
    "                identifier = \" \".join([identifier, match_properties[1]])\n",
    "                offset_ra, offset_dec, offset, magnitude = match_properties[2:6]\n",
    "            \n",
    "            try:\n",
    "                magnitude = float(magnitude)\n",
    "            except ValueError:\n",
    "                LOG.warning(\"Magnitude '{}' could not be converted to float.\"\n",
    "                            .format(magnitude))\n",
    "                magnitude = None\n",
    "        \n",
    "            # Add match to output table\n",
    "            output_row = (transient_number, str(identifier), float(offset_ra),\n",
    "                          float(offset_dec), float(offset), magnitude, initial_flag)\n",
    "            output_table.add_row(output_row)\n",
    "    \n",
    "    # If a solar system object was matched to multiple transient sources in the\n",
    "    # image, assign it a flag of 2\n",
    "    unique_objects = np.unique(output_table[\"ID_SSO\"])\n",
    "    if len(unique_objects) != len(output_table):\n",
    "        for obj in unique_objects:\n",
    "            obj_indices = np.where(output_table[\"ID_SSO\"] == obj)[0]\n",
    "            if len(obj_indices) > 1:\n",
    "                output_table[\"FLAGS_SSO\"][obj_indices] += 2\n",
    "    \n",
    "    # Set dummy parameter (dummy means that there were no matches found)\n",
    "    dummy = False\n",
    "    if not output_table: #Empty table\n",
    "        dummy = True\n",
    "    \n",
    "    # Create header for SSO catalogue and save catalogue\n",
    "    sso_header = create_sso_header(rundir, N_det, N_sso, dummy, True)\n",
    "    fitstable = format_cat(output_table, start_header=sso_header)\n",
    "    save_fits(fitstable, sso_cat, rundir=rundir)\n",
    "    \n",
    "    LOG.info(\"Matches saved to SSO catalogue: {}\".format(sso_cat))\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"create_sso_catalogue\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MPC_report(sso_cat, mpcformat_file, reportname, rundir, mpc_code):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make an MPC report using the SSO catalogue and the MPC-formatted file that\n",
    "    were created within match2SSO to link the transient detections from a single\n",
    "    catalogue to known solar system objects. The detections corresponding to\n",
    "    matches are grouped in a 'known objects report'. The identifiers used in the\n",
    "    report are the packed designations of the matching objects. These are the\n",
    "    packed permanent designations if available. Otherwise, the packed\n",
    "    provisional designations are used.\n",
    "    \n",
    "    The MPC report will be compiled in a temporary folder and the complete file\n",
    "    will be moved to the reports folder at the end of the function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sso_cat: string\n",
    "        Path to and name of the SSO catalogue of which the matches need to be\n",
    "        converted to a MPC report.\n",
    "    mpcformat_file: string\n",
    "        Name of the MPC formatted file that was made in match2SSO for the\n",
    "        matching (but does not contain the correct SSO identifiers yet for\n",
    "        reporting to the MPC).\n",
    "    reportname: string\n",
    "        Name of the MPC report that will be made from the SSO catalogue. The\n",
    "        report should have the extension \".txt\".\n",
    "    rundir: string\n",
    "        Run directory in which the MPC report will be created, before being\n",
    "        moved to its proper destination as given in the [sso_cat] path.\n",
    "    mpc_code: string\n",
    "        MPC code corresponding to the telescope.\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of create_MPC_report\")\n",
    "    LOG.info(\"Creating MPC report...\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    # Compose temporary report name using run directory as path\n",
    "    destination = os.path.dirname(reportname)\n",
    "    report_basename = os.path.basename(reportname).replace(\n",
    "        \".txt\", \"_{}.txt\".format(Time.now().strftime(\"%Y%m%dT%H%M%S\")))\n",
    "    reportname = rundir + report_basename\n",
    "    destination_file = \"/\".join([destination, report_basename])\n",
    "    \n",
    "    # Check if file already exists (will only happen when running this function\n",
    "    # multiple times in close succession, as the production time is used in the\n",
    "    # file name)\n",
    "    if not OVERWRITE_FILES and isfile(destination_file):\n",
    "        LOG.info(\"MPC report already exists and will not be re-made.\")\n",
    "        return\n",
    "    \n",
    "    # Open SSO catalogue\n",
    "    with fits.open(sso_cat) as hdu:\n",
    "        sso_cat_content = Table(hdu[1].data)\n",
    "    \n",
    "    # Check SSO catalogue for matches\n",
    "    if not sso_cat_content:\n",
    "        LOG.info(\"No matches found. MPC report will not be made.\")\n",
    "        return\n",
    "    \n",
    "    # Create MPC report\n",
    "    LOG.info(\"Writing report {}\".format(reportname))\n",
    "    report_content = open(reportname, \"w\")\n",
    "    \n",
    "    # Write header to the report\n",
    "    report_content.write(create_report_header(reportname, mpc_code))\n",
    "    \n",
    "    # Open MPC-formatted file as the MPC report will be very similar, only with\n",
    "    # MPC designations rather than transient numbers as the first column. For a\n",
    "    # large part, the MPC-formatted file content will therefore be copied over.\n",
    "    detections_mpcformat = pd.read_fwf(mpcformat_file, widths=[14, 66],\n",
    "                                       names=[\"char1to14\", \"char15to80\"],\n",
    "                                       dtype={\"char1to14\":np.int32, \n",
    "                                              \"char15to80\":str})\n",
    "    \n",
    "    # For each detection that was matched to a known solar system object,\n",
    "    # get the packed designation of the matching object and write the detection\n",
    "    # to the report\n",
    "    for match_index in range(len(sso_cat_content[NUMBER_COLUMN])):\n",
    "        designation = sso_cat_content[\"ID_SSO\"][match_index].strip()\n",
    "        \n",
    "        # Start creating the line of the report corresponding to the detection,\n",
    "        # by adding the packed designation of the object to the line\n",
    "        if re.match(r\"^[0-9]{4}\\s[A-Z]\", designation) or \"/\" in designation:\n",
    "            # Provisional or survey designation\n",
    "            packed_designation = wrapper_pack_provisional_designation(\n",
    "                designation)\n",
    "            if packed_designation is None:\n",
    "                continue\n",
    "            detection_line = \"    {}  \".format(packed_designation)\n",
    "        \n",
    "        else:\n",
    "            # Asteroid or comet with permanent designation\n",
    "            packed_designation, fragment = pack_permanent_designation(\n",
    "                designation)\n",
    "            if packed_designation is None:\n",
    "                continue\n",
    "            detection_line = \"{}{}  \".format(packed_designation,\n",
    "                                             fragment.rjust(7))\n",
    "        \n",
    "        # Get the detection details from the MPC-formatted file and add to the\n",
    "        # line of the MPC report corresponding to the detection\n",
    "        detection_index = np.where(\n",
    "            np.array(detections_mpcformat[\"char1to14\"])\n",
    "            == int(sso_cat_content[NUMBER_COLUMN][match_index]))[0]\n",
    "        if len(detection_index) != 1:\n",
    "            LOG.error(\"{} detections found that correspond to transient number\"\n",
    "                      \" {}. Should be only one.\".format(len(detection_index),\n",
    "                      int(sso_cat_content[NUMBER_COLUMN][match_index])))\n",
    "            continue\n",
    "        detection_index = detection_index[0]\n",
    "        detection_line = \"\".join([detection_line, detections_mpcformat[\n",
    "            \"char15to80\"][detection_index]])\n",
    "        \n",
    "        # Check line corresponding to detection and write to the MPC report if\n",
    "        # all is well\n",
    "        if len(detection_line) != 80:\n",
    "            LOG.error(\"Detection not formatted correctly in 80 columns:\\n{}\"\n",
    "                      .format(detection_line))\n",
    "        report_content.write(detection_line+\"\\n\")\n",
    "    \n",
    "    report_content.close()\n",
    "    \n",
    "    # Move report from run directory to final destination\n",
    "    LOG.info(\"Moving report to {}\".format(destination))\n",
    "    copy_file(reportname, destination_file, move=True)\n",
    "    if isfile(destination_file):\n",
    "        LOG.warning(\"MPC report {} is overwritten.\".format(destination_file))\n",
    "    else:\n",
    "        LOG.info(\"MPC report saved to {}\".format(destination_file))\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"create_MPC_report\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_header(reportname, mpc_code, comment=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function composes the header of the MPC report corresponding to a single\n",
    "    transient catalogue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    reportname: string\n",
    "        Name of the MPC report for which the header is composed.\n",
    "    mpc_code: string\n",
    "        MPC code of the telescope with which the observation was made.\n",
    "    comment: string\n",
    "        Comment to be added to the header in the COM line. By default, this is\n",
    "        None, meaning that the COM line is not added to the header.\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of create_report_header\")\n",
    "    LOG.info(\"Creating header for MPC report...\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    firstline = \"COD {}\\n\".format(mpc_code)\n",
    "    mainheader = get_par(settingsFile.MPCreportHeader, mpc_code)\n",
    "    \n",
    "    # Special cases for which a phrase needs to be included in the ACK line\n",
    "    # of the header of the MPC report:\n",
    "    # neocand = \"NEO CANDIDATE\" #submitting new NEO candidate\n",
    "    # neocp = \"NEOCP\"           #submitting observations of NEOCP objects\n",
    "    \n",
    "    # Add ACK line to the header of the MPC report.\n",
    "    ack_line = \"ACK {}\\n\".format(Path(reportname).stem)\n",
    "    if len(ack_line) > 82:\n",
    "        LOG.error(\"ACK line in report {} is too long!\".format(reportname))\n",
    "    \n",
    "    # Add COM line to the header\n",
    "    com_line = \"\"\n",
    "    if comment is not None:\n",
    "        if len(comment) > 76:\n",
    "            LOG.warning(\"COM line is too long and therefore not used. \"\n",
    "                        \"Use at most 76 characters!\")\n",
    "        else:\n",
    "            com_line = \"COM {}\\n\".format(comment)\n",
    "    \n",
    "    LOG.info(\"MPC report header complete.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"create_report_header\")\n",
    "    \n",
    "    return \"\".join([firstline, mainheader, ack_line, com_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subroutines\n",
    "The functions below are subroutines, which we split up in\n",
    "- subroutines to support selecting input catalogues\n",
    "- subroutines to pack SSO designations\n",
    "- general subroutines for checks and set-up\n",
    "- subroutines to support Google Cloud bucket systems\n",
    "- subroutines for formatting & saving output and removing temporary files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subroutines to support selecting input catalogues\n",
    "Support selecting the correct input transient catalogues to run match2SSO on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transient_filenames(input_folder, minimal_date, maximal_date, tel,\n",
    "                            exclude_flagged=False):\n",
    "    \"\"\"\n",
    "    Function returns a list with the transient file names that were taken\n",
    "    between the minimal and maximal specified dates with the specified [tel]\n",
    "    telescope. This function works for MeerLICHT and BlackGEM data and runs on\n",
    "    the lighter version of the transient catalogues if available. Otherwise it\n",
    "    runs on the 'standard' transient catalogues. If exclude_flagged is True, the\n",
    "    dummy transient catalogues (which are red-flagged) are excluded.\n",
    "    \n",
    "    Function assumes a directory and filename structure and hence might not be\n",
    "    directly applicable to other telescopes than MeerLICHT & BlackGEM.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_folder: string\n",
    "        Folder which contains the yyyy/mm/dd/ folders in which the transient\n",
    "        catalogues are stored.\n",
    "    minimal_date: datetime object, incl time zone\n",
    "        Minimal observation date of the time block for which the observations\n",
    "        are selected.\n",
    "    maximal_date: datetime object, incl time zone\n",
    "        Maximal observation date of the time block for which the observations\n",
    "        are selected.\n",
    "    tel: string\n",
    "        Telescope abbreviation.\n",
    "    exclude_flagged: boolean\n",
    "        Boolean indicating whether red-flagged (dummy) catalogues should be\n",
    "        excluded or not.\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of get_transient_filenames\")\n",
    "    LOG.info(\"Selecting transient catalogues between {} and {}.\"\n",
    "             .format(minimal_date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                     maximal_date.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    # Convert to local time\n",
    "    local_timezone = timezone(get_par(settingsFile.timeZoneTelescope, tel))\n",
    "    minimal_date = minimal_date.astimezone(local_timezone)\n",
    "    maximal_date = maximal_date.astimezone(local_timezone)\n",
    "    \n",
    "    # Select the transient directories from observation date\n",
    "    search_path = input_folder\n",
    "    if minimal_date.year == maximal_date.year:\n",
    "        yeardir = \"%d/\"%(minimal_date.year)\n",
    "        search_path += yeardir\n",
    "        if minimal_date.month == maximal_date.month:\n",
    "            monthdir = \"{:0>2}/\".format(minimal_date.month)\n",
    "            search_path += monthdir\n",
    "            \n",
    "            # Get observing date (defined to start at noon and end at noon the\n",
    "            # next day)\n",
    "            minday = minimal_date.day\n",
    "            if minday.hour < 12.:\n",
    "                minday -= 1\n",
    "            maxday = maximal_date.day\n",
    "            if maxday.hour < 12.:\n",
    "                maxday -= 1\n",
    "            \n",
    "            if minday == maxday:\n",
    "                daydir = \"{:0>2}/\".format(maxday)\n",
    "                search_path += daydir\n",
    "    \n",
    "    # Infer folder depth to use\n",
    "    if 'gs://' in input_folder:\n",
    "        bucket_name, __ = get_bucket_name(input_folder)\n",
    "        folder_tmp = input_folder.split(bucket_name)[-1]\n",
    "        depth = folder_tmp.rstrip('/').count('/')\n",
    "    else:\n",
    "        depth = input_folder.rstrip('/').count('/')\n",
    "    depth += 3 # Adds 3 to the depth for the yyyy/mm/dd folders\n",
    "    \n",
    "    # Use function [list_folders] to list all reduced data folders that will be\n",
    "    # searched for transient catalogues\n",
    "    list_paths = list_folders(search_path, depth=depth)\n",
    "    \n",
    "    # List transient catalogues in the selected folders. List the lighter\n",
    "    # version of the catalogue if it exists. Otherwise list the 'standard'\n",
    "    # version.\n",
    "    transient_files = []\n",
    "    for reddir in list_paths:\n",
    "        lighttransnames = list_files(reddir, end_str=\"_trans_light.fits\")\n",
    "        transient_files.extend(lighttransnames)\n",
    "        \n",
    "        transnames = list_files(reddir, end_str=\"_trans.fits\")\n",
    "        for transname in transnames:\n",
    "            if transname.replace(\".fits\", \"_light.fits\") not in lighttransnames:\n",
    "                transient_files.append(transname)\n",
    "    transient_files = sorted(transient_files)\n",
    "    \n",
    "    if not transient_files:\n",
    "        return []\n",
    "    \n",
    "    files2process = []\n",
    "    for transient_cat in transient_files:\n",
    "        # Parse date encoded in filename and compare with our limits\n",
    "        # (e.g. ML1_20200517_034221_red_trans_light.fits)\n",
    "        splitted_filename = os.path.basename(transient_cat).split(\"_\")\n",
    "        date_obs = splitted_filename[1]\n",
    "        time_obs = splitted_filename[2]\n",
    "        observation_time = Time.strptime(date_obs+time_obs, \"%Y%m%d%H%M%S\").mjd\n",
    "        if (observation_time >= Time(minimal_date).mjd and\n",
    "                observation_time <= Time(maximal_date).mjd):\n",
    "            with fits.open(transient_cat) as hdu:\n",
    "                header = hdu[1].header\n",
    "            \n",
    "            if not exclude_flagged:\n",
    "                files2process.append(transient_cat)\n",
    "            else:\n",
    "                LOG.info(\"Excluding red-flagged (dummy) catalogues.\")\n",
    "                \n",
    "                if DUMMY_KEYWORD not in header.keys():\n",
    "                    LOG.critical(\"{} not in the header!\".format(DUMMY_KEYWORD))\n",
    "                    return []\n",
    "                \n",
    "                if not header[DUMMY_KEYWORD]:\n",
    "                    files2process.append(transient_cat)\n",
    "    \n",
    "    LOG.info(\"{} transient catalogues have been selected.\"\n",
    "             .format(len(files2process)))\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label=\"get_transient_filenames\")\n",
    "    return files2process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_night_start_from_date(cat_name, tel, noon_type=\"local\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the noon corresponding to the start of the\n",
    "    observation night, as a datetime object. This is either the local noon or\n",
    "    the noon in UTC, as specified. The noon is deduced from the catalogue\n",
    "    header.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cat_name: string\n",
    "        Name of the catalogue corresponding to an observation that took place\n",
    "        on the observation night for which the noon that signifies the start of\n",
    "        the night must be determined.\n",
    "    tel: string\n",
    "        Telescope abbreviation.\n",
    "    noon_type: string\n",
    "        Must be either \"local\" or \"utc\". If \"utc\", this function will return\n",
    "        the noon corresponding to the start of the night in UTC. This can be\n",
    "        different from the local noon.\n",
    "    \"\"\"\n",
    "    noon_type = noon_type.lower()\n",
    "    \n",
    "    # Get observation time from catalogue header and define as being in UTC\n",
    "    with fits.open(cat_name) as hdu:\n",
    "        hdr = hdu[1].header\n",
    "    \n",
    "    observation_time = pytz.utc.localize(Time(hdr[DATE_KEYWORD],\n",
    "                                              format=\"isot\").datetime)\n",
    "    observation_date = str(observation_time.date())\n",
    "    \n",
    "    if noon_type == \"local\":\n",
    "        local_timezone = timezone(get_par(settingsFile.timeZoneTelescope, tel))\n",
    "        \n",
    "        # Get local noon corresponding to the start of the observing night\n",
    "        local_noon = local_timezone.localize(datetime.strptime(\" \".join([\n",
    "            observation_date, \"120000\"]), \"%Y-%m-%d %H%M%S\"))\n",
    "        # Get date of observing night\n",
    "        if observation_time < local_noon:\n",
    "            date = (observation_time - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "        else:\n",
    "            date = observation_time.strftime(\"%Y%m%d\")\n",
    "        \n",
    "        # Make local noon variable\n",
    "        night_start = local_timezone.localize(datetime.strptime(\" \".join([\n",
    "            date, \"120000\"]), \"%Y%m%d %H%M%S\"))\n",
    "    else:\n",
    "        if noon_type != \"utc\":\n",
    "            LOG.error(\"Noon type not understood. Assuming noon in utc.\")\n",
    "        \n",
    "        night_start = pytz.utc.localize(datetime.strptime(\" \".join([\n",
    "            observation_date, \"120000\"]), \"%Y-%m-%d %H%M%S\"))\n",
    "        if int(observation_time.hour) < 12.:\n",
    "            night_start -= timedelta(days=1)\n",
    "    \n",
    "    return night_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subroutines for SSO designation packing\n",
    "Pack asteroid & comet designations for the MPC reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_permanent_designation(full_designation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function converts the permanent minor planet designation into its packed\n",
    "    form (5 characters), using the definitions given in\n",
    "    https://www.minorplanetcenter.net/iau/info/PackedDes.html#perm\n",
    "    Return the packed designation and - if applicable - the letter\n",
    "    corresponding to the comet fragment. If the object is not a comet fragment,\n",
    "    an empty string will be returned for the fragment letter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    full_designation: string\n",
    "        Unpacked permanent designation assigned to the object by the MPC.\n",
    "    \"\"\"\n",
    "    fragment = \"\"\n",
    "    \n",
    "    if not full_designation.isdigit():\n",
    "        # Object is a comet\n",
    "        if len(full_designation.split(\"-\")) == 2:\n",
    "            designation, fragment = full_designation.split(\"-\")\n",
    "        else:\n",
    "            designation = full_designation\n",
    "            fragment = \"\"\n",
    "        packed_designation = designation.zfill(5)\n",
    "        fragment = fragment.lower()\n",
    "    \n",
    "    elif int(full_designation) < 99999:\n",
    "        packed_designation = \"{:0>5}\".format(int(full_designation))\n",
    "    \n",
    "    elif int(full_designation) < 620000:\n",
    "        quotient = int(full_designation)//10000\n",
    "        packed_designation = \"{}{:0>4}\".format(abbreviate_number(quotient),\n",
    "                                               int(full_designation)%10000)\n",
    "    else:\n",
    "        remainder = int(full_designation) - 620000\n",
    "        quotient3 = remainder//62**3\n",
    "        remainder -= quotient3*62**3\n",
    "        quotient2 = remainder//62**2\n",
    "        remainder -= quotient2*62**2\n",
    "        quotient1 = remainder//62\n",
    "        remainder -= quotient1*62\n",
    "        packed_designation = \"~{}{}{}{}\".format(abbreviate_number(quotient3),\n",
    "                                                abbreviate_number(quotient2),\n",
    "                                                abbreviate_number(quotient1),\n",
    "                                                abbreviate_number(remainder))\n",
    "    # Final check\n",
    "    if len(packed_designation) != 5:\n",
    "        LOG.error(\"Packed permanent designation is of incorrect length: '{}'\"\n",
    "                  .format(packed_designation))\n",
    "        return None, \"\"\n",
    "    \n",
    "    return packed_designation, fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_pack_provisional_designation(full_designation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Wrapper function for packing provisional minor planet designations into\n",
    "    their packed forms. The function first checks whether [full_designation] is\n",
    "    a survey designation. If so, it is packed using the definitions in\n",
    "    https://www.minorplanetcenter.net/iau/info/PackedDes.html#prov\n",
    "    \n",
    "    If the designation is not a survey designation, the function determines\n",
    "    whether it belongs to a comet or an asteroid. For comets, we call the\n",
    "    function pack_provisional_designation_comet. For asteroids, we call\n",
    "    pack_provisional_designation_asteroid.\n",
    "    \n",
    "    The function returns an 8-character long string, spanning columns 5-12 in\n",
    "    the MPC report, or None in case of an issue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    full_designation: string\n",
    "        Unpacked provisional designation assigned to the object by the MPC.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove space before or after designation\n",
    "    full_designation = full_designation.strip()\n",
    "    \n",
    "    # There are four special survey designation forms (for surveys that were\n",
    "    # undertaken between 1960 and 1977) that should be packed differently\n",
    "    survey_strings = (\"P-L\", \"T-1\", \"T-2\", \"T-3\")\n",
    "    for survey_string in survey_strings:\n",
    "        if (re.match(r\"^[0-9]{4}\\s[A-Z]\", full_designation)\n",
    "                and full_designation.endswith(survey_string)):\n",
    "            packed_designation = \"{}S{}\".format(survey_string.replace(\"-\", \"\"),\n",
    "                                                full_designation[:4])\n",
    "            return packed_designation\n",
    "    \n",
    "    pack_year = {18: \"I\", 19: \"J\", 20: \"K\"}\n",
    "    \n",
    "    # For a comet\n",
    "    if \"/\" in full_designation:\n",
    "        packed_designation = pack_provisional_designation_comet(\n",
    "            full_designation, pack_year)\n",
    "        return packed_designation\n",
    "    \n",
    "    # For an asteroid\n",
    "    packed_designation = pack_provisional_designation_asteroid(\n",
    "        full_designation, pack_year)\n",
    "    \n",
    "    return packed_designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_provisional_designation_asteroid(full_designation, pack_year):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function converts the provisional asteroid designation into its packed form\n",
    "    using the definitions given in\n",
    "    https://www.minorplanetcenter.net/iau/info/PackedDes.html#prov\n",
    "    We add a space in front so that the returned string is 8 characters long,\n",
    "    spanning columns 5-12 in the MPC report. The function returns None in\n",
    "    case of an issue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    full_designation: string\n",
    "        Unpacked provisional designation assigned to the asteroid by the MPC.\n",
    "    pack_year: dict\n",
    "        Dictionary needed to pack the first two numbers of the year (indicating\n",
    "        the century) into a single letter, according to the MPC standard.\n",
    "    \"\"\"\n",
    "    \n",
    "    if int(full_designation[:2]) not in pack_year.keys():\n",
    "        LOG.error(\"Provisional designation of asteroid {} cannot be packed. \"\n",
    "                  \"Skipping it.\".format(full_designation))\n",
    "        return None\n",
    "    \n",
    "    packed_year = \"{}{}\".format(pack_year[int(full_designation[:2])],\n",
    "                                full_designation[2:4])\n",
    "    packed_designation = (\" {}{}{}{}\"\n",
    "                          .format(packed_year, full_designation[5],\n",
    "                                  pack_cycle_number(full_designation[7:]),\n",
    "                                  full_designation[6]))\n",
    "    # Final check\n",
    "    if len(packed_designation) != 8:\n",
    "        LOG.error(\"Packed provisional designation is of incorrect length: \"\n",
    "                  \"'{}'\".format(packed_designation))\n",
    "        return None\n",
    "    \n",
    "    return packed_designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_provisional_designation_comet(full_designation, pack_year):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function converts the provisional comet designation into its packed form\n",
    "    using the definitions given in\n",
    "    https://www.minorplanetcenter.net/iau/info/PackedDes.html#prov\n",
    "    As described in https://www.minorplanetcenter.net/iau/info/OpticalObs.html,\n",
    "    for comets a character is added in front of the provisional designation (at\n",
    "    column 5), describing the comet type.\n",
    "    The function returns an 8-character long string, spanning columns 5-12 in\n",
    "    the MPC report, or None in case of an issue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    full_designation: string\n",
    "        Unpacked provisional designation assigned to the comet by the MPC.\n",
    "    pack_year: dict\n",
    "        Dictionary needed to pack the first two numbers of the year (indicating\n",
    "        the century) into a single letter, according to the MPC standard.\n",
    "    \"\"\"\n",
    "    \n",
    "    comet_type, designation = full_designation.split(\"/\")\n",
    "    \n",
    "    # In case of a comet fragment, the last character of the packed designation\n",
    "    # is the fragment letter. Otherwise, it is zero.\n",
    "    fragment = \"0\"\n",
    "    if \"-\" in designation:\n",
    "        designation, fragment = designation.split(\"-\")\n",
    "        fragment = fragment.lower()\n",
    "    year, remainder = designation.split(\" \")\n",
    "        \n",
    "    if int(year[:2]) not in pack_year.keys():\n",
    "        LOG.error(\"Provisional designation of comet {} cannot be packed. \"\n",
    "                  \"Skipping it.\".format(full_designation))\n",
    "        return None\n",
    "    \n",
    "    packed_year = \"{}{}\".format(pack_year[int(year[:2])], year[2:])\n",
    "    \n",
    "    # In case there are two letters after the space in the designation. This\n",
    "    # can be the case if the object was thought to be an asteroid early on.\n",
    "    if remainder[1].isalpha():\n",
    "        \n",
    "        if fragment != \"0\":\n",
    "            # A comet with two letters in its provisional designation after\n",
    "            # the space and a fragment letter cannot be submitted in the old\n",
    "            # report format. It can in the new ADES format, but we are\n",
    "            # not yet using this. Skip detection.\n",
    "            LOG.error(\"Provisional designation of comet {} cannot be packed.\"\n",
    "                      \"Skipping it.\".format(full_designation))\n",
    "            return None\n",
    "        \n",
    "        # Although this object is not a fragment, its provisional designation\n",
    "        # does contain a second letter after the space which should be written\n",
    "        # to the same position as the fragment letter.\n",
    "        fragment = remainder[1]\n",
    "        remainder = \"{}{}\".format(remainder[0], remainder[2:])\n",
    "    \n",
    "    # There should be at most three digits after the space-letter combination\n",
    "    # in the provisional designation.\n",
    "    if len(remainder) > 4:\n",
    "        LOG.error(\"Unclear how to pack provisional designation of comet {}. \"\n",
    "                  \"Skipping it.\".format(full_designation))\n",
    "        return None\n",
    "    \n",
    "    if int(year[:2]) not in pack_year.keys():\n",
    "        LOG.error(\"Data from before 1800 or after 2099 cannot be assigned a \"\n",
    "                  \"packed provisional designation.\")\n",
    "        return None\n",
    "    \n",
    "    packed_designation = (\"{}{}{}{}{}\"\n",
    "                          .format(comet_type, packed_year, remainder[0],\n",
    "                                  pack_cycle_number(remainder[1:]), fragment))\n",
    "    # Final check\n",
    "    if len(packed_designation) != 8:\n",
    "        LOG.error(\"Packed provisional designation is of incorrect length: \"\n",
    "                  \"'{}'\".format(packed_designation))\n",
    "        return None\n",
    "    \n",
    "    return packed_designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate_number(num):\n",
    "    \n",
    "    \"\"\"\n",
    "    Number packing function needed to pack MPC designations.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_dict = {str(index): letter for index, letter in \n",
    "                enumerate(\"\".join([ascii_uppercase, ascii_lowercase]), start=10)}\n",
    "    \n",
    "    if int(num) > 9:\n",
    "        return num_dict[str(num)]\n",
    "    \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_cycle_number(number_of_cycles):\n",
    "    \n",
    "    \"\"\"Input parameter number_of_cycles is a string of 0-3 digits.\"\"\"\n",
    "    \n",
    "    if not number_of_cycles: #Empty string\n",
    "        return \"00\"\n",
    "    \n",
    "    if int(number_of_cycles) > 99:\n",
    "        return \"{}{}\".format(abbreviate_number(number_of_cycles[0:2]),\n",
    "                             number_of_cycles[2])\n",
    "    \n",
    "    return \"{:0>2}\".format(number_of_cycles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General subroutines for checks and set-up\n",
    "Subroutines that support match2SSO's functionality by checking parameters, files, folders and software versions; setting up logging and time & memory tracking and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input_parameters(mode, cat2process, date2process, list2process):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if the correct (combination of) input parameters was/were defined for\n",
    "    run_match2SSO. If so, this function returns True. Otherwise, False is\n",
    "    returned.\n",
    "    \"\"\"\n",
    "    all_good = True\n",
    "    \n",
    "    # General checks on the input parameters\n",
    "    param_list = (date2process, cat2process, list2process)\n",
    "    param_num_none = sum([isinstance(par, type(None)) for par in param_list])\n",
    "    \n",
    "    if  param_num_none < len(param_list)-1:\n",
    "        print(\"CRITICAL: either specify --date, --catalog OR --catlist. A \"\n",
    "              \"combination is not allowed.\")\n",
    "        all_good = False\n",
    "    \n",
    "    if mode not in [\"day\", \"night\", \"historic\", \"hist\"]:\n",
    "        print(\"CRITICAL: unknown mode.\")\n",
    "        all_good = False\n",
    "    \n",
    "    # Checks per mode\n",
    "    if mode == \"day\":\n",
    "        if cat2process is not None or list2process is not None:\n",
    "            print(\"CRITICAL: the day mode cannot be combined with the \"\n",
    "                  \"--catalog or --catlist arguments.\")\n",
    "            all_good = False\n",
    "    \n",
    "    elif mode == \"night\":\n",
    "        if cat2process is None:\n",
    "            print(\"CRITICAL: --catalog needs to be specified when running \"\n",
    "                  \"match2SSO in night mode.\")\n",
    "            all_good = False\n",
    "        if date2process is not None or list2process is not None:\n",
    "            print(\"CRITICAL: the night mode cannot be combined with the \"\n",
    "                  \"--date or --catlist arguments.\")\n",
    "            all_good = False\n",
    "    \n",
    "    elif (mode == \"historic\" or \"hist\") and param_num_none == len(param_list):\n",
    "        print(\"CRITICAL: --date, --catalog and --catlist are all None. Nothing\"\n",
    "              \" to process.\")\n",
    "        all_good = False\n",
    "    \n",
    "    # Check on the existence of the specified input\n",
    "    if cat2process is not None and not isfile(cat2process):\n",
    "        print(\"CRITICAL: the specified catalog does not exist.\")\n",
    "        all_good = False\n",
    "    \n",
    "    if list2process is not None and not isfile(list2process):\n",
    "        print(\"CRITICAL: the specified catalog list does not exist.\")\n",
    "        all_good = False\n",
    "    \n",
    "    return all_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_check_folders(tel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function loads the folders specified in the settings file and checks\n",
    "    whether they end with a slash. In addition, checks on the existence of the\n",
    "    folders are performed. A tuple of the folder names is returned. The returned\n",
    "    tuple is empty if there was an issue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tel: string\n",
    "        Telescope abbreviation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load folders\n",
    "    input_folder = get_par(settingsFile.inputFolder, tel)\n",
    "    tmp_folder = get_par(settingsFile.tmpFolder, tel)\n",
    "    log_folder = get_par(settingsFile.logFolder, tel)\n",
    "    report_folder = get_par(settingsFile.MPCreportFolder, tel)\n",
    "    \n",
    "    # Check if folder names end with a slash\n",
    "    input_folder = check_folder_name(input_folder)\n",
    "    tmp_folder = check_folder_name(tmp_folder)\n",
    "    log_folder = check_folder_name(log_folder)\n",
    "    report_folder = check_folder_name(report_folder)\n",
    "    \n",
    "    # Check if critical folders exists. If not, return an empty list.\n",
    "    if not isdir(input_folder):\n",
    "        print(\"CRITICAL: input folder given in settings file doesn't exist\")\n",
    "        return ()\n",
    "    \n",
    "    # Create the other folders if they don't exist, except for the log folder\n",
    "    # as that is taken care of in the setup_logfile function.\n",
    "    if not os.path.isdir(tmp_folder):\n",
    "        os.makedirs(tmp_folder)\n",
    "    if report_folder[0:5] != 'gs://' and not isdir(report_folder):\n",
    "        os.makedirs(report_folder)\n",
    "    \n",
    "    folders = (input_folder, tmp_folder, log_folder, report_folder)\n",
    "    \n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder_name(directory_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function checks if directory name ends with a slash. If not, it is added.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If name is None or an empty string, don't check folder name\n",
    "    if not directory_name:\n",
    "        return directory_name\n",
    "    \n",
    "    if directory_name[-1] != \"/\":\n",
    "        directory_name = \"\".join([directory_name, \"/\"])\n",
    "    \n",
    "    return directory_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_settings():\n",
    "    \n",
    "    \"\"\"Function checks the parameters in the settings file for validity.\"\"\"\n",
    "    \n",
    "    # Check that astcheck parameters are numbers\n",
    "    if not isinstance(settingsFile.matchingRadius, (float, int)):\n",
    "        print(\"CRITICAL: incorrectly specified matching radius in settings \"\n",
    "              \"file. Must be float or integer.\")\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(settingsFile.limitingMagnitude, (float, int)):\n",
    "        print(\"CRITICAL: incorrectly specified limiting mag. in settings \"\n",
    "              \"file.\")\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(settingsFile.maximalNumberOfAsteroids, int):\n",
    "        print(\"CRITICAL: incorrectly specified max. number of asteroids in \"\n",
    "              \"settings file.\")\n",
    "        return False\n",
    "    \n",
    "    if (not isinstance(settingsFile.maxUncertainty, int) and\n",
    "            settingsFile.maxUncertainty is not None):\n",
    "        print(\"CRITICAL: incorrectly specified max. uncertainty in settings \"\n",
    "              \"file. Must be 0-9 or None.\")\n",
    "        return False\n",
    "    \n",
    "    # Check if JPL ephemeris file exists\n",
    "    if not isfile(FILE_JPLEPH):\n",
    "        print(\"CRITICAL: JPL ephemeris file specified in settings file doesn't\"\n",
    "              \" exist.\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logfile(logname, log_folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function creates log file and configures the log handler.\n",
    "    \"\"\"\n",
    "    \n",
    "    if logname is None:\n",
    "        return\n",
    "    \n",
    "    log_dir, log_filename = os.path.split(logname)\n",
    "    \n",
    "    # If no folder is specified, use the log folder from the settings file\n",
    "    if not log_dir:\n",
    "        if not log_folder:\n",
    "            print(\"Warning: no log folder specified. Log will be written to\"\n",
    "                  \" directory from which match2SSO was run!\")\n",
    "            log_folder=\".\"\n",
    "        log_dir = log_folder\n",
    "    \n",
    "    # Create folder to store log in, if it does not yet exist\n",
    "    if log_dir[0:5] != 'gs://' and not isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    # Configure log handling\n",
    "    log_file = \"{}/{}\".format(log_dir, log_filename)\n",
    "    if isfile(log_file):\n",
    "        file_path_and_name, extension = os.path.splitext(log_file)\n",
    "        log_file = \"{}_{}{}\".format(file_path_and_name,\n",
    "                                    Time.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "                                    extension)\n",
    "        print(\"Log file already exists. Creating a new log named {}\"\n",
    "              .format(log_file))\n",
    "    \n",
    "    file_handler = logging.FileHandler(log_file, \"a\")\n",
    "    file_handler.setFormatter(LOG_FORMATTER)\n",
    "    file_handler.setLevel(\"INFO\")\n",
    "    LOG.addHandler(file_handler)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function\n",
    "def log_timing_memory(t_in, label=\"\"):\n",
    "    \n",
    "    \"\"\"Function to report the time and memory spent in a function.\"\"\"\n",
    "    \n",
    "    LOG.info(\"wall-time spent in {}: {:.3f} s\".format(label, time.time()-t_in))\n",
    "    #mem_use(label=label)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function\n",
    "def mem_use(label=\"\"):\n",
    "    \n",
    "    \"\"\"Function keeps track of the memory usage.\"\"\"\n",
    "    \n",
    "    # ru_maxrss is in units of kilobytes on Linux; however, this seems\n",
    "    # to be OS dependent as on mac os it is in units of bytes; see\n",
    "    # manpages of \"getrusage\"\n",
    "    if sys.platform == \"darwin\":\n",
    "        norm = 1024**3\n",
    "    else:\n",
    "        norm = 1024**2\n",
    "    \n",
    "    mem_max = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/norm\n",
    "    mem_now = psutil.Process().memory_info().rss / 1024**3\n",
    "    mem_virt = psutil.Process().memory_info().vms / 1024**3\n",
    "    \n",
    "    LOG.info(\"memory use [GB]: rss={:.3f}, maxrss={:.3f}, vms={:.3f} in {}\"\n",
    "             .format(mem_now, mem_max, mem_virt, label))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function\n",
    "def get_par(par, tel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to check if [par] is a dictionary with one of the keys being [tel]\n",
    "    or the alphabetic part of [tel] (e.g. 'BG'), and if so, return the\n",
    "    corresponding value. Otherwise just return the parameter value.\n",
    "    \"\"\"\n",
    "    \n",
    "    par_val = par\n",
    "    if isinstance(par, dict):\n",
    "        if tel in par:\n",
    "            par_val = par[tel]\n",
    "        else:\n",
    "            # cut off digits from [tel]\n",
    "            tel_base = \"\".join([char for char in tel if char.isalpha()])\n",
    "            if tel_base in par:\n",
    "                par_val = par[tel_base]\n",
    "    \n",
    "    return par_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input_catalogue(cat_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if the input catalogue exists and if it is a dummy (red-flagged)\n",
    "    catalogue or not. If a light version of the catalogue is available, use\n",
    "    that version. This function returns a boolean for \"does the catalogue \n",
    "    exist?\", a boolean for \"is the catalogue a dummy?\" and the catalogue name\n",
    "    is returned, as the light version might have been selected instead of the\n",
    "    transient catalogue that includes the thumbnails.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check whether the (light) catalogue exists and ensure the use of the\n",
    "    # light version of the catalogue if it is available (better in terms of\n",
    "    # memory usage & processing speed)\n",
    "    if \"_light\" not in cat_name:\n",
    "        light_cat = cat_name.replace(\".fits\", \"_light.fits\")\n",
    "        if isfile(light_cat):\n",
    "            cat_name = light_cat\n",
    "    \n",
    "    if not isfile(cat_name):\n",
    "        LOG.critical(\"The specified catalog does not exist:\\n{}\"\n",
    "                     .format(cat_name))\n",
    "        return False, None, cat_name\n",
    "    \n",
    "    # Check quality control flag of the catalogue\n",
    "    with fits.open(cat_name) as hdu:\n",
    "        header = hdu[1].header\n",
    "    \n",
    "    if DUMMY_KEYWORD not in header.keys():\n",
    "        LOG.critical(\"{} not in the header of {}!\".format(DUMMY_KEYWORD,\n",
    "                                                          cat_name))\n",
    "        return False, None, cat_name\n",
    "    \n",
    "    if header[DUMMY_KEYWORD]:\n",
    "        LOG.info(\"{} is a dummy catalogue.\".format(cat_name))\n",
    "        return True, True, cat_name\n",
    "    \n",
    "    return True, False, cat_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_database_products(rundir):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function checks if the database products that astcheck needs in order\n",
    "    to process transient catalogues are located in the run directory. These are\n",
    "    the known objects catalogue, the symbolic link to the asteroid catalogue\n",
    "    (needed for reading out the asteroid database version) and ELEMENTS.COMET,\n",
    "    which is either an empty comet database (if comets should not be included\n",
    "    in the matching) or a symbolic link to the comet database used (again \n",
    "    needed to read out the version). The function also indirectly checks for\n",
    "    the existence of the run directory. A boolean is returned: True if all is\n",
    "    well and False if something is missing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rundir: string\n",
    "        Directory in which astcheck will be run.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for known objects catalogue\n",
    "    if not isfile(\"{}mpcorb.sof\".format(rundir)):\n",
    "        LOG.critical(\"The known objects catalogue (SOF format) could not be \"\n",
    "                     \"found.\")\n",
    "        return False\n",
    "    \n",
    "    # Check for symbolic links pointing to the used version of the SSO\n",
    "    # databases\n",
    "    if not isfile(\"{}MPCORB.DAT\".format(rundir)):\n",
    "        LOG.critical(\"MPCORB.DAT could not be found\")\n",
    "        return False\n",
    "    if not isfile(\"{}ELEMENTS.COMET\".format(rundir)):\n",
    "        LOG.critical(\"ELEMENTS.COMET could not be found\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_astcheck_header_and_footer(astcheck_file_content):\n",
    "    \n",
    "    \"\"\"\n",
    "    Before the -h switch was implemented in astcheck, the header and footer\n",
    "    needed to be removed manually. Check if astcheck has done this already, or\n",
    "    if manual removal is required. Return the content of the astcheck file\n",
    "    excluding the header and footer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The footer is variable in terms of the number of lines it spans, but it\n",
    "    # always starts with the footer_string as defined below and can hence be\n",
    "    # recognized by this string.\n",
    "    footer_string = \"The apparent motion and arc length\"\n",
    "    header_size = 5 # Number of header lines\n",
    "    footer_index = [index for index in range(len(astcheck_file_content)) if \\\n",
    "                   footer_string in astcheck_file_content[index]]\n",
    "    if footer_index:\n",
    "        return astcheck_file_content[header_size:footer_index[0]]\n",
    "    \n",
    "    else:\n",
    "        return astcheck_file_content[1:] # Just remove first empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_version(package_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function retrieves the string corresponding to the version of the software\n",
    "    package, from the versions.txt file listed in the settings file. In the\n",
    "    versions file, each line should correspond to a different package and should\n",
    "    contain the package name and the version string, separated by a space.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    package_name: string\n",
    "        Name of the software package, as listed in versions.txt\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        versions_file = open(settingsFile.versionsFile)\n",
    "        for line in versions_file:\n",
    "            if package_name in line:\n",
    "                line = line.replace(\"\\n\",\"\")\n",
    "                vers = line.split(\" \")[1]\n",
    "                break\n",
    "        versions_file.close()\n",
    "    except:\n",
    "        vers = None\n",
    "    \n",
    "    return vers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subroutines that support the use of Google Cloud systems\n",
    "The subroutines below make match2SSO compatible with a Google Cloud bucket system,\n",
    "whilst also keeping the compatibility with a Unix type file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function (for compatibility with Google Cloud system)\n",
    "def get_bucket_name(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Infer bucket- and filename from [path], which is expected to be\n",
    "    gs://[bucket name]/some/path/file or [bucket name]/some/path/file;\n",
    "    if [path] starts with a forward slash, empty strings will be returned\n",
    "    \"\"\"\n",
    "    \n",
    "    bucket_name = path.split('gs://')[-1].split('/')[0]\n",
    "    if len(bucket_name) > 0:\n",
    "        # N.B.: returning filename without the starting '/'\n",
    "        bucket_file = path.split(bucket_name)[-1][1:]\n",
    "    else:\n",
    "        bucket_file = ''\n",
    "    \n",
    "    return bucket_name, bucket_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function (for compatibility with Google Cloud system)\n",
    "def isdir(folder):\n",
    "    \n",
    "    if folder[0:5] == 'gs://':\n",
    "        if folder[-1] != '/':\n",
    "            folder = '{}/'.format(folder)\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name, bucket_file = get_bucket_name(folder)\n",
    "        blobs = storage_client.list_blobs(bucket_name, prefix=bucket_file,\n",
    "                                          max_results=1)\n",
    "        nblobs = len(list(blobs))\n",
    "        return nblobs > 0\n",
    "    \n",
    "    else:\n",
    "        return os.path.isdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function (for compatibility with Google Cloud system)\n",
    "def isfile(filename):\n",
    "    \n",
    "    if filename[0:5] == 'gs://':\n",
    "        storage_client = storage.Client()\n",
    "        bucket_name, bucket_file = get_bucket_name(filename)\n",
    "        # N.B.: bucket_file should not start with '/'\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(bucket_file)\n",
    "        return blob.exists()\n",
    "\n",
    "    else:\n",
    "        return os.path.isfile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function (for compatibility with Google Cloud system)\n",
    "def list_files(path, search_str='', end_str='', start_str=None,\n",
    "               recursive=False):\n",
    "    \"\"\"\n",
    "    Function to return a list of files starting with [path] (can be a folder or\n",
    "    google cloud bucket name and path; this does not have to be a precise or\n",
    "    complete folder/path, e.g. [path] can be some_path/some_file_basename), with\n",
    "    possible [end_str] and containing [search_str] without any wildcards. If\n",
    "    [path] is an exact folder, then [start_str] can be used as the beginning of\n",
    "    the filename.\n",
    "    Beware: if [end_str] is an empty string, subfolders will also be listed\n",
    "    among the files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Is google cloud being used?\n",
    "    google_cloud = (path[0:5] == 'gs://')\n",
    "    \n",
    "    # Split input [path] into folder_bucket and prefix; if path is a folder, we\n",
    "    # need to add a slash at the end. Otherwise the prefix will be the name of\n",
    "    # the deepest folder\n",
    "    if isdir(path) and path[-1] != '/':\n",
    "        path = '{}/'.format(path)\n",
    "    \n",
    "    # If path is indeed a folder, then prefix will be an empty string, which is\n",
    "    # fine below\n",
    "    folder_bucket, prefix = os.path.split(path.split('gs://')[-1])\n",
    "    \n",
    "    # If path consists of just the bucket name including gs:// or just a path\n",
    "    # without any slashes at all, the above will lead to an empty folder_bucket\n",
    "    # and prefix=path; turn these around\n",
    "    if len(folder_bucket) == 0:\n",
    "        folder_bucket = prefix\n",
    "        prefix = ''\n",
    "    \n",
    "    # If prefix is empty and [start] is defined, use\n",
    "    # that as the prefix\n",
    "    if prefix == '' and start_str is not None:\n",
    "        prefix = start_str\n",
    "    \n",
    "    # If not dealing with google cloud buckets, use glob\n",
    "    if not google_cloud:\n",
    "        if recursive:\n",
    "            files = glob.glob('{}/**/{}*{}*{}'.format(folder_bucket, prefix,\n",
    "                                                      search_str, end_str),\n",
    "                              recursive=True)\n",
    "            if path in files:\n",
    "                files.remove(path)\n",
    "        else:\n",
    "            files = glob.glob('{}/{}*{}*{}'.format(folder_bucket, prefix,\n",
    "                                                   search_str, end_str))\n",
    "    else:\n",
    "        # For buckets, use storage.Client().list_blobs; see\n",
    "        # https://cloud.google.com/storage/docs/samples/storage-list-files-with-prefix#storage_list_files_with_prefix-python\n",
    "        # setting delimiter to '/' restricts the results to only the\n",
    "        # files in a given folder\n",
    "        if recursive:\n",
    "            delimiter = None\n",
    "        else:\n",
    "            delimiter = '/'\n",
    "        \n",
    "        # Bucket name and prefix (e.g. gs://) to add to output files\n",
    "        bucket_name, bucket_file = get_bucket_name(path)\n",
    "        bucket_prefix = path.split(bucket_name)[0]\n",
    "        \n",
    "        if False:\n",
    "            LOG.info('folder_bucket: {}'.format(folder_bucket))\n",
    "            LOG.info('prefix: {}'.format(prefix))\n",
    "            LOG.info('path: {}'.format(path))\n",
    "            LOG.info('bucket_name: {}'.format(bucket_name))\n",
    "            LOG.info('bucket_file: {}'.format(bucket_file))\n",
    "        \n",
    "        # Get the blobs\n",
    "        storage_client = storage.Client()\n",
    "        blobs = storage_client.list_blobs(bucket_name, prefix=bucket_file,\n",
    "                                          delimiter=delimiter)\n",
    "        \n",
    "        # Collect blobs' names in list of files\n",
    "        files = []\n",
    "        for blob in blobs:\n",
    "            filename = blob.name\n",
    "            \n",
    "            # Check for search string; if search string is empty, the following\n",
    "            # if statement will be False\n",
    "            if search_str not in filename:\n",
    "                continue\n",
    "            \n",
    "            # Check if filename ends with [end_str]. If not, continue with next\n",
    "            # blob\n",
    "            len_ext = len(end_str)\n",
    "            if len_ext > 0 and filename[-len_ext:] != end_str:\n",
    "                continue\n",
    "            \n",
    "            # After surviving above checks, append filename including the bucket\n",
    "            # prefix and name\n",
    "            files.append('{}{}/{}'.format(bucket_prefix, bucket_name, filename))\n",
    "    \n",
    "    #LOG.info('number of files returned by [list_files]: {}'.format(len(files)))\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function (for compatibility with Google Cloud system)\n",
    "def list_folders(path, search_str='', depth=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to return list of existing folders starting with [path] (can be a\n",
    "    folder or google cloud bucket name and path; this does not have to be a\n",
    "    precise or complete folder/path, e.g. [path] can be \n",
    "    some_path/some_file_basename), optionally containing [search_str] without\n",
    "    any wildcards. [depth] determines how many folder depths are returned,\n",
    "    starting from the root directory; for google cloud buckets the root folder\n",
    "    (depth=1) is considered to start after the bucket name,\n",
    "    i.e. gs://[bucket name]/[root folder]/etc. If [depth] is not defined, all\n",
    "    existing folders are returned.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use function [list_files] to list all files containing 'search_str'\n",
    "    file_list = list_files(path, search_str=search_str, recursive=True)\n",
    "    \n",
    "    # Running in google cloud?\n",
    "    google_cloud = (path[0:5]=='gs://')\n",
    "    \n",
    "    # Remove potential 'gs://' and bucket name\n",
    "    if google_cloud:\n",
    "        bucket_name, __ = get_bucket_name(path)\n",
    "        \n",
    "        # Chop off 'gs://[bucket_name]'\n",
    "        file_list = [fn.split(bucket_name)[-1] for fn in file_list]\n",
    "    \n",
    "    if depth is None:\n",
    "        # List all files and folders in path\n",
    "        file_list = ['/'.join(fn.split('/')[:-1]) for fn in file_list]\n",
    "    else:\n",
    "        # Create list of files and folders with depth [depth]\n",
    "        file_list = ['/'.join(fn.split('/')[:depth+1]) for fn in file_list\n",
    "                     if len(fn.rstrip('/').split('/')) > depth]\n",
    "    \n",
    "    # Remove duplicate folders and sort\n",
    "    file_list = sorted(list(set(file_list)))\n",
    "    \n",
    "    # Append 'gs://[bucket_name]' if needed\n",
    "    if google_cloud:\n",
    "        file_list = ['gs://{}{}'.format(bucket_name, fn) for fn in file_list]\n",
    "    \n",
    "    # Only folders, not files\n",
    "    isfolder = [isdir(fn) for fn in file_list]\n",
    "    folder_list = np.array(file_list)[isfolder].tolist()\n",
    "    \n",
    "    LOG.info('folder_list: {}'.format(folder_list))\n",
    "    \n",
    "    return folder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BlackBOX function (for compatibility with Google Cloud system)\n",
    "def copy_file(src_file, dest, move=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to copy or move a file [src_file] to [dest], which may be a file or\n",
    "    folder; [src_file] and/or [dest] may be part of the usual filesystem or in a\n",
    "    google cloud bucket; in the latter case the argument(s) should start with\n",
    "    gs://[bucket_name]\n",
    "    \"\"\"\n",
    "    \n",
    "    if move:\n",
    "        label = 'moving'\n",
    "    else:\n",
    "        label = 'copying'\n",
    "    LOG.info('{} {} to {}'.format(label, src_file, dest))\n",
    "    \n",
    "    # If not dealing with google cloud buckets, use shutil.copy2 or shutil.move\n",
    "    if not (src_file[0:5] == 'gs://' or dest[0:5] == 'gs://'):\n",
    "        if not move:\n",
    "            shutil.copy2(src_file, dest)\n",
    "        else:\n",
    "            shutil.move(src_file, dest)\n",
    "    \n",
    "    else:\n",
    "        # This could be done in python, but much easier with gsutil from the\n",
    "        # shell\n",
    "        if move:\n",
    "            cp_cmd = 'mv'\n",
    "        else:\n",
    "            cp_cmd = 'cp'\n",
    "        \n",
    "        cmd = ['gsutil', '-q', cp_cmd, src_file, dest]\n",
    "        result = subprocess.run(cmd)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subroutines for formatting & saving output and removing temporary files\n",
    "Also includes functionality for Google cloud file systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_cat(data, header=None, header_keys=[], start_header=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function formats the output data, composes the header and combines the two\n",
    "    into a hdu fits table. The fits table is returned.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : table data\n",
    "        Astropy table data which is to be used as the data for the output fits\n",
    "        table.\n",
    "    header: header or None\n",
    "        Header from which certain keywords are copied to the header of the\n",
    "        output catalogue.\n",
    "    header_keys: list of strings\n",
    "        Contains names of header keywords from the header mentioned above, that\n",
    "        will be included in the output catalogue header. List can be empty, in\n",
    "        which case the fits header is either the start_header or empty.\n",
    "    start_header: header\n",
    "        Header which will be included in the header of the output catalogue.\n",
    "        start_header can be None.\n",
    "    \"\"\"\n",
    "    #mem_use(label=\"at start of format_cat\")\n",
    "    \n",
    "    # Format fits table data\n",
    "    columns = []\n",
    "    for column_name in data.columns:\n",
    "        \n",
    "        column_format = data[column_name].dtype\n",
    "        \n",
    "        # Converting bytestring format to fits format does not work properly\n",
    "        # for strings, as the length is not taken into account properly.\n",
    "        # Manually correct this.\n",
    "        if \"S\" in column_format.str:\n",
    "            string_length = column_format.str.split(\"S\")[-1]\n",
    "            column_format = \"{}A\".format(string_length)\n",
    "        \n",
    "        column_unit = str(data[column_name].unit)\n",
    "        if column_unit == \"None\":\n",
    "            column_unit = \"\"\n",
    "        \n",
    "        column = fits.Column(name=column_name, format=column_format,\n",
    "                             unit=column_unit, array=data[column_name])\n",
    "        columns.append(column)\n",
    "    \n",
    "    # Compose fits table header\n",
    "    if start_header is not None:\n",
    "        finalheader = start_header\n",
    "    else:\n",
    "        finalheader = fits.Header()\n",
    "    \n",
    "    for key in header_keys:\n",
    "        finalheader[key] = (header[key], header.comments[key])\n",
    "    \n",
    "    # Combine formatted fits columns and header into output binary fits table\n",
    "    fitstable = fits.BinTableHDU.from_columns(columns, header=finalheader)\n",
    "    \n",
    "    LOG.info(\"{} sources in catalogue\".format(len(fitstable)))\n",
    "    #mem_use(label=\"at end of format_cat\")\n",
    "    return fitstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on BlackBOX function [write_fits] (for compatibility with Google Cloud system)\n",
    "def save_fits(fitstable, outputname, rundir=\"\", overwrite=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function saves fits table in the directory specified in the string in\n",
    "    [outputname]. For the Google cloud, writing the file directly to a bucket\n",
    "    isn't possible. Instead, it's written to a folder [rundir] on a virtual\n",
    "    machine before being moved over to the bucket.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fitstable: binary table HDU object\n",
    "        Fits table (data + header) that needs to be saved to file.\n",
    "    outputname: string\n",
    "        Full path to and name of the output file.\n",
    "    rundir: string\n",
    "        Only used in case of a Google cloud file system. Name of the folder on\n",
    "        the VM where the fits table is temporarily written to.\n",
    "    overwrite: boolean\n",
    "        Determines whether fits table may be overwritten if it exists already.\n",
    "    \"\"\"\n",
    "    #mem_use(label='save_fits at start')\n",
    "    \n",
    "    # Dealing with google cloud bucket?\n",
    "    google_cloud = (outputname[0:5] == 'gs://')\n",
    "    \n",
    "    # Add time stamp of file creation to header\n",
    "    header = fitstable.header\n",
    "    header['DATEFILE'] = (Time.now().isot, 'UTC date of writing file')\n",
    "    data = fitstable.data\n",
    "    \n",
    "    if not google_cloud:\n",
    "        \n",
    "        # Make dir for output file if it doesn't exist yet\n",
    "        os.makedirs(os.path.dirname(outputname), exist_ok=True)\n",
    "        \n",
    "        # Write fits directly to the output [outputname]\n",
    "        fits.writeto(outputname, data, header, overwrite=overwrite)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Write the tmp fits file\n",
    "        fits_tmp = '{}{}'.format(rundir, outputname.split('/')[-1])\n",
    "        LOG.info('writing tmp fits file {}'.format(fits_tmp))\n",
    "        fits.writeto(fits_tmp, data, header, overwrite=overwrite)\n",
    "        \n",
    "        # Move fits_tmp to [dest_folder] in bucket\n",
    "        dest_folder = os.path.dirname(outputname)\n",
    "        copy_file(fits_tmp, dest_folder+'/', move=True)\n",
    "    \n",
    "    #mem_use(label='save_fits at end')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on BlackBOX function clean_tmp\n",
    "def remove_tmp_folder(tmp_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that removes the specified folder and its contents.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isdir(tmp_path):\n",
    "        shutil.rmtree(tmp_path)\n",
    "        LOG.info(\"Removing temporary folder: {}\".format(tmp_path))\n",
    "    else:\n",
    "        LOG.warning(\"tmp folder {} does not exist\".format(tmp_path))\n",
    "    \n",
    "    #mem_use(label=\"after removing temporary folder\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run match2SSO using command line parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    PARSER = argparse.ArgumentParser(description=\"User parameters\")\n",
    "    \n",
    "    PARSER.add_argument(\"--telescope\", type=str, default=\"ML1\",\n",
    "                        help=\"Telescope name (ML1, BG2, BG3 or BG4); \"\n",
    "                        \"default='ML1'\")\n",
    "    \n",
    "    PARSER.add_argument(\"--mode\", type=str, default=\"historic\",\n",
    "                        help=\"Day, night or historic mode of pipeline; \"\n",
    "                        \"default='historic'\")\n",
    "    \n",
    "    PARSER.add_argument(\"--catalog\", type=str, default=None,\n",
    "                        help=\"Only process this particular transient catalog. \"\n",
    "                        \"Requires full path and requires mode to be 'historic'\"\n",
    "                        \" or 'night'; default=None\")\n",
    "    \n",
    "    PARSER.add_argument(\"--date\", type=str, default=None,\n",
    "                        help=\"Date to process (yyyymmdd, yyyy-mm-dd, yyyy/mm/d\"\n",
    "                        \"d or yyyy.mm.dd). Mode must be 'historic' or 'day'; \"\n",
    "                        \"default=None\")\n",
    "    \n",
    "    PARSER.add_argument(\"--catlist\", type=str, default=None,\n",
    "                        help=\"Process all transient catalogs in the input list\"\n",
    "                        \". List entries require full path. Mode must be \"\n",
    "                        \"'historic'; default=None\")\n",
    "    \n",
    "    PARSER.add_argument(\"--logname\", type=str, default=None,\n",
    "                        help=\"Name of log file to save. Requires full path; \"\n",
    "                        \"default of None will not create a log file\")\n",
    "    \n",
    "    PARSER.add_argument(\"--redownload\", type=str2bool, default=True,\n",
    "                        help=\"Boolean to indicate whether the asteroid (and \"\n",
    "                        \"comet) databases should be redownloaded. Alternatively\"\n",
    "                        \" the most recently downloaded databases will be used.\")\n",
    "    \n",
    "    PARSER.add_argument(\"--overwrite\", type=str2bool, default=False,\n",
    "                        help=\"Boolean to indicate whether files will be remade \"\n",
    "                        \"and overwritten. Alternatively existing files will be \"\n",
    "                        \"used.\")\n",
    "    \n",
    "    ARGS = PARSER.parse_args()\n",
    "    run_match2SSO(tel=ARGS.telescope, mode=ARGS.mode, cat2process=ARGS.catalog,\n",
    "                  date2process=ARGS.date, list2process=ARGS.catlist,\n",
    "                  logname=ARGS.logname, redownload=ARGS.redownload,\n",
    "                  overwrite=ARGS.overwrite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
