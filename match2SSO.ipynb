{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# match2SSO\n",
    "* Running instructions are given at the start of function run_match2SSO\n",
    "* Originally written as a Jupyter Notebook using Python 3.8.10\n",
    "* Compatible with BlackBOX / ZOGY version 1.0.0 and up\n",
    "* Compatible with MeerLICHT / BlackGEM observations\n",
    "\n",
    "Output (SSO) catalogue columns and header keywords are listed here:\n",
    "https://www.overleaf.com/read/zrhqwcbkfqns\n",
    "\n",
    "<i>match2SSO</i> makes grateful use of the <i>lunar</i> and <i>jpl_eph</i> \n",
    "repositories that were written by Bill Gray under Project Pluto. The core of \n",
    "<i>match2SSO</i> is <i>astcheck</i>: a C++ script in the <i>lunar</i> \n",
    "repository that matches detections to known solar system objects. More \n",
    "information on <i>astcheck</i> can be found at: \n",
    "https://www.projectpluto.com/astcheck.htm\n",
    "\n",
    "<b>Dependencies on scripts and files</b>\n",
    "* <i>lunar</i> package (https://github.com/Bill-Gray/lunar)\n",
    "* <i>jpl_eph</i> package (https://github.com/Bill-Gray/jpl_eph)\n",
    "* JPL DE ephemeris file (ftp://ssd.jpl.nasa.gov/pub/eph/planets/Linux/)\n",
    "* MPC's Observatory codes list \n",
    "  (https://www.minorplanetcenter.net/iau/lists/ObsCodes.html)\n",
    "\n",
    "In addition, match2SSO uses MPCORB.DAT (MPC's asteroid database) and \n",
    "COMET.ELEMENTS (JPL's comet database), but these are downloaded when running \n",
    "the script and hence do not need to be pre-downloaded.\n",
    "\n",
    "if keep_tmp is False, all temporary files and folders are removed except for \n",
    "the most recent, unintegrated version of the asteroid & comet databases in\n",
    "the databaseFolder.\n",
    "For the night mode, we'll need to implement the cleaning function in \n",
    "BlackBOX that is commented out at the end of the night mode section in \n",
    "run_match2SSO. (As the night mode will be run in parallel on different\n",
    "catalogues and the individual night mode processes don't communicate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python packages and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import set_match2SSO as settingsFile #load match2SSO settings file\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob, re\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "\n",
    "import argparse #for parsing command line arguments when starting match2SSO\n",
    "import requests #for downloading databases\n",
    "import subprocess #for running command line commands / C++ scripts\n",
    "import shutil #to remove temporary folders\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "\n",
    "from astropy.time import Time\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table, Column\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "from multiprocessing import Pool, Manager, Lock, Queue, Array\n",
    "import sys, resource, psutil, platform #for memory usage logging\n",
    "import logging\n",
    "\n",
    "#Set up log\n",
    "logFormat = ('%(asctime)s.%(msecs)03d [%(levelname)s, %(process)s] %(message)s '\n",
    "          '[%(funcName)s, line %(lineno)d]')\n",
    "dateFormat = '%Y-%m-%dT%H:%M:%S'\n",
    "logging.basicConfig(level='INFO', format=logFormat, datefmt=dateFormat)\n",
    "logFormatter = logging.Formatter(logFormat, dateFormat)\n",
    "logging.Formatter.converter = time.gmtime #convert time in logger to UTC\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set version\n",
    "__version__ = \"1.0.0\"\n",
    "keywords_version = '1.0.0'\n",
    "\n",
    "#Relevant transient catalogue column names\n",
    "numberColumn = 'NUMBER'\n",
    "RA_column = 'RA_PSF_D'   #ra in deg\n",
    "DEC_column = 'DEC_PSF_D' #dec in deg\n",
    "magnitudeColumn = 'MAG_ZOGY'\n",
    "dummyColumn = 'TDUMCAT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions to run match2SSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_match2SSO(tel, mode, cat2process, date2process, list2process,\n",
    "                  logName, keep_tmp, redownloadDatabases, includeComets,\n",
    "                  overwriteFiles, timeFunctions):\n",
    "    \"\"\"\n",
    "    Run match2SSO on the input catalogue(s)/date. match2SSO can be run in \n",
    "    different mode / date2process / cat2process / list2process combinations.\n",
    "    Allowed combinations are: (if not mentioned, the variable is None)\n",
    "    # * Day mode\n",
    "    # * Day mode + date2process\n",
    "    # * Night mode + cat2process\n",
    "    # * Historic mode + cat2process\n",
    "    # * Historic mode + date2process\n",
    "    # * Historic mode + list2process\n",
    "    \n",
    "    Day mode: \n",
    "    Create known objects database & CHK files for the upcoming night, or \n",
    "    - in case date2process is specified - for the specified night.\n",
    "    0) Creates a run directory in preparation of the nightly processing\n",
    "    1) Downloads asteroid and comet databases\n",
    "    2) Integrates the asteroid database to midnight of the observation night\n",
    "    3) Combines the comet and integrated asteroid databases into a SOF-formatted\n",
    "       known objects database. \n",
    "    4) Runs astcheck on a fake detection in order to create the CHK files that\n",
    "       astcheck will need for faster processing when running on observations. \n",
    "    5) Creates symbolic links to the used databases and the observatory codes\n",
    "       list in the run directory.\n",
    "    (Products of steps 1-2 are saved to the databaseFolder, those of steps 3-5\n",
    "    to the run directory.)\n",
    "    \n",
    "    Night mode: \n",
    "    Run match2SSO on a single transient catalogue. The day mode should have\n",
    "    been run once before the night mode. This allows the night mode to run in\n",
    "    parallel on multiple transient catalogues of the same night, as steps that \n",
    "    cannot be parallelised (making known objects database and CHK files) have\n",
    "    already been executed in the day mode. The night mode:\n",
    "    1) Converts the transient catalogue into an MPC-formatted text file.\n",
    "    2) Runs astcheck on that file, to find matches between the transient \n",
    "       detections and known solar system objects. \n",
    "    3) Makes an SSO catalogue containing the matches.\n",
    "    4) Makes an MPC submission file of the matches.\n",
    "    \n",
    "    Historic mode: \n",
    "    The historic mode does the entire processing - executing steps 0-3 & 5 of \n",
    "    the day mode, followed by all steps of the night mode. It can be run on a \n",
    "    single transient catalogue, an entire night of observations or a list of \n",
    "    observations (possibly spanning multiple nights). For the first catalogue \n",
    "    that is processed of each observation night, a new known objects catalogue\n",
    "    is created that is integrated to the observation midnight. The asteroid and\n",
    "    comet databases used for this are only downloaded once per historic mode run\n",
    "    (and only if redownloadDatabases is True). For the remaining files (or if \n",
    "    redownloadDatabases is False), the most recently downloaded versions are used. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tel: string\n",
    "            Abbreviated telescope name. Can be either ML1, BG2, BG3 or BG4.\n",
    "    mode: string\n",
    "            Mode in which match2SSO is run. This can be 'day', 'night' or\n",
    "            'historic'.\n",
    "    cat2process: string\n",
    "            Path to and name of the transient catalogue that is to be processed.\n",
    "    date2process: string\n",
    "            Formatted as yyyymmdd, yyyy-mm-dd, yyyy/mm/dd or yyyy.mm.dd. When \n",
    "            used with day mode: date for which the known objects database needs\n",
    "            to be prepared. When used with historic mode: date for which all \n",
    "            light transient catalogues need to be processed.\n",
    "    list2process: string\n",
    "            Path to and name of the text file that contains the paths to and \n",
    "            names of transient catalogues (one per line) that need to be \n",
    "            processed.\n",
    "    logName: string\n",
    "            Path to and name of the log file in which comments about the run are\n",
    "            stored.\n",
    "    keep_tmp: string\n",
    "            String that can be converted to a boolean with str2bool(). This\n",
    "            boolean indicates whether the temporary files made during the \n",
    "            processing should be kept or removed at the end of the processing.\n",
    "    redownloadDatabases: string\n",
    "            String that can be converted to a boolean with str2bool(). This\n",
    "            boolean indicates whether the asteroid and comet databases will need\n",
    "            to be redownloaded when making the known objects database. \n",
    "            Alternatively, the most recently downloaded version of the databases\n",
    "            are used.\n",
    "    includeComets: string\n",
    "            String that can be converted to a boolean with str2bool(). This\n",
    "            boolean indicates whether comets should be included in the known\n",
    "            objects database. There have been issues with matching to comets\n",
    "            with large orbital uncertainties in astcheck, so as long as this has\n",
    "            not been solved, comet matching should be avoided. (time of writing:\n",
    "            23 Dec 2021)\n",
    "    overwriteFiles: string\n",
    "            String that can be converted to a boolean with str2bool(). This\n",
    "            boolean indicates whether files are allowed to be overwritten.\n",
    "    timeFunctions: string\n",
    "            String that can be converted to a boolean with str2bool(). This\n",
    "            boolean indicates whether functions need to be (wall-)timed.\n",
    "    \"\"\"\n",
    "    t_glob = time.time()\n",
    "    \n",
    "    #Format input parameters\n",
    "    mode = mode.lower()\n",
    "    \n",
    "    keep_tmp = str2bool(keep_tmp)\n",
    "    redownloadDatabases = str2bool(redownloadDatabases)\n",
    "    includeComets = str2bool(includeComets)\n",
    "    overwriteFiles = str2bool(overwriteFiles)\n",
    "    timeFunctions = str2bool(timeFunctions)\n",
    "    localTimeZone = timezone(get_par(settingsFile.timeZoneTelescope, tel))\n",
    "    if date2process != None:\n",
    "        date2process = date2process.replace(\".\",\"\").replace(\"/\",\"\").replace(\"-\",\n",
    "                                                                            \"\")\n",
    "    #Perform checks on input parameter combinations and setting file parameters\n",
    "    if not checkInputParameters(mode, cat2process, date2process, list2process):\n",
    "        return\n",
    "    if not checkSettingsFile(tel):\n",
    "        return\n",
    "    \n",
    "    #Logging\n",
    "    setUpLogFile(logName)\n",
    "    log.info(\"Mode: {}\".format(mode))\n",
    "    \n",
    "    mem_use(label='at start of run_match2SSO')\n",
    "    \n",
    "    #Get local noon corresponding to the night start in case date2process or\n",
    "    #cat2process are specified. nightStart is a datetime object (incl. timezone\n",
    "    #information).\n",
    "    if cat2process!=None:\n",
    "        nightStart = getNightStartFromCatalogueName(cat2process, tel)\n",
    "        \n",
    "    elif date2process!=None:\n",
    "        nightStart = localTimeZone.localize(datetime.strptime(date2process +\n",
    "                                                    \" 120000\", \"%Y%m%d %H%M%S\"))\n",
    "    \n",
    "    if mode==\"day\":\n",
    "        \n",
    "        log.info(\"Running the day mode.\")\n",
    "        \n",
    "        #If no observation night is specified, use the upcoming local night\n",
    "        if date2process==None:\n",
    "            nightStart = (datetime.now(localTimeZone)).strftime(\"%Y%m%d 120000\")\n",
    "            #Add timezone info to datetime object\n",
    "            nightStart = localTimeZone.localize(datetime.strptime(nightStart,\n",
    "                                                               \"%Y%m%d %H%M%S\"))\n",
    "        \n",
    "        #Create a run directory corresponding to the observation night\n",
    "        runDirectory = (\"{}{}/\"\n",
    "                        .format(databaseFolder, nightStart.strftime(\"%Y%m%d\")))\n",
    "        log.info(\"Run directory: {}\".format(runDirectory))\n",
    "        if not os.path.isdir(runDirectory):\n",
    "            os.makedirs(runDirectory)\n",
    "        \n",
    "        #Create symbolic link to the observatory codes list\n",
    "        if not os.path.exists(\"{}ObsCodes.html\".format(runDirectory)):\n",
    "            log.info(\"Creating symbolic link to ObsCodes.html\")\n",
    "            os.symlink(\"{}ObsCodes.html\".format(softwareFolder),\n",
    "                       \"{}ObsCodes.html\".format(runDirectory))\n",
    "        \n",
    "        #Download and integrate known object databases\n",
    "        midnight = nightStart + timedelta(days=0.5)\n",
    "        createKnownObjectsDatabase(midnight, runDirectory, redownloadDatabases,\n",
    "                                   includeComets, keep_tmp, timeFunctions)\n",
    "        \n",
    "        def create_CHKfiles(noonType):\n",
    "            \n",
    "            \"\"\"\n",
    "            Function that creates the CHK files that astcheck uses (and produces\n",
    "            if they don't exist yet) when matching. These files describe the\n",
    "            positions of all asteroids at the start and end of the night (both\n",
    "            at noon) in UTC. By running astcheck on a fake detection, we can\n",
    "            produce these CHK files in advance (which allows parallelisation of\n",
    "            match2SSO runs in the night mode). Of course we will subsequently\n",
    "            remove the fake detection and fake matches.\n",
    "            \n",
    "            As the 24-hour local observing night (between local noons) can\n",
    "            overlap with two UTC nights (between UTC noons) if there is a large\n",
    "            time difference between the timezone of the telescope and UTC, we'll\n",
    "            have to try producing CHK files for a time close to the start of the\n",
    "            local night (we take 1 min after) as well as a time close to the end\n",
    "            of it (1 min before). This function should hence be run in for both\n",
    "            noon types (\"noonstart\" and \"noonend\"). This will produce a total of\n",
    "            2 or 3 CHK files that astcheck will subsequently use to match any \n",
    "            observation taken during the local observing night to the known \n",
    "            solar system objects.\n",
    "            \"\"\"\n",
    "            \n",
    "            #Check noonType parameter and set observation time for fake \n",
    "            #detection\n",
    "            noonType = noonType.lower()\n",
    "            if noonType not in [\"nightstart\", \"nightend\"]:\n",
    "                log.error(\"Unknown noon type!\")\n",
    "                return\n",
    "            if noonType == \"nightstart\":\n",
    "                obstime = nightStart + timedelta(minutes=1)\n",
    "            elif noonType == \"nightend\":\n",
    "                obstime = nightStart + timedelta(days=1) - timedelta(minutes=1)\n",
    "            \n",
    "            #Convert observation time of fake detection to UTC\n",
    "            obstime = obstime.astimezone(pytz.utc)\n",
    "            \n",
    "            #Create MPC-formatted file with fake detection\n",
    "            MPCfileName_fake = (\"{}fakedetection_{}_MPCformat.txt\"\n",
    "                                .format(runDirectory, noonType))\n",
    "            log.info(\"Creating fake detection: {}\".format(MPCfileName_fake))\n",
    "            MPCfile_fake = open(MPCfileName_fake, \"w\")\n",
    "            fakeDetection = (\"     0000001  C{} {:0>2} {:08.5f} \"\n",
    "                            .format(obstime.year, obstime.month, obstime.day)\n",
    "                           + \"00 00 00.00 +00 00 00.0          0.00 G      L66\")\n",
    "            MPCfile_fake.write(fakeDetection)\n",
    "            MPCfile_fake.close()\n",
    "            \n",
    "            #Run astcheck on fake observation to create CHK files\n",
    "            log.info(\"Running astcheck on fake detection\")\n",
    "            astcheckOutputFileName_fake = MPCfileName_fake.replace(\"_MPCformat.txt\", \n",
    "                                                             \"_astcheckMatches.txt\")\n",
    "            runAstcheck(MPCfileName_fake, runDirectory, astcheckOutputFileName_fake,\n",
    "                        timeFunctions, overwriteFiles, matchingRadius=0)\n",
    "\n",
    "            #Remove MPC-formatted file and astcheck output related to the fake \n",
    "            #detection\n",
    "            os.remove(MPCfileName_fake)\n",
    "            log.info(\"Removed {}\".format(MPCfileName_fake))\n",
    "            os.remove(astcheckOutputFileName_fake)\n",
    "            log.info(\"Removed {}\".format(astcheckOutputFileName_fake))\n",
    "            \n",
    "            return\n",
    "        \n",
    "        #Create CHK files that astcheck needs in advance, to allow \n",
    "        #parallelisation\n",
    "        create_CHKfiles(\"nightstart\")\n",
    "        create_CHKfiles(\"nightend\")\n",
    "        \n",
    "        #Check for known object database products\n",
    "        if not checkForDatabaseProducts(runDirectory):\n",
    "            log_timing_memory(t_glob, label='run_match2SSO')\n",
    "            logging.shutdown()\n",
    "            return\n",
    "        \n",
    "        log.info(\"Day mode finished.\")\n",
    "    \n",
    "    \n",
    "    elif mode == \"night\":\n",
    "        \n",
    "        log.info(\"Running the night mode on transient catalogue: \\n{}\" \n",
    "                 .format(cat2process))\n",
    "        \n",
    "        runDirectory = \"{}{}/\".format(databaseFolder,\n",
    "                                      nightStart.strftime(\"%Y%m%d\"))\n",
    "        log.info(\"Run directory: {}\".format(runDirectory))\n",
    "            \n",
    "        #Check for known object database products. Stop processing if it doesn't\n",
    "        #exist\n",
    "        if not checkForDatabaseProducts(runDirectory):\n",
    "            logging.shutdown()\n",
    "            return\n",
    "        \n",
    "        #Check for CHK files\n",
    "        utcNightStart = getNightStartFromCatalogueName(cat2process, tel, \"utc\")\n",
    "        utcNightEnd = utcNightStart + timedelta(days=1)\n",
    "        utcNightStart = utcNightStart.strftime(\"%Y%m%d\")\n",
    "        utcNightEnd = utcNightEnd.strftime(\"%Y%m%d\")\n",
    "        if not os.path.exists(\"{}{}.chk\".format(runDirectory, utcNightStart)):\n",
    "            log.critical(\"Missing {}.chk!\".format(utcNightStart))\n",
    "            logging.shutdown()\n",
    "            return\n",
    "        if not os.path.exists(\"{}{}.chk\".format(runDirectory, utcNightEnd)):\n",
    "            log.critical(\"Missing {}.chk!\".format(utcNightEnd))\n",
    "            logging.shutdown()\n",
    "            return\n",
    "        del utcNightStart\n",
    "        del utcNightEnd\n",
    "        \n",
    "        #Check for observatory codes list. Stop processing if it doesn't exist.\n",
    "        if not os.path.exists(\"{}ObsCodes.html\".format(runDirectory)):\n",
    "            log.critical(\"{}ObsCodes.html doesn't exist.\".format(runDirectory))\n",
    "            logging.shutdown()\n",
    "            return\n",
    "        \n",
    "        _ = matchSingleCatalogue(cat2process, runDirectory, nightStart, \n",
    "                                 makeKOD=False, redownloadDatabases=False,\n",
    "                                 includeComets=includeComets, keep_tmp=keep_tmp,\n",
    "                                 timeFunctions=timeFunctions,\n",
    "                                 overwriteFiles=overwriteFiles)\n",
    "        \n",
    "        #Beware that the run directory created for the processing of the \n",
    "        #catalogue is not removed. This is the case because a single parallel\n",
    "        #process does not know about the rest. A cleaning function should be\n",
    "        #run at the end of the nightly processing if one wants to remove the\n",
    "        #runDirectory. Also delete the integrated asteroid database that was\n",
    "        #made during the day mode at this time. See code below.\n",
    "        #if not keep_tmp:\n",
    "        #    if os.path.exists(\"{}MPCORB.DAT\".format(runDirectory)):\n",
    "        #        asteroidDBname = os.readlink(\"{}MPCORB.DAT\".format(runDirectory))\n",
    "        #        if \"epoch\" in asteroidDBname:\n",
    "        #            os.remove(asteroidDBname)\n",
    "        #            log.info(\"Removed {}\".format(asteroidDBname))\n",
    "        #    removeTemporaryFolder(runDirectory)\n",
    "    \n",
    "    elif mode==\"historic\":\n",
    "        \n",
    "        def matchCataloguesSingleNight(cataloguesSingleNight, startNight,\n",
    "                                       redownloadDB):\n",
    "            \"\"\"\n",
    "            Process input catalogues corresponding to observations taken on the\n",
    "            same night.\n",
    "            \"\"\"\n",
    "            if timeFunctions:\n",
    "                t0 = time.time()\n",
    "            \n",
    "            log.info(\"{} catalogues to process for the night around {}.\"\n",
    "                    .format(len(cataloguesSingleNight), \n",
    "                            startNight.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "            \n",
    "            #Create run directory\n",
    "            runDirectory = \"{}{}/\".format(databaseFolder,\n",
    "                                          startNight.strftime(\"%Y%m%d\"))\n",
    "            log.info(\"Run directory: {}\".format(runDirectory))\n",
    "            if not os.path.exists(runDirectory):\n",
    "                os.makedirs(runDirectory)\n",
    "            \n",
    "            #Run matching per catalogue\n",
    "            makeKOD = True\n",
    "            for index, catalogueName in enumerate(cataloguesSingleNight):\n",
    "                log.info(\"Processing {}\".format(catalogueName))\n",
    "                \n",
    "                madeKOD = matchSingleCatalogue(catalogueName, runDirectory,\n",
    "                                               startNight, makeKOD,redownloadDB,\n",
    "                                               includeComets, keep_tmp, \n",
    "                                               timeFunctions, overwriteFiles)\n",
    "                if madeKOD:\n",
    "                    makeKOD = False #Only make known objects database once\n",
    "            \n",
    "            #Remove the run directory after processing the last catalogue of the\n",
    "            #night\n",
    "            if not keep_tmp:\n",
    "                #Remove integrated database made for this night\n",
    "                if os.path.exists(\"{}MPCORB.DAT\".format(runDirectory)):\n",
    "                    asteroidDBname = os.readlink(\"{}MPCORB.DAT\"\n",
    "                                                 .format(runDirectory))\n",
    "                    if \"epoch\" in asteroidDBname:\n",
    "                        os.remove(asteroidDBname)\n",
    "                        log.info(\"Removed {}\".format(asteroidDBname))\n",
    "                \n",
    "                #Remove temporary folder made for this night\n",
    "                removeTemporaryFolder(runDirectory)\n",
    "            \n",
    "            if timeFunctions:\n",
    "                log_timing_memory(t0, label='matchCataloguesSingleNight')\n",
    "            \n",
    "            return\n",
    "        \n",
    "        \n",
    "        if cat2process!=None:\n",
    "            log.info(\"Running historic mode on transient catalogue: \\n{}\"\n",
    "                     .format(cat2process))\n",
    "            matchCataloguesSingleNight([cat2process], nightStart, \n",
    "                                       redownloadDatabases)\n",
    "        \n",
    "        elif date2process!=None:\n",
    "            log.info(\"Running historic mode on night {}\".format(date2process))\n",
    "            catalogues2process = getTransientFileNames(nightStart, nightStart +\n",
    "                                                       timedelta(days=1), tel,\n",
    "                                                       timeFunctions)\n",
    "            if len(catalogues2process)==0:\n",
    "                log.critical(\"No light transient catalogues exist for night {}\"\n",
    "                             .format(date2process))\n",
    "                log_timing_memory(t_glob, label='run_match2SSO')\n",
    "                logging.shutdown()\n",
    "                return\n",
    "            \n",
    "            matchCataloguesSingleNight(catalogues2process, nightStart,\n",
    "                                       redownloadDatabases)\n",
    "        \n",
    "        elif list2process!=None:\n",
    "            log.info(\"Running historic mode on catalogue list: \\n{}\"\n",
    "                     .format(list2process))\n",
    "            with open(list2process, \"r\") as catalogueList:\n",
    "                listedCatalogues = [name.strip() for name in catalogueList \n",
    "                                    if name[0]!='#']\n",
    "            \n",
    "            #Order by observation date (noon that equals the start of the \n",
    "            #observation day)\n",
    "            noons = []\n",
    "            catalogues2process = []\n",
    "            for catalogueName in listedCatalogues:\n",
    "                noon = getNightStartFromCatalogueName(catalogueName, tel)\n",
    "                noons.append(noon.strftime(\"%Y%m%d %H%M%S\"))\n",
    "                catalogues2process.append(catalogueName)\n",
    "            \n",
    "            #Process files per night\n",
    "            log.info(\"Catalogue list spans {} nights\"\n",
    "                     .format(len(np.unique(noons))))\n",
    "            firstNight = True\n",
    "            for noon in np.unique(noons):\n",
    "                log.info(\"Processing night that starts at {}\" .format(noon))\n",
    "                nightIndex = np.where(np.array(noons)==noon)[0]\n",
    "                catalogues2process_1night = np.array(catalogues2process\n",
    "                                                    )[nightIndex]\n",
    "                \n",
    "                #Use the same version of the asteroid and comet databases for\n",
    "                #all data to be processed. If redownloadDatabases, this version\n",
    "                #corresponds to the time of processing the first image from the\n",
    "                #list. If redownloadDatabases is Fale, the latest existing \n",
    "                #downloaded version of the databases is used.\n",
    "                noon = localTimeZone.localize(datetime.strptime(noon, \n",
    "                                                               \"%Y%m%d %H%M%S\"))\n",
    "                if firstNight:\n",
    "                    matchCataloguesSingleNight(catalogues2process_1night, \n",
    "                                               noon, redownloadDatabases)\n",
    "                else:\n",
    "                    matchCataloguesSingleNight(catalogues2process_1night, \n",
    "                                               noon, redownloadDB=False)\n",
    "                firstNight = False\n",
    "    \n",
    "    \n",
    "    log.info(\"Finished running match2SSO.\")\n",
    "    log_timing_memory(t_glob, label='run_match2SSO')\n",
    "    logging.shutdown()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchSingleCatalogue(catalogueName, runDirectory, nightStart, makeKOD, \n",
    "                         redownloadDatabases, includeComets, keep_tmp, \n",
    "                         timeFunctions, overwriteFiles):\n",
    "    \"\"\"\n",
    "    Run matching routine on a single transient catalogue. Optionally, a new \n",
    "    known objects database is created where the reference epoch corresponds to \n",
    "    midnight on the observation night. The detections in the transient catalogue\n",
    "    are then matched to the positions of the solar system bodies in the known\n",
    "    objects catalogue. Matches are saved to an SSO catalogue.\n",
    "    Function returns a boolean indicating whether a known objects catalogue was\n",
    "    (successfully) made.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    catalogueName: string\n",
    "            Name of the transient catalogue of which the detections are to be\n",
    "            matched to known solar system objects.\n",
    "    runDirectory: string\n",
    "            Directory corresponding to observation night where all temporary\n",
    "            products will be stored during the running of match2SSO.\n",
    "    nightStart: datetime object, including time zone\n",
    "            Noon corresponding to the start of the local night during which the\n",
    "            observation corresponding to the transient catalogue was made.\n",
    "    makeKOD: boolean\n",
    "            Boolean indicating whether a new known objects database needs to\n",
    "            be made. In night mode, this should be False. In historic mode, \n",
    "            this is only True for the first catalogue that is processed per\n",
    "            observation night, as the database will need to be integrated to\n",
    "            that observation night.\n",
    "    redownloadDatabases: boolean\n",
    "            Only used when makeKOD is True. This boolean indicates whether the\n",
    "            asteroid and comet databases will need to be redownloaded before\n",
    "            making the known objects database. Alternatively, the most recent,\n",
    "            previously downloaded version of the databases are used.\n",
    "    includeComets: boolean\n",
    "            Boolean indicating whether comets are included in the known objects\n",
    "            catalogue.\n",
    "    keep_tmp: boolean\n",
    "            Boolean indicating whether the temporary files corresponding to\n",
    "            the transient catalogue that are made in this function should be \n",
    "            kept or removed at the end of the processing.\n",
    "    timeFunctions: boolean\n",
    "            Boolean indicating whether functions need to be (wall-)timed.\n",
    "    overwriteFiles: boolean\n",
    "            Boolean indicating whether files are allowed to be overwritten.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of matchSingleCatalogue')\n",
    "    if timeFunctions:\n",
    "        t0 = time.time()\n",
    "    \n",
    "    madeKOD = False\n",
    "    \n",
    "    #Check if input catalogue exists and is not flagged red\n",
    "    isExisting, isDummy, catalogueName = checkInputCatalogue(catalogueName)\n",
    "    if not isExisting:\n",
    "        return madeKOD\n",
    "    del isExisting\n",
    "    \n",
    "    if isDummy:\n",
    "        SSOcatalogueName = catalogueName.replace(\"_light\", \"\").replace(\".fits\",\n",
    "                                                                   \"_sso.fits\")\n",
    "        if os.path.exists(SSOcatalogueName) and not overwriteFiles:\n",
    "            log.warning(\"{} already exists and will not be re-made.\"\n",
    "                        .format(SSOcatalogueName))\n",
    "            return makeKOD\n",
    "        \n",
    "        create_SSOcatalogue(None, runDirectory, SSOcatalogueName, includeComets,\n",
    "                            timeFunctions, overwriteFiles)\n",
    "        return madeKOD\n",
    "    del isDummy\n",
    "    \n",
    "    #If makeKOD, create a new known objects database with a reference epoch \n",
    "    #corresponding to midnight of the observation night. Also create a symbolic\n",
    "    #link to the MPC observatory codes list.\n",
    "    if makeKOD:\n",
    "        midnight = nightStart + timedelta(days=0.5)\n",
    "        createKnownObjectsDatabase(midnight, runDirectory, redownloadDatabases,\n",
    "                                   includeComets, keep_tmp, timeFunctions)\n",
    "        \n",
    "        #Check if the run directory contains the proper known objects database\n",
    "        #files for further processing\n",
    "        if not checkForDatabaseProducts(runDirectory):\n",
    "            return madeKOD\n",
    "        \n",
    "        madeKOD = True\n",
    "        \n",
    "        #Make symbolic link to observatory codes list if it doesn't exist yet\n",
    "        if not os.path.exists(\"{}ObsCodes.html\".format(runDirectory)):\n",
    "            log.info(\"Creating symbolic link to ObsCodes.html\")\n",
    "            os.symlink(\"{}ObsCodes.html\".format(softwareFolder),\n",
    "                       \"{}ObsCodes.html\".format(runDirectory))\n",
    "        \n",
    "    #Convert the transient catalogue to an MPC-formatted text file\n",
    "    MPCfileName = \"{}{}\".format(runDirectory, os.path.basename(catalogueName\n",
    "                      ).replace(\"_light\", \"\").replace(\".fits\",\"_MPCformat.txt\"))\n",
    "    MPC_code = convertCatalogue2MPCformat(catalogueName, MPCfileName, \n",
    "                                          overwriteFiles, timeFunctions)\n",
    "    if MPC_code == None:\n",
    "        log.critical(\"Stop running match2SSO on catalogue because of unknown \"\n",
    "                     +\"MPC code.\")\n",
    "        return madeKOD\n",
    "    \n",
    "    #Run astcheck on the MPC-formatted transient file\n",
    "    astcheckOutputFileName = MPCfileName.replace(\"_MPCformat.txt\",\n",
    "                                                 \"_astcheckMatches.txt\")\n",
    "    runAstcheck(MPCfileName, runDirectory, astcheckOutputFileName, timeFunctions,\n",
    "                overwriteFiles)\n",
    "    \n",
    "    #Save matches found by astcheck to an SSO catalogue\n",
    "    SSOcatalogueName = catalogueName.replace(\"_light\", \"\").replace(\".fits\",\n",
    "                                                                   \"_sso.fits\")\n",
    "    create_SSOcatalogue(astcheckOutputFileName, runDirectory, SSOcatalogueName,\n",
    "                        includeComets, timeFunctions, overwriteFiles)\n",
    "    \n",
    "    #Create a submission file that can be used to submit the detections that \n",
    "    #were matched to known solar system objects to the MPC\n",
    "    createSubmissionFile(SSOcatalogueName, MPCfileName, MPC_code, timeFunctions,\n",
    "                         overwriteFiles)\n",
    "    \n",
    "    #Delete temporary files corresponding to the processed transient catalogue. \n",
    "    #The other temporary files (the CHK files, the SOF file and the symbolic \n",
    "    #links) in the runDirectory are not (yet) removed, as they might be needed \n",
    "    #for processing of other data from the same night.\n",
    "    if not keep_tmp:\n",
    "        os.remove(MPCfileName)\n",
    "        log.info(\"Removed {}\".format(MPCfileName))\n",
    "        os.remove(astcheckOutputFileName)\n",
    "        log.info(\"Removed {}\".format(astcheckOutputFileName))\n",
    "    \n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t0, label='matchSingleCatalogue')\n",
    "    \n",
    "    return madeKOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core functions in match2SSO\n",
    "* Create known objects catalogue (with asteroids with a max. orbital uncertainty)\n",
    "* Convert transient catalogue to MPC format\n",
    "* Run astcheck\n",
    "* Save astcheck matches to SSO catalogue\n",
    "* Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createKnownObjectsDatabase(midnight, runDirectory, redownloadDatabases,\n",
    "                               includeComets, keep_tmp, timeFunctions):\n",
    "    \"\"\"\n",
    "    Function downloads the most recent versions of the asteroid database and the\n",
    "    comet database. It then uses integrat.cpp from the lunar repository to \n",
    "    integrate the asteroid orbits to midnight of the observation night, in order\n",
    "    to optimize the predicted positions of known objects. \n",
    "    \n",
    "    Beware: the current version of integrat.cpp cannot be used on JPL's comet \n",
    "    file, as it is not compatible with its format. As a consequence, there might\n",
    "    be an offset in the predictions of the comet positions, perhaps causing us \n",
    "    to miss these objects in the linking routine. Note that the MPC's comet file\n",
    "    could be integrated to the right epoch using integrat.cpp, but as this file\n",
    "    is not compatible with astcheck, it cannot be used here.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    midnight: datetime object, including time zone\n",
    "            Local midnight during the observation night.\n",
    "    runDirectory: string\n",
    "            Directory in which mpc2sof is run and which the known objects\n",
    "            catalogue is saved to.\n",
    "    redownloadDatabases: boolean\n",
    "            If False, the databases will not be redownloaded. They will only be \n",
    "            integrated to the observation epoch (midnight on the observation\n",
    "            night).\n",
    "    includeComets: boolean\n",
    "            Boolean indicating whether comets are to be included in the known \n",
    "            objects database. As of 8 dec 2021, there are issues with matching\n",
    "            to known comets with large orbital uncertainties, so this boolean\n",
    "            should be False until these issues can be fixed in astcheck.\n",
    "    keep_tmp: boolean\n",
    "            Only relevant if a new database version is downloaded. If keep_tmp \n",
    "            is False, the old versions of the database will be removed.\n",
    "    timeFunctions: boolean\n",
    "            Boolean indicating whether functions need to be (wall-)timed.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of createKnownObjectsDatabase')\n",
    "    if timeFunctions:\n",
    "        t0 = time.time()\n",
    "    \n",
    "    def downloadDatabase(SSOtype):\n",
    "        \n",
    "        \"\"\"\n",
    "        SSOtype: string\n",
    "                Solar system object type that is in the database. Can be either \n",
    "                'asteroid' or 'comet'. (Capitals are allowed as well.)\n",
    "        \"\"\"\n",
    "        if timeFunctions:\n",
    "            t1 = time.time()\n",
    "        \n",
    "        SSOtype = SSOtype.lower()\n",
    "        if SSOtype == \"asteroid\":\n",
    "            databaseURL = settingsFile.URL_asteroidDatabase\n",
    "        elif SSOtype == \"comet\":\n",
    "            databaseURL = settingsFile.URL_cometDatabase\n",
    "        else:\n",
    "            errorString = \"Database type unknown. Cannot be downloaded.\"\n",
    "            log.critical(errorString)\n",
    "            raise ValueError(errorString)\n",
    "        \n",
    "        #Determine whether database needs to be downloaded\n",
    "        existingDatabases = glob.glob(\"{}{}DB_*.dat\".format(databaseFolder,\n",
    "                                                            SSOtype))\n",
    "        existingUnintegratedDatabases = [DB for DB in existingDatabases \n",
    "                                         if \"epoch\" not in DB]\n",
    "        download = True\n",
    "        if not redownloadDatabases and len(existingDatabases)>0:\n",
    "            download = False\n",
    "        \n",
    "        #Download database if desired and get database version\n",
    "        if download:\n",
    "            log.info(\"Downloading {} database...\".format(SSOtype))\n",
    "            databaseVersion = datetime.utcnow().strftime(\"%Y%m%dT%H%M\")\n",
    "            databaseName = \"{}{}DB_version{}.dat\".format(databaseFolder,SSOtype,\n",
    "                                                         databaseVersion)\n",
    "            req = requests.get(databaseURL, allow_redirects=True)\n",
    "            open(databaseName, \"wb\").write(req.content)\n",
    "            log.info(\"{} database version: {}\".format(SSOtype, databaseVersion))\n",
    "            \n",
    "            #Remove asteroids with large orbital uncertainties from database\n",
    "            if SSOtype == \"asteroid\":\n",
    "                selectAsteroidsOnUncertainty(databaseName, timeFunctions)\n",
    "            \n",
    "            if not keep_tmp and len(existingDatabases)>0:\n",
    "                log.info(\"Removing older {} database versions.\".format(SSOtype))\n",
    "                for oldDatabase in existingDatabases:\n",
    "                    os.remove(oldDatabase)\n",
    "                    log.info('Removed {}.'.format(oldDatabase))\n",
    "        else:\n",
    "            #Retrieve most recent (unintegrated) database version. If there is\n",
    "            #no unintegrated database, retrieve the most recent integrated one.\n",
    "            #Empty databases (created when includeComets=False) are not taken \n",
    "            #into account, as these are in a different folder.\n",
    "            databaseFiles = np.sort(existingUnintegratedDatabases)\n",
    "            if len(databaseFiles)==0:\n",
    "                databaseFiles = np.sort(existingDatabases)\n",
    "            databaseName = databaseFiles[-1]\n",
    "            databaseVersion = os.path.splitext(os.path.basename(databaseName)\n",
    "                                        )[0].split(\"_\")[1].replace(\"version\",\"\")\n",
    "            log.info(\"{} database version: {}\".format(SSOtype, databaseVersion))\n",
    "            \n",
    "        if timeFunctions:\n",
    "            log_timing_memory(t1, label='downloadDatabase ({})'\n",
    "                              .format(SSOtype))\n",
    "        \n",
    "        return databaseName, databaseVersion\n",
    "    \n",
    "    #Download asteroid database\n",
    "    asteroidDatabaseName, asteroidDatabaseVersion = downloadDatabase(\"asteroid\")\n",
    "    \n",
    "    #Download comet database if requested\n",
    "    if includeComets:\n",
    "        _, cometDatabaseVersion = downloadDatabase(\"comet\")\n",
    "    else:\n",
    "        log.info(\"Do not download comet database. Instead, create an empty \"\n",
    "                 +\"comet database so that there's no matching to comets.\")\n",
    "        cometDBheader = (\"Num  Name                                     Epoch \"+\n",
    "                         \"     q           e        i         w        Node   \"+\n",
    "                         \"       Tp       Ref\\n-------------------------------\"+\n",
    "                         \"------------ ------- ----------- ---------- --------\"+\n",
    "                         \"- --------- --------- -------------- ------------\")\n",
    "        open(\"{}ELEMENTS.COMET\".format(runDirectory), \"w\").write(cometDBheader)\n",
    "    \n",
    "    #Integrat only accepts UTC midnights. Choose the one closest to local \n",
    "    #midnight.\n",
    "    date_midnight = midnight.date()\n",
    "    if midnight.hour >= 12.:\n",
    "        date_midnight = date_midnight + timedelta(days=1)\n",
    "    midnight_utc = pytz.utc.localize(datetime.strptime(date_midnight.strftime(\n",
    "                                         \"%Y%m%d\")+ \" 000000\", \"%Y%m%d %H%M%S\"))\n",
    "    del midnight\n",
    "    \n",
    "    #Integrate the asteroid database to the observation date.\n",
    "    midnight_utc_str = midnight_utc.strftime(\"%Y%m%dT%H%M\")\n",
    "    filenameIntegratedAsteroidDB = (\"{}asteroidDB_version{}_epoch{}.dat\"\n",
    "                                    .format(databaseFolder,\n",
    "                                            asteroidDatabaseVersion,\n",
    "                                            midnight_utc_str))\n",
    "    if not os.path.exists(filenameIntegratedAsteroidDB):\n",
    "        log.info(\"Integrating asteroid database to epoch {}...\"\n",
    "                 .format(midnight_utc_str))\n",
    "        if timeFunctions:\n",
    "            t1 = time.time()\n",
    "        subprocess.run([\"integrat\", asteroidDatabaseName, \n",
    "                        filenameIntegratedAsteroidDB,str(Time(midnight_utc).jd),\n",
    "                        \"-f{}\".format(settingsFile.JPL_ephemerisFile)],\n",
    "                       cwd=databaseFolder)\n",
    "        if timeFunctions:\n",
    "            log_timing_memory(t1, label='integrat')\n",
    "        \n",
    "        #Remove temporary file created by integrat\n",
    "        if os.path.exists(\"{}ickywax.ugh\".format(databaseFolder)):\n",
    "            os.remove(\"{}ickywax.ugh\".format(databaseFolder))\n",
    "    \n",
    "    #Create the symbolic links in the run directory that mpc2sof needs\n",
    "    runNameAsteroidDatabase = \"{}MPCORB.DAT\".format(runDirectory)\n",
    "    if os.path.exists(runNameAsteroidDatabase):\n",
    "        log.info(\"Removing the old MPCORB.DAT symbolic link\")\n",
    "        os.unlink(runNameAsteroidDatabase)\n",
    "    os.symlink(filenameIntegratedAsteroidDB, runNameAsteroidDatabase)\n",
    "    log.info(\"Created symbolic link {}\".format(runNameAsteroidDatabase))\n",
    "    \n",
    "    if includeComets:\n",
    "        runNameCometDatabase = \"{}ELEMENTS.COMET\".format(runDirectory)\n",
    "        if os.path.exists(runNameCometDatabase):\n",
    "            log.info(\"Removing the old ELEMENTS.COMET symbolic link\")\n",
    "            os.unlink(runNameCometDatabase)\n",
    "        os.symlink(\"{}cometDB_version{}.dat\".format(databaseFolder, \n",
    "                                                    cometDatabaseVersion),\n",
    "                   runNameCometDatabase)\n",
    "        log.info(\"Created symbolic link {}\".format(runNameCometDatabase))\n",
    "    mem_use(label='after creating symbolic links to the databases')\n",
    "    \n",
    "    #Combine the known comets and asteroids into a SOF file, which astcheck will\n",
    "    #then use as input\n",
    "    log.info(\"Combining asteroids and comets into SOF file.\")\n",
    "    if timeFunctions:\n",
    "        t2 = time.time()\n",
    "    subprocess.run(\"mpc2sof\", cwd=runDirectory)\n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t2, label='mpc2sof')\n",
    "    \n",
    "    log.info(\"Finished loading and formatting external databases.\")\n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t0, label='createKnownObjectsDatabase')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectAsteroidsOnUncertainty(asteroidDBname, timeFunctions):\n",
    "    \n",
    "    \"\"\"\n",
    "    Go through the asteroid database (MPCORB format) and select the asteroids \n",
    "    which have orbital uncertainty parameters smaller than maxUncertainty. \n",
    "    The MPC uncertainty parameters that we consider are explained here: \n",
    "    https://www.minorplanetcenter.net/iau/info/UValue.html\n",
    "    \n",
    "    Overwrite the database with just the asteroids selected on their orbital\n",
    "    uncertainties, so that there will be no matching with poorly known objects.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    asteroidDBname: string\n",
    "            Name of the full asteroid database (MPCORB-formatted text-file).\n",
    "    timeFunctions: boolean\n",
    "            Boolean indicating whether functions need to be (wall-)timed.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of selectAsteroidsOnUncertainty')\n",
    "    \n",
    "    if settingsFile.maxUncertainty == None:\n",
    "        log.info(\"All known solar system bodies are used in the matching, \"\n",
    "                 +\"irrespective of their uncertainty parameter.\")\n",
    "        return\n",
    "    \n",
    "    if timeFunctions:\n",
    "        t0 = time.time()\n",
    "    log.info(\"Removing asteroids with too large uncertainties...\\n\")\n",
    "    \n",
    "    #Open asteroid database\n",
    "    asteroidDB = open(asteroidDBname, \"r\").readlines()\n",
    "    \n",
    "    #Find the size of the header of the asteroid database, assuming that the\n",
    "    #header ends with a line of dashes.\n",
    "    headerEndIndex = 0\n",
    "    lineIndex = 0\n",
    "    for line in asteroidDB:\n",
    "        if line[:5] == \"-----\":\n",
    "            headerEndIndex = lineIndex\n",
    "            break\n",
    "        lineIndex += 1\n",
    "    \n",
    "    #Re-write the asteroid database file, including only the header and the \n",
    "    #lines corresponding to asteroids that have small orbital uncertainties.\n",
    "    Nast_preSelection = 0\n",
    "    Nast_postSelection = 0\n",
    "    with open(asteroidDBname, \"w\") as f:\n",
    "        for lineIndex in range(len(asteroidDB)-1):\n",
    "            \n",
    "            #Copy header to file\n",
    "            if lineIndex <= headerEndIndex:\n",
    "                f.write(asteroidDB[lineIndex])\n",
    "                continue\n",
    "            \n",
    "            line = asteroidDB[lineIndex]\n",
    "            \n",
    "            #Copy empty lines\n",
    "            if line == \"\\n\":\n",
    "                f.write(line)\n",
    "                continue\n",
    "            \n",
    "            Nast_preSelection += 1\n",
    "            \n",
    "            #Filter on uncertainty parameter. Copy lines of asteroids for\n",
    "            #which orbits are determined reasonably well.\n",
    "            uncertainty = line[105]\n",
    "            if uncertainty.isdigit():\n",
    "                if float(uncertainty) <= settingsFile.maxUncertainty:\n",
    "                    f.write(line)\n",
    "                    Nast_postSelection += 1\n",
    "    \n",
    "    log.info(\"{} out of {} asteroids have U<={}\"\n",
    "             .format(Nast_postSelection, Nast_preSelection,\n",
    "                     settingsFile.maxUncertainty))\n",
    "    log.info(\"Asteroid database now only includes sources with U<={}\"\n",
    "            .format(settingsFile.maxUncertainty))\n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t0, label='selectAsteroidsOnUncertainty')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCatalogue2MPCformat(transientCatalogue, MPCfileName, overwriteFiles,\n",
    "                               timeFunctions):\n",
    "    \"\"\"\n",
    "    Function converts the transient catalogue to a text file of the MPC \n",
    "    80-column format, so that astcheck can run on it. For the asteroid / comet\n",
    "    identifier used in the MPC file, the transient number is used. This \n",
    "    transient number cannot be used for MPC submissions as it is not all-time \n",
    "    unique (per telescope). But it is a straight-forward way to link detections \n",
    "    to known solar system objects within match2SSO.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    transientCatalogue: string\n",
    "            Path to and name of the transient catalogue.\n",
    "    MPCfileName: string\n",
    "            Path to and name of the MPC-formatted text file that is made in this\n",
    "            function.\n",
    "    overwriteFiles: boolean\n",
    "            Boolean indicating whether files are allowed to be overwritten.\n",
    "    timeFunctions: boolean\n",
    "            Boolean indicating whether functions need to be (wall-)timed.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of convertCatalogue2MPCformat')\n",
    "    log.info(\"Converting transient catalogue to MPC-format.\")\n",
    "    if timeFunctions:\n",
    "        t0 = time.time()\n",
    "    \n",
    "    #Load transient catalogue header\n",
    "    with fits.open(transientCatalogue) as hdu:\n",
    "        transientHeader = hdu[1].header\n",
    "    \n",
    "    #Get the MPC observatory code from the header\n",
    "    MPC_code = transientHeader[\"MPC-CODE\"].strip()\n",
    "    if MPC_code not in list(pd.read_fwf(\"{}ObsCodes.html\"\n",
    "                                        .format(softwareFolder),widths=[4,2000],\n",
    "                                        skiprows=1)['Code'])[:-1]:\n",
    "        log.critical(\"MPC code {} is not in the MPC list of observatory codes\"\n",
    "                    .format(MPC_code))\n",
    "        return None\n",
    "    \n",
    "    #Check if MPC-formatted file exists and if it should be overwritten or not\n",
    "    if not overwriteFiles and os.path.exists(MPCfileName):\n",
    "        log.info(\"MPC-formatted file already exists and will not re-made.\")\n",
    "        return MPC_code\n",
    "    \n",
    "    #Get observation date in the right format\n",
    "    observationTime = Time(transientHeader['MJD-OBS'], format='mjd').datetime\n",
    "    decimalDay = observationTime.day + (observationTime.hour + \n",
    "                 (observationTime.minute + (observationTime.second + \n",
    "                 (observationTime.microsecond/10.**6))/60.)/60.)/24.\n",
    "    MPC_char16to32 = \"{} {:0>2} {:08.5f} \".format(observationTime.year, \n",
    "                                                  observationTime.month, \n",
    "                                                  decimalDay)\n",
    "    #Load transient catalogue data\n",
    "    with fits.open(transientCatalogue) as hdu:\n",
    "        detections = Table(hdu[1].data)\n",
    "    \n",
    "    #Create output file\n",
    "    MPCfile = open(MPCfileName, \"w\")\n",
    "    \n",
    "    #Loop over the detections and add data to the MPC-formatted file\n",
    "    for detection in range(len(detections)):\n",
    "        #Use the source numbers as \"temporary designations\" in the MPC format. \n",
    "        #In this way, we will be able to link the known objects to the right \n",
    "        #source.\n",
    "        MPC_char1to15 = (\"     {:0>7}  C\"\n",
    "                         .format(detections[numberColumn][detection]))\n",
    "        \n",
    "        #Get the coordinates and magnitude of the source\n",
    "        coord = SkyCoord(detections[RA_column][detection]*u.deg, \\\n",
    "                         detections[DEC_column][detection]*u.deg, frame='icrs')\n",
    "        ra = coord.to_string('hmsdms', sep=' ', precision=2)[:11]\n",
    "        dec = coord.to_string('hmsdms', sep=' ', precision=1)[-11:]\n",
    "        mag = detections[magnitudeColumn][detection]\n",
    "        mag = \"{:.1f}\".format(mag)\n",
    "        \n",
    "        MPC_char33to80 = \"{} {}          {} G      {}\".format(ra, dec, \\\n",
    "                                          mag.rjust(4), MPC_code)\n",
    "        \n",
    "        #Write the data to the MPC-formatted file\n",
    "        MPC_line = \"{}{}{}\".format(MPC_char1to15, MPC_char16to32, \\\n",
    "                                   MPC_char33to80)\n",
    "        MPCfile.write(\"{}\\n\".format(MPC_line))\n",
    "    \n",
    "    MPCfile.close()\n",
    "    \n",
    "    log.info(\"MPC-formatted file saved to {}.\".format(MPCfileName))\n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t0, label='convertCatalogue2MPCformat')\n",
    "    \n",
    "    return MPC_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAstcheck(MPC_formattedFile, runDirectory, outputFileName, timeFunctions,\n",
    "                overwriteFiles, matchingRadius=settingsFile.matchingRadius, \n",
    "                limitingMagnitude=settingsFile.limitingMagnitude, \n",
    "                maximalNumberOfObjects=settingsFile.maximalNumberOfAsteroids):\n",
    "    \"\"\"\n",
    "    Run astcheck on the input transient catalogue to find matches between \n",
    "    transient detections and known solar system objects. Per detection, all\n",
    "    matches within the matchingRadius are selected and saved to the output text\n",
    "    file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    MPC_formattedFile: string\n",
    "            Name of the input text-file that is formatted according to the MPC's\n",
    "            80-column MPC format. This file can list detections / tracklets for\n",
    "            astcheck to match, but it can also contain just a single row \n",
    "            representing the observation. In the latter use case, the specified\n",
    "            coordinates should correspond to the centre of the observation.\n",
    "    runDirectory: string\n",
    "            Directory in which astcheck is run. This directory should contain\n",
    "            the mpc2sof catalogue that contains the known solar system objects.\n",
    "    outputFileName: string\n",
    "            Path to and name of the output text file in which the matches found\n",
    "            by astcheck are stored.\n",
    "    timeFunctions: boolean\n",
    "            Boolean indicating whether functions need to be (wall-)timed.\n",
    "    overwriteFiles: boolean\n",
    "            Boolean indicating whether files are allowed to be overwritten.\n",
    "    matchingRadius: int or float\n",
    "            Matching radius in arcsec. The default value is the one specified in\n",
    "            the settings file [set_match2sso.py].\n",
    "    limitingMagnitude: int or float\n",
    "            Limiting V-magnitude up to which asteroids are taken into account. \n",
    "            The default value is the one specified in the settings file \n",
    "            [set_match2sso.py].\n",
    "    maximalNumberOfObjects: int\n",
    "            Maximal number of matches that are returned by astcheck. The default\n",
    "            value is the one specified in the settings file [set_match2sso.py].\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of runAstcheck')\n",
    "    log.info(\"Running astcheck: matching detections to known solar system \" + \\\n",
    "              \"bodies.\")\n",
    "    if timeFunctions:\n",
    "        t0 = time.time()\n",
    "    \n",
    "    if not overwriteFiles and os.path.exists(outputFileName):\n",
    "        log.info(\"Astcheck output file already exists and will not be re-made.\")\n",
    "        return\n",
    "    \n",
    "    #Create a file for storing the output of the astcheck run\n",
    "    outputFile = open(outputFileName, 'w')\n",
    "    \n",
    "    #Run astcheck from folder containing .sof-file\n",
    "    subprocess.call([\"astcheck\", MPC_formattedFile, \n",
    "                     \"-r{}\".format(matchingRadius), \n",
    "                     \"-m{}\".format(limitingMagnitude),  \n",
    "                     \"-M{}\".format(maximalNumberOfObjects)], \n",
    "                    stdout=outputFile, cwd=runDirectory)\n",
    "    outputFile.close()\n",
    "    \n",
    "    log.info(\"Matches saved to {}.\".format(outputFileName))\n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t0, label='runAstcheck')\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SSOcatalogue(astcheckOutputFileName, runDirectory,\n",
    "                        SSOcatalogueName, includeComets, timeFunctions,\n",
    "                        overwriteFiles):\n",
    "    \"\"\"\n",
    "    Open the text-file that was produced when running astcheck \n",
    "    (astcheckOutputFileName) and save the information to an SSO catalogue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    astcheckOutputFileName: string  \n",
    "            Name of the file containing astcheck's output (the matches). This\n",
    "            can be None, in which case we will create a dummy SSO catalogue\n",
    "            without matches.\n",
    "    runDirectory: string\n",
    "            Directory in which astcheck was run. This directory also contains\n",
    "            symbolic links to the asteroid and comet databases that were used to\n",
    "            create the known objects catalogue that astcheck used.\n",
    "    SSOcatalogueName: string\n",
    "            Name of the SSO catalogue to be created in which the matches\n",
    "            are stored.\n",
    "    includeComets: boolean\n",
    "            Boolean indicating whether comets are included in the known objects \n",
    "            catalogue.\n",
    "    timeFunctions: boolean\n",
    "            Boolean indicating whether functions need to be (wall-)timed.\n",
    "    overwriteFiles: boolean\n",
    "            Boolean indicating whether files are allowed to be overwritten.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of create_SSOcatalogue')\n",
    "    log.info(\"Converting astcheck output into an SSO catalogue.\")\n",
    "    if timeFunctions:\n",
    "        t0 = time.time()\n",
    "    \n",
    "    if not overwriteFiles and os.path.exists(SSOcatalogueName):\n",
    "        log.info(\"SSO catalogue already exists and will not be re-made.\")\n",
    "        return\n",
    "    \n",
    "    #If the transient catalogue was red-flagged, matching was not performed and\n",
    "    #an empty SSO catalogue needs to be created.\n",
    "    if astcheckOutputFileName == None:\n",
    "        log.info(\"Creating a dummy SSO catalogue.\")\n",
    "        SSOheader = create_SSOheader(runDirectory, includeComets, True, \n",
    "                                     timeFunctions)\n",
    "        write2fitsFile(Table(), None, [], SSOcatalogueName, \n",
    "                       startHeader=SSOheader)\n",
    "        if timeFunctions:\n",
    "            log_timing_memory(t0, label='create_SSOcatalogue')\n",
    "        return\n",
    "    \n",
    "    #Read astcheck's output file\n",
    "    #The footer is variable in terms of the number of lines it spans, but it \n",
    "    #always starts with the footerString as defined below and can hence be\n",
    "    #recognized by this string.\n",
    "    footerString = \"The apparent motion and arc length\"\n",
    "    astcheckOutputFile = open(astcheckOutputFileName, \"r\").readlines()\n",
    "    indexFooter = [index for index in range(len(astcheckOutputFile)) if \\\n",
    "                   footerString in astcheckOutputFile[index]][0]\n",
    "    astcheckOutputFile = astcheckOutputFile[5:indexFooter]\n",
    "    \n",
    "    separator = \"\\n\"\n",
    "    indicesSeparator = np.where(np.array(astcheckOutputFile)==separator)[0]\n",
    "    indicesSeparator = np.append(-1, indicesSeparator)\n",
    "    indicesSeparator = np.append(indicesSeparator, len(astcheckOutputFile))\n",
    "    \n",
    "    #Create table to store match information in\n",
    "    #Dictionary of all output columns with their (numpy) formats and their units\n",
    "    outputColumns = {\n",
    "                    numberColumn:   [\"i4\",  \"\"],\n",
    "                    \"ID_SSO\":       [\"12a\", \"\"],\n",
    "                    \"DIST_RA_SSO\":  [\"i2\",  \"arcsec\"],\n",
    "                    \"DIST_DEC_SSO\": [\"i2\",  \"arcsec\"],\n",
    "                    \"DIST_SSO\":     [\"i2\",  \"arcsec\"],\n",
    "                    \"MAG_V_SSO\":    [\"f4\",  \"\"]\n",
    "                    }\n",
    "    outputTable = Table()\n",
    "    for key in outputColumns.keys():\n",
    "        outputTable.add_column(Column(name=key, dtype=outputColumns[key][0],\n",
    "                                     unit=outputColumns[key][1]))\n",
    "    #Loop over sources\n",
    "    for index in range(len(indicesSeparator)-1):\n",
    "        minimalIndex = indicesSeparator[index]+1\n",
    "        maximalIndex = indicesSeparator[index+1]\n",
    "        \n",
    "        if minimalIndex == maximalIndex:\n",
    "            continue\n",
    "        \n",
    "        #Name of the source in the MPC-formatted input file (= transient number)\n",
    "        transientNumber = astcheckOutputFile[minimalIndex:maximalIndex][0].split(\\\n",
    "                                                                 ':')[0].split()\n",
    "        #Lines corresponding to matches in the astcheck output file\n",
    "        matches = astcheckOutputFile[minimalIndex:maximalIndex][1:]\n",
    "        \n",
    "        if len(matches) == 0:\n",
    "            continue\n",
    "        \n",
    "        #Get properties of closest match\n",
    "        matchProperties = re.split('  +', matches[0])\n",
    "        if len(matchProperties) == 7:\n",
    "            identifier, offsetRA, offsetDEC, offset, magnitude, \\\n",
    "            properMotionRA, properMotionDEC = matchProperties\n",
    "        elif len(matchProperties) == 8:\n",
    "            _, identifier, offsetRA, offsetDEC, offset, magnitude, \\\n",
    "            properMotionRA, properMotionDEC = matchProperties\n",
    "        else:\n",
    "            log.critical(\"Match could not be split into correct parameters:\\n{}\"\n",
    "                        .format(matches[0]))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            magnitude = float(magnitude)\n",
    "        except:\n",
    "            log.warning(\"Magnitude '{}' could not be converted to float.\"\n",
    "                        .format(magnitude))\n",
    "            magnitude = None\n",
    "            \n",
    "        #Add match to output table\n",
    "        outputTableRow = [transientNumber, str(identifier), float(offsetRA), \n",
    "                          float(offsetDEC), float(offset), magnitude]\n",
    "        outputTable.add_row(outputTableRow)\n",
    "    \n",
    "    #If a solar system object was matched to multiple transient sources, remove\n",
    "    #all these matches as they are unreliable.\n",
    "    uniqueObjects = np.unique(outputTable['ID_SSO'])\n",
    "    if len(uniqueObjects) != len(outputTable):\n",
    "        for obj in uniqueObjects:\n",
    "            objIndices = np.where(outputTable['ID_SSO'] == obj)[0]\n",
    "            if len(objIndices) > 1:\n",
    "                log.warning(\"{} was matched to multiple transients. Removing \" \n",
    "                            .format(obj) + \"these matches from the SSO \" +\n",
    "                            \"catalogue due to unreliability!\")\n",
    "                outputTable.remove_rows(objIndices)\n",
    "    \n",
    "    #Set dummy parameter (dummy means that there were no matches found)\n",
    "    dummy = False\n",
    "    if len(outputTable) == 0:\n",
    "        dummy = True\n",
    "    \n",
    "    #Create header for SSO catalogue\n",
    "    SSOheader = create_SSOheader(runDirectory, includeComets, dummy,\n",
    "                                timeFunctions)\n",
    "    \n",
    "    write2fitsFile(outputTable, None, [], SSOcatalogueName, \n",
    "                   startHeader=SSOheader)\n",
    "    \n",
    "    log.info(\"Matches saved to SSO catalogue: {}\".format(SSOcatalogueName))\n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t0, label='create_SSOcatalogue')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SSOheader(runDirectory, includeComets, dummy, timeFunctions):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function creates the header for the SSO catalogue.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    runDirectory: string\n",
    "            name of the folder in which the symbolic links to the databases are\n",
    "            are stored. These are used to get the version numbers of the \n",
    "            databases.\n",
    "    includeComets: boolean\n",
    "            Boolean indicating whether comets were included in the known objects\n",
    "            database.\n",
    "    dummy: boolean\n",
    "            Boolean indicating whether the catalogue is a dummy catalogue without\n",
    "            sources (dummy=True). If False, there are sources in the catalogue.\n",
    "    timeFunctions: boolean\n",
    "            Boolean indicating whether functions need to be (wall-)timed.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of create_SSOheader')\n",
    "    log.info(\"Creating SSO header.\")\n",
    "    if timeFunctions:\n",
    "        t0 = time.time()\n",
    "    \n",
    "    #Create empty SSO header\n",
    "    header = fits.Header()\n",
    "    \n",
    "    #Add Python version to SSO header\n",
    "    header['PYTHON-V'] = (platform.python_version(), \"Python version used\")\n",
    "    \n",
    "    #Get C++ version and add to the SSO header. Based on [https://stackoverflow\n",
    "    # .com/questions/44734397/which-c-standard-is-the-default-when-compiling-\n",
    "    # with-g/44735016#44735016]\n",
    "    proc = subprocess.run(\"g++ -dM -E -x c++  /dev/null | grep -F __cplusplus\", \n",
    "                          capture_output=True, shell=True)\n",
    "    CPP_macro = proc.stdout.decode(\"utf-8\").replace(\"\\n\",\"\").split()[-1]\n",
    "    \n",
    "    if CPP_macro not in settingsFile.CPPmacro2version.keys():\n",
    "        log.error(\"C++ macro unknown: {}\".format(CPP_macro))\n",
    "        CPP_version = \"None\"\n",
    "    else:\n",
    "        CPP_version = settingsFile.CPPmacro2version[CPP_macro]\n",
    "    header['CPP-V'] = (CPP_version, \"C++ version used\")\n",
    "    \n",
    "    #Get G++ version and add to SSO header\n",
    "    proc = subprocess.run(\"g++ --version\", capture_output=True, shell=True)\n",
    "    GPP_version = proc.stdout.decode(\"utf-8\").split(\"\\n\")[0].split()[-1]\n",
    "    header['GPP-V'] = (GPP_version, \"G++ version used\")\n",
    "    \n",
    "    #Add match2SSO & header keyword versions to the SSO header\n",
    "    header['SSO-V'] = (__version__, \"match2SSO version used\")\n",
    "    header['SSOKW-V'] = (keywords_version, \n",
    "                         \"SSO header keywords version used\")\n",
    "    \n",
    "    #Get unique strings with git, signifying the latest commit that was made to\n",
    "    #the lunar & jpl_eph repositories and hence signifynig the versions of these\n",
    "    #repositories. Save the strings to the SSO header.\n",
    "    proc = subprocess.run(\"git rev-parse --short=4 HEAD\", capture_output=True, \n",
    "                          shell=True, cwd=\"{}lunar/\".format(softwareFolder))\n",
    "    lunarVersion = proc.stdout.decode(\"utf-8\").replace(\"\\n\",\"\")\n",
    "    header['LUNAR-V'] = (lunarVersion, \"lunar repository version used\")\n",
    "    \n",
    "    proc = subprocess.run(\"git rev-parse --short=4 HEAD\", capture_output=True, \n",
    "                          shell=True, cwd=\"{}jpl_eph/\".format(softwareFolder))\n",
    "    jplEphVersion = proc.stdout.decode(\"utf-8\").replace(\"\\n\",\"\")\n",
    "    header['JPLEPH-V'] = (jplEphVersion, \"jpl_eph repository version used\")\n",
    "    \n",
    "    #Add version of JPL lunar & planetary ephemerides file to SSO header\n",
    "    header['JPLDE-V'] = (\"DE{}\".format(settingsFile.JPL_ephemerisFile.split(\".\"\n",
    "                                       )[-1]),\"JPL ephemeris file version used\")\n",
    "    \n",
    "    #Add asteroid database version & reference epoch to the SSO header.\n",
    "    #The MPCORB.DAT symbolic link in the run directory refers to the \n",
    "    #asteroid database version that was used. The name structure of this \n",
    "    #database is: asteroidDB_version[yyyymmddThhmm]_epoch[yyyymmddThhmm].dat\n",
    "    if os.path.exists(\"{}MPCORB.DAT\".format(runDirectory)):\n",
    "        asteroidDBname = os.readlink(\"{}MPCORB.DAT\".format(runDirectory))\n",
    "        asteroidDBdate = os.path.basename(asteroidDBname).split(\"_\"\n",
    "                                                      )[1].replace(\"version\",\"\")\n",
    "        asteroidDBversion = \"{}-{}-{}T{}:{}\".format(asteroidDBdate[0:4],\n",
    "                                       asteroidDBdate[4:6], asteroidDBdate[6:8],\n",
    "                                    asteroidDBdate[9:11], asteroidDBdate[11:13])\n",
    "        \n",
    "        referenceEpoch = os.path.basename(asteroidDBname).split(\"_\"\n",
    "                                                        )[2].replace(\"epoch\",\"\")\n",
    "        asteroidDBepoch = (\"{}-{}-{}T{}:{}\" .format(referenceEpoch[0:4],\n",
    "                                       referenceEpoch[4:6], referenceEpoch[6:8],\n",
    "                                   referenceEpoch[9:11], referenceEpoch[11:13]))\n",
    "    else:\n",
    "        asteroidDBversion = \"None\"\n",
    "        asteroidDBepoch = \"None\"\n",
    "    header['ASTDB-V'] = (asteroidDBversion, \n",
    "                         \"asteroid database version (date in UTC)\")\n",
    "    header['ASTDB-EP'] = (asteroidDBepoch, \"asteroid database epoch in UTC\")\n",
    "    \n",
    "    #Add comet database version to the SSO header.\n",
    "    #The ELEMENTS.COMET symbolic link in the run directory refers to the \n",
    "    #comet database version that was used. The name structure of this \n",
    "    #database is: cometDB_version[yyyymmddThhmm].dat\n",
    "    cometDBversion = \"None\"\n",
    "    if includeComets:\n",
    "        try:\n",
    "            cometDBname = os.readlink(\"{}ELEMENTS.COMET\".format(runDirectory))\n",
    "            cometDBdate = os.path.basename(cometDBname).split(\"_\"\n",
    "                                                      )[1].replace(\"version\",\"\")\n",
    "            cometDBversion = \"{}-{}-{}T{}:{}\".format(cometDBdate[0:4], \n",
    "                                          cometDBdate[4:6], cometDBdate[6:8], \n",
    "                                          cometDBdate[9:11], cometDBdate[11:13])\n",
    "        except:\n",
    "            #Readlink won't work for an empty comet database that was created \n",
    "            #when includeComet=False, because ELEMENTS.COMET is a file instead \n",
    "            #of a symbolic link.\n",
    "            log.info(\"Empty comet database was used.\")\n",
    "    header['COMDB-V'] = (cometDBversion, \"comet database version (date in UTC)\")\n",
    "    \n",
    "    #Add matching radius and maximum orbital uncertainty parameter to header\n",
    "    header['RADIUS'] = (float(settingsFile.matchingRadius), \n",
    "                        \"matching radius in arcsec\")\n",
    "    header['U-MAX'] = (settingsFile.maxUncertainty, \n",
    "                       \"maximum orbital uncertainty parameter\")\n",
    "    \n",
    "    #Add keyword indicating whether there is\n",
    "    header['SDUMCAT'] = (bool(dummy), \"dummy SSO catalogue without sources?\")\n",
    "    \n",
    "    log.info(\"SSO header complete.\")\n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t0, label='create_SSOheader')\n",
    "    \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubmissionFile(SSOcatalogueName, MPCformattedFileName, MPC_code,\n",
    "                        timeFunctions, overwriteFiles):\n",
    "    \"\"\"\n",
    "    Make an MPC submission file using the SSO catalogue and the MPC-formatted \n",
    "    file that were created within match2SSO to link the transient detections \n",
    "    from a single catalogue to known solar system objects. The detections \n",
    "    corresponding to matches are grouped in a 'known objects submission file'. \n",
    "    The identifiers used in the submission file are the packed designation of \n",
    "    the matching objects. These are the packed permanent designations if \n",
    "    available. Otherwise, the packed provisional designations are used.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    SSOcatalogueName: string\n",
    "            Path to and name of the SSO catalogue of which the matches need to \n",
    "            be converted to a submission file.\n",
    "    MPCformattedFileName: string\n",
    "            Name of the MPC formatted file that was made in match2SSO for the\n",
    "            matching (but does not contain the correct SSO identifiers yet for\n",
    "            submission to the MPC).\n",
    "    MPC_code: string\n",
    "            MPC code corresponding to the telescope.\n",
    "    timeFunctions: boolean\n",
    "            Boolean indicating whether functions need to be (wall-)timed.\n",
    "    overwriteFiles: boolean\n",
    "            Boolean indicating whether files are allowed to be overwritten.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of createSubmissionFile')\n",
    "    log.info(\"Creating MPC submission file.\")\n",
    "    if timeFunctions:\n",
    "        t0 = time.time()\n",
    "    \n",
    "    #Compose submission file name\n",
    "    submissionFileVersion = Time.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "    submissionFileName = \"{}{}_{}.txt\".format(submissionFolder, \n",
    "                          os.path.basename(SSOcatalogueName).replace(\".fits\",\n",
    "                                                                     \"_submit\"),\n",
    "                          submissionFileVersion)\n",
    "    \n",
    "    #Check if file already exists (will only happen when running this function\n",
    "    #multiple times in close succession, as the production time is used in the\n",
    "    #file name)\n",
    "    if not overwriteFiles and os.path.exists(submissionFileName):\n",
    "        log.info(\"Submission file already exists and will not be re-made.\")\n",
    "        return\n",
    "    \n",
    "    #Open SSO catalogue\n",
    "    with fits.open(SSOcatalogueName) as hdu:\n",
    "        SSOcatalogue = Table(hdu[1].data)\n",
    "    \n",
    "    #Check SSO catalogue for matches\n",
    "    if len(SSOcatalogue)==0:\n",
    "        log.info(\"No matches found. Submission file will not be made.\")\n",
    "        return\n",
    "    \n",
    "    #Create submission file (in same folder as SSO catalogue)\n",
    "    log.info(\"Making submission file {}\".format(submissionFileName))\n",
    "    if os.path.exists(submissionFileName):\n",
    "        log.warning(\"MPC submission file {} is overwritten.\"\n",
    "                    .format(submissionFileName))\n",
    "    \n",
    "    submissionFile = open(submissionFileName, 'w')\n",
    "    \n",
    "    #Create header for the submission file\n",
    "    submissionHeader = createSubmissionHeader(submissionFileName, MPC_code,\n",
    "                                             timeFunctions)\n",
    "    submissionFile.write(submissionHeader)\n",
    "    \n",
    "    #Open MPC-formatted file\n",
    "    detections_MPCformat = pd.read_fwf(MPCformattedFileName, widths=[14,66],\n",
    "                                 names=[\"char1to14\", \"char15to80\"],\n",
    "                                 dtype={'char1to14':np.int32, 'char15to80':str})\n",
    "    \n",
    "    #For each detection that was matched to a known solar system object, \n",
    "    #get the packed designation of the matching object and write the detection\n",
    "    #to the submission file.\n",
    "    for matchIndex, transientNumber in enumerate(SSOcatalogue[numberColumn]):\n",
    "        designation = SSOcatalogue[\"ID_SSO\"][matchIndex].strip()\n",
    "        #Get packed designation\n",
    "        if re.match(\"^[0-9]{4}\\s[A-Z]\", designation) or \"/\" in designation:\n",
    "            #Asteroid or comet with provisional designation (or survey \n",
    "            #designation)\n",
    "            packedDesignation = packProvisionalDesignation(designation)\n",
    "            if packedDesignation == None:\n",
    "                continue\n",
    "            char1to12 = \"    {}\".format(packedDesignation)\n",
    "            \n",
    "        else:\n",
    "            #Asteroid or comet with permanent designation\n",
    "            packedDesignation, fragment = packPermanentDesignation(designation)\n",
    "            if packedDesignation == None:\n",
    "                continue\n",
    "            char1to12 = \"{}{}\".format(packedDesignation, fragment.rjust(7))\n",
    "            \n",
    "        #Get detection details\n",
    "        detectionIndex = np.where(np.array(detections_MPCformat[\"char1to14\"]) ==\n",
    "                                  int(transientNumber))[0]\n",
    "        if len(detectionIndex) != 1:\n",
    "            log.error(\"{} detections found that correspond to transient number \"\n",
    "                      .format(len(detectionIndex)) + \"{}. Should be only one.\"\n",
    "                      .format(transientNumber))\n",
    "            continue\n",
    "        detectionDetails = detections_MPCformat[\"char15to80\"][detectionIndex[0]]\n",
    "        \n",
    "        #Write detection to submission file\n",
    "        detectionLine = \"{}  {}\".format(char1to12, detectionDetails)\n",
    "        if len(detectionLine) != 80:\n",
    "            log.error(\"Detection not formatted correctly in 80 columns:\\n{}\"\n",
    "                     .format(detectionLine))\n",
    "        submissionFile.write(detectionLine+\"\\n\")\n",
    "    \n",
    "    submissionFile.close()\n",
    "    log.info(\"MPC submission file saved to {}\".format(submissionFileName))\n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t0, label='createSubmissionFile')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubmissionHeader(submissionFileName, MPC_code, timeFunctions, \n",
    "                           comment=None):\n",
    "    \"\"\"\n",
    "    Function composes the header of the MPC submission file corresponding to a\n",
    "    single transient catalogue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    submissionFileName: string\n",
    "            Name of the submission file for which the header is composed.\n",
    "    MPC_code: string\n",
    "            MPC code of the telescope with which the observation was made.\n",
    "    timeFunctions: boolean\n",
    "            Boolean indicating whether functions need to be (wall-)timed.\n",
    "    comment: string\n",
    "            Comment to be added to the header in the COM line. By default, this\n",
    "            is None, meaning that the COM line is not added to the header.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of createSubmissionHeader')\n",
    "    log.info(\"Creating header for submission file.\")\n",
    "    if timeFunctions:\n",
    "        t0 = time.time()\n",
    "    \n",
    "    firstline = \"COD {}\\n\".format(MPC_code)\n",
    "    defaultHeader = (\n",
    "       \"CON Radboud University, Houtlaan 4, 6525XZ, Nijmegen, The Netherlands\\n\"\n",
    "       +\"CON [p.groot@astro.ru.nl]\\n\"\n",
    "       +\"OBS P. J. Groot, S. L. D. Bloemen, L. Townsend\\n\"\n",
    "       +\"MEA P. M. Vreeswijk, D. L. A. Pieterse, K. Paterson\\n\"\n",
    "       +\"TEL 0.65-m reflector + CCD\\n\"\n",
    "       +\"NET Gaia-DR2\\n\"\n",
    "       +\"AC2 mpc-response@blackgem.org\\n\")\n",
    "    \n",
    "\n",
    "    #Special cases for which a phrase needs to be included in the ACK line \n",
    "    #of the header of the MPC submission file:\n",
    "    #neocand = \"NEO CANDIDATE\" #submitting new NEO candidate\n",
    "    #neocp = \"NEOCP\"           #submitting observations of NEOCP objects\n",
    "    #Add ACK line to the header of the MPC submission file\n",
    "    \n",
    "    ACK_line = \"ACK {}\\n\".format(Path(submissionFileName).stem)\n",
    "    if len(ACK_line)>82:\n",
    "        log.error(\"ACK line in submission file {} is too long!\"\n",
    "                  .format(submissionFileName))\n",
    "    \n",
    "    #Add COM line to the header\n",
    "    COM_line = ''\n",
    "    if comment != None:\n",
    "        if len(comment)>76:\n",
    "            log.warning(\"COM line is too long and therefore not used. \" \n",
    "                        +\"Use at most 76 characters!\")\n",
    "        else:\n",
    "            COM_line = \"COM {}\\n\".format(comment)\n",
    "    \n",
    "    log.info(\"Submission file header complete.\")\n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t0, label='createSubmissionHeader')\n",
    "    \n",
    "    return firstline + defaultHeader + ACK_line + COM_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to make an MPC submission file\n",
    "Convert asteroid designations to their packed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviateNumber(num):\n",
    "    \n",
    "    \"\"\"\n",
    "    Number packing function needed to pack MPC designations.\n",
    "    \"\"\"\n",
    "    \n",
    "    Ndict = {str(index): letter for index, letter in \\\n",
    "             enumerate(ascii_uppercase + ascii_lowercase, start=10)}\n",
    "    \n",
    "    if int(num)>9:\n",
    "        return Ndict[str(num)]\n",
    "    else:\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def packProvisionalDesignation(fullDesignation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function converts provisional minor planet designation into its packed form,\n",
    "    using the definitions given in\n",
    "    https://www.minorplanetcenter.net/iau/info/PackedDes.html#prov .\n",
    "    As described in https://www.minorplanetcenter.net/iau/info/OpticalObs.html,\n",
    "    for comets a character is added in front of the provisional designation (at\n",
    "    column 5), describing the comet type. For asteroids, we add a space in front\n",
    "    so that the returned string is 8 characters long (spanning columns 5-12 in\n",
    "    the submission file). \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fullDesignation: string\n",
    "            Unpacked provisional designation assigned to the object by the MPC.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Remove space before or after designation\n",
    "    fullDesignation = fullDesignation.strip()\n",
    "    \n",
    "    #Their are four special survey designation forms (for surveys that were \n",
    "    #undertaken between 1960 and 1977)that should be packed differently\n",
    "    surveyStrings = [\"P-L\", \"T-1\", \"T-2\", \"T-3\"]\n",
    "    for surveyString in surveyStrings:\n",
    "        if re.match(\"^[0-9]{4}\\s\"+\"{}$\".format(surveyString),fullDesignation):\n",
    "            packedDesignation = \"{}S{}\".format(surveyString.replace(\"-\",\"\"), \n",
    "                                              fullDesignation[:4])\n",
    "            return packedDesignation\n",
    "    \n",
    "    packYear = {18: \"I\", 19: \"J\", 20: \"K\"}\n",
    "    \n",
    "    def packCycleNumber(Ncycle):\n",
    "        \n",
    "        \"\"\"Input parameter Ncycle is a string of 0-3 digits.\"\"\"\n",
    "        \n",
    "        if len(Ncycle) == 0:\n",
    "            return \"00\"\n",
    "        \n",
    "        elif int(Ncycle)>99:\n",
    "            return \"{}{}\".format(abbreviateNumber(Ncycle[0:2]), Ncycle[2])\n",
    "        \n",
    "        return \"{:0>2}\".format(Ncycle)\n",
    "    \n",
    "    \n",
    "    #For a comet\n",
    "    if \"/\" in fullDesignation:\n",
    "        cometType, designation = fullDesignation.split(\"/\")\n",
    "        \n",
    "        #In case of a comet fragment, the last character of the packed \n",
    "        #designation is the fragment letter. Otherwise, it is zero.\n",
    "        fragment = \"0\"\n",
    "        if '-' in designation:\n",
    "            designation, fragment = designation.split(\"-\")\n",
    "            fragment = fragment.lower()\n",
    "        year, remainder = designation.split(\" \")\n",
    "        \n",
    "        if int(year[:2]) not in packYear.keys():\n",
    "            log.error(\"Provisional designation of comet {} \"\n",
    "                     .format(fullDesignation) +\n",
    "                     \"cannot be packed. Skipping it.\")\n",
    "            return None\n",
    "        \n",
    "        packedYear = \"{}{}\".format(packYear[int(year[:2])], year[2:])\n",
    "        \n",
    "        \n",
    "        #In case there are two letters after the space in the designation. This\n",
    "        #can be the case if the object was thought to be an asteroid early on.\n",
    "        if remainder[1].isalpha():\n",
    "            \n",
    "            if fragment!=\"0\":\n",
    "                #A comet with two letters in its provisional designation after\n",
    "                #the space and a fragment letter cannot be submitted in the old\n",
    "                #submission format. It can in the new ADES format, but we are\n",
    "                #not yet using this. Skip detection.\n",
    "                log.error(\"Provisional designation of comet {} \"\n",
    "                          .format(fullDesignation) +\n",
    "                          \"cannot be packed. Skipping it.\")\n",
    "                return None\n",
    "            \n",
    "            #Although this object is not a fragment, its provisional designation\n",
    "            #does contain a second letter after the space which should be \n",
    "            #written to the same position as the fragment letter.\n",
    "            fragment = remainder[1]\n",
    "            remainder = \"{}{}\".format(remainder[0], remainder[2:])\n",
    "            \n",
    "        #There should be at most three digits after the space-letter combination\n",
    "        #in the provisional designation\n",
    "        if len(remainder)>4:\n",
    "            log.error(\"Unclear how to pack provisional designation of comet \"\n",
    "                      + \"{}. Skipping it.\".format(fullDesignation))\n",
    "            return None\n",
    "        \n",
    "        if int(year[:2]) not in packYear.keys():\n",
    "            log.error(\"Data from before 1800 or after 2099 cannot be assigned a\"\n",
    "                      + \" packed provisional designation.\")\n",
    "            return None\n",
    "        \n",
    "        packedDesignation = (\"{}{}{}{}{}\".format(cometType, packedYear,\n",
    "                        remainder[0], packCycleNumber(remainder[1:]), fragment))\n",
    "        return packedDesignation\n",
    "    \n",
    "    #For an asteroid\n",
    "    if int(fullDesignation[:2]) not in packYear.keys():\n",
    "        log.error(\"Provisional designation of asteroid {} \"\n",
    "                 .format(fullDesignation) +\n",
    "                 \"cannot be packed. Skipping it.\")\n",
    "        return None\n",
    "    \n",
    "    packedYear = \"{}{}\".format(packYear[int(fullDesignation[:2])], \n",
    "                               fullDesignation[2:4])\n",
    "    packedDesignation = (\" {}{}{}{}\".format(packedYear, fullDesignation[5],\n",
    "                                           packCycleNumber(fullDesignation[7:]),\n",
    "                                           fullDesignation[6]))\n",
    "    \n",
    "    #Final check\n",
    "    if len(packedDesignation)!=8:\n",
    "        log.error(\"Packed provisional designation is of incorrect length: '{}'\"\n",
    "                 .format(packedDesignation))\n",
    "        return None\n",
    "    \n",
    "    return packedDesignation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def packPermanentDesignation(fullDesignation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function converts the permanent minor planet designation into its packed \n",
    "    form (5 characters), using the definitions given in\n",
    "    https://www.minorplanetcenter.net/iau/info/PackedDes.html#perm\n",
    "    Return the packed designation and - if applicable - the letter \n",
    "    corresponding to the comet fragment. If the object is not a comet fragment,\n",
    "    an empty string will be returned for the fragment letter.\n",
    "    Parameters\n",
    "    ----------\n",
    "    fullDesignation: string\n",
    "            Unpacked permanent designation assigned to the object by the MPC.\n",
    "    \"\"\"\n",
    "    fragment = ''\n",
    "    \n",
    "    if not fullDesignation.isdigit():\n",
    "        #Object is a comet\n",
    "        if len(fullDesignation.split(\"-\"))==2:\n",
    "            designation, fragment = fullDesignation.split(\"-\")\n",
    "        else:\n",
    "            designation = fullDesignation\n",
    "            fragment = ''\n",
    "        packedDesignation = designation.zfill(5)\n",
    "        fragment = fragment.lower()\n",
    "        \n",
    "    elif int(fullDesignation)<99999:\n",
    "        packedDesignation = \"{:0>5}\".format(int(fullDesignation))\n",
    "    \n",
    "    elif int(fullDesignation)<620000:\n",
    "        quotient = int(fullDesignation)//10000\n",
    "        packedDesignation = \"{}{:0>4}\".format(abbreviateNumber(quotient), \n",
    "                                              int(fullDesignation)%10000)\n",
    "    \n",
    "    else:\n",
    "        remainder = int(fullDesignation) - 620000\n",
    "        quotient3 = remainder//62**3\n",
    "        remainder -= quotient3*62**3\n",
    "        quotient2 = remainder//62**2\n",
    "        remainder -= quotient2*62**2\n",
    "        quotient1 = remainder//62\n",
    "        remainder -= quotient1*62\n",
    "        packedDesignation = \"~{}{}{}{}\".format(abbreviateNumber(quotient3), \n",
    "                                               abbreviateNumber(quotient2), \n",
    "                                               abbreviateNumber(quotient1), \n",
    "                                               abbreviateNumber(remainder))\n",
    "    #Final check\n",
    "    if len(packedDesignation)!=5:\n",
    "        log.error(\"Packed permanent designation is of incorrect length: '{}'\"\n",
    "                  .format(packedDesignation))\n",
    "        return None, ''\n",
    "    \n",
    "    return packedDesignation, fragment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BlackBOX function\n",
    "# from https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkInputParameters(mode, cat2process, date2process, list2process):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if the correct (combination of) input parameters was/were defined for\n",
    "    run_match2SSO. If so, this function returns True. Otherwise, False is \n",
    "    returned.\n",
    "    \"\"\"\n",
    "    if mode not in [\"day\", \"night\", \"historic\"]:\n",
    "        print(\"CRITICAL: unknown mode.\")\n",
    "        return False\n",
    "    \n",
    "    if mode==\"day\":\n",
    "        if cat2process!=None:\n",
    "            print(\"CRITICAL: when processing a specified catalog, the mode must\"\n",
    "                  \" be 'historic' or 'night'.\")\n",
    "            return False\n",
    "        if list2process!=None:\n",
    "            print(\"CRITICAL: when processing a catalog list, the mode must be \"\n",
    "                  \"'historic'.\")\n",
    "            return False\n",
    "    \n",
    "    elif mode==\"night\":\n",
    "        if cat2process==None:\n",
    "            print(\"Critical: --catalog needs to be specified when running \"\n",
    "                  \"match2SSO in night mode.\")\n",
    "            return False\n",
    "        if date2process!=None:\n",
    "            print(\"CRITICAL: when processing a specified date, the mode must \"\n",
    "                  \"be 'historic' or 'day'.\")\n",
    "            return False\n",
    "        if list2process!=None:\n",
    "            print(\"CRITICAL: when processing a catalog list, the mode must be \"\n",
    "                  \"'historic'.\")\n",
    "            return False\n",
    "    \n",
    "    elif mode==\"historic\" and cat2process==None and (date2process==None and\n",
    "                                                     list2process==None):\n",
    "        print(\"CRITICAL: --date, --catalog and --catlist are all None. Nothing \"\n",
    "              \"to process.\")\n",
    "        return False\n",
    "        \n",
    "    if (date2process!=None and cat2process!=None) or (date2process!=None and\n",
    "        list2process!=None) or (cat2process!=None and list2process!=None):\n",
    "        print(\"CRITICAL: either specify --date, --catalog OR --catlist. A \"\n",
    "              \"combination is not allowed.\")\n",
    "        return False\n",
    "    \n",
    "    if cat2process!=None:\n",
    "        if \"trans\" not in cat2process:\n",
    "            print(\"CRITICAL: specified catalog should correspond to a transient\"\n",
    "                  \" catalog.\")\n",
    "            return False\n",
    "        if not os.path.exists(cat2process):\n",
    "            print(\"CRITICAL: the specified catalog does not exist.\")\n",
    "            return False\n",
    "    \n",
    "    if list2process!=None and not os.path.exists(list2process):\n",
    "        print(\"CRITICAL: the specified catalog list does not exist.\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSettingsFile(tel):\n",
    "    \n",
    "    #Set up global variables for all folders in the settings file and ensure the\n",
    "    #paths end with a slash\n",
    "    global inputDataFolder\n",
    "    inputDataFolder = get_par(settingsFile.inputDataFolder, tel)\n",
    "    if inputDataFolder[-1] != \"/\":\n",
    "        inputDataFolder += \"/\"\n",
    "    \n",
    "    global softwareFolder\n",
    "    softwareFolder = get_par(settingsFile.softwareFolder, tel)\n",
    "    if softwareFolder[-1] != \"/\":\n",
    "        softwareFolder += \"/\"\n",
    "    \n",
    "    global databaseFolder\n",
    "    databaseFolder = get_par(settingsFile.databaseFolder, tel)\n",
    "    if databaseFolder[-1] != \"/\":\n",
    "        databaseFolder += \"/\"\n",
    "    \n",
    "    global logFolder\n",
    "    logFolder = get_par(settingsFile.logFolder, tel)\n",
    "    if logFolder[-1] != \"/\":\n",
    "        logFolder += \"/\"\n",
    "    \n",
    "    global submissionFolder\n",
    "    submissionFolder = get_par(settingsFile.submissionFolder, tel)\n",
    "    if submissionFolder[-1] != \"/\":\n",
    "        submissionFolder += \"/\"\n",
    "    \n",
    "    #Check if database folder and submission folder exists (it will be needed / \n",
    "    #prepared in any mode) and create the folders if they don't exist\n",
    "    if not os.path.isdir(databaseFolder):\n",
    "        os.makedirs(databaseFolder)\n",
    "    if not os.path.exists(submissionFolder):\n",
    "        os.makedirs(submissionFolder)\n",
    "    \n",
    "    #Check if software folder exists. If it doesn't, exit match2SSO\n",
    "    if not os.path.isdir(softwareFolder):\n",
    "        print(\"CRITICAL: software folder given in settings file doesn't exist.\")\n",
    "        return False\n",
    "    \n",
    "    #Check that astcheck parameters are numbers\n",
    "    if not (isinstance(settingsFile.matchingRadius, float) or \n",
    "            isinstance(settingsFile.matchingRadius, int)):\n",
    "        print(\"CRITICAL: incorrectly specified matching radius in settings \"\n",
    "              +\"file. Must be float or integer.\")\n",
    "        return False\n",
    "    \n",
    "    if not (isinstance(settingsFile.limitingMagnitude, float) or\n",
    "            isinstance(settingsFile.limitingMagnitude, int)):\n",
    "        print(\"CRITICAL: incorrectly specified limiting mag. in settings file.\")\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(settingsFile.maximalNumberOfAsteroids, int):\n",
    "        print(\"CRITICAL: incorrectly specified max. number of asteroids in \"\n",
    "              +\"settings file.\")\n",
    "        return False\n",
    "    \n",
    "    if (not isinstance(settingsFile.maxUncertainty, int) and \n",
    "        settingsFile.maxUncertainty != None):\n",
    "        print(\"CRITICAL: incorrectly specified max. uncertainty in settings \"\n",
    "              +\"file. Must be 0-9 or None.\")\n",
    "        return False\n",
    "        \n",
    "    #Check if JPL ephemeris file exists\n",
    "    if not os.path.exists(settingsFile.JPL_ephemerisFile):\n",
    "        print(\"CRITICAL: JPL ephemeris file specified in settings file doesn't \"\n",
    "              +\"exist.\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setUpLogFile(logName):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function creates log file and configures the log handler.\n",
    "    \"\"\"\n",
    "    \n",
    "    if logName==None:\n",
    "        return\n",
    "    \n",
    "    logDir, logFileName = os.path.split(logName)\n",
    "    \n",
    "    #If no folder is specified, use the log folder from the settings file.\n",
    "    if len(logDir)==0:\n",
    "        logDir = logFolder\n",
    "    \n",
    "    #Create folder to store log in, if it does not yet exist\n",
    "    if not os.path.isdir(logDir):\n",
    "        os.makedirs(logDir)\n",
    "    \n",
    "    #Configure log handling\n",
    "    logFile = \"{}/{}\" .format(logDir, logFileName)\n",
    "    if os.path.exists(logFile):\n",
    "        filePathAndName, extension = os.path.splitext(logFile)\n",
    "        logFile = \"{}_{}{}\".format(filePathAndName, \n",
    "                                Time.now().strftime(\"%Y%m%d_%H%M%S\"), extension)\n",
    "        print(\"Log file already exists. Creating a new log named {}\"\n",
    "              .format(logFile))\n",
    "        \n",
    "    fileHandler = logging.FileHandler(logFile, 'a')\n",
    "    fileHandler.setFormatter(logFormatter)\n",
    "    fileHandler.setLevel('INFO')\n",
    "    log.addHandler(fileHandler)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZOGY function\n",
    "def get_par(par, tel):\n",
    "    \n",
    "    \"\"\"Function to check if [par] is a dictionary with one of the keys\n",
    "       being [tel] or the alphabetic part of [tel] (e.g. 'BG'), and if\n",
    "       so, return the corresponding value. Otherwise just return the\n",
    "       parameter value.\"\"\"\n",
    "\n",
    "    par_val = par\n",
    "    if type(par) is dict:\n",
    "        if tel in par:\n",
    "            par_val = par[tel]\n",
    "        else:\n",
    "            # cut off digits from [tel]\n",
    "            tel_base = ''.join([char for char in tel if char.isalpha()])\n",
    "            if tel_base in par:\n",
    "                par_val = par[tel_base]\n",
    "        \n",
    "    return par_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZOGY function\n",
    "def mem_use(label=''):\n",
    "\n",
    "    # ru_maxrss is in units of kilobytes on Linux; however, this seems\n",
    "    # to be OS dependent as on mac os it is in units of bytes; see\n",
    "    # manpages of \"getrusage\"\n",
    "    if sys.platform=='darwin':\n",
    "        norm = 1024**3\n",
    "    else:\n",
    "        norm = 1024**2\n",
    "        \n",
    "    mem_max = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/norm\n",
    "    mem_now = psutil.Process().memory_info().rss / 1024**3\n",
    "    mem_virt = psutil.Process().memory_info().vms / 1024**3\n",
    "    \n",
    "    log.info ('memory use [GB]: rss={:.3f}, maxrss={:.3f}, vms={:.3f} in {}'\n",
    "              .format(mem_now, mem_max, mem_virt, label))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZOGY function\n",
    "def log_timing_memory(t_in, label=''):\n",
    "  \n",
    "    log.info('wall-time spent in {}: {:.3f} s'.format(label, time.time()-t_in))\n",
    "    mem_use(label=label)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransientFileNames(minimalDate, maximalDate, tel, timeFunctions,\n",
    "                         excludeFlagged=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function returns a list with the transient file names that were taken\n",
    "    between the minimal and maximal specified dates. The input dates should be\n",
    "    Time objects. If excludeFlagged is True, the dummy transient catalogues\n",
    "    (which are red-flagged) are excluded.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    minimalDate: datetime object, incl time zone\n",
    "            Minimal observation date of the time block for which the \n",
    "            observations are selected.\n",
    "    maximalDate: datetime object, incl time zone\n",
    "            Maximal observation date of the time block for which the\n",
    "            observations are selected.\n",
    "    tel: string\n",
    "            Telescope abbreviation.\n",
    "    timeFunctions: boolean\n",
    "            Boolean indicating whether functions need to be (wall-)timed.\n",
    "    excludeFlagged: boolean\n",
    "            Boolean indicating whether red-flagged (dummy) catalogues should be\n",
    "            excluded or not.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of getTransientFileNames')\n",
    "    log.info(\"Selecting transient catalogues between {} and {}.\" \\\n",
    "             .format(minimalDate.strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "                     maximalDate.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    if timeFunctions:\n",
    "        t0 = time.time()\n",
    "    \n",
    "    #Convert to local time\n",
    "    localTimeZone = timezone(get_par(settingsFile.timeZoneTelescope, tel))\n",
    "    minimalDate = minimalDate.astimezone(localTimeZone)\n",
    "    maximalDate = maximalDate.astimezone(localTimeZone)\n",
    "    \n",
    "    #Select the transient files by observation date\n",
    "    year, month, day = \"*\", \"*\", \"*\"\n",
    "    if minimalDate.year == maximalDate.year:\n",
    "        year = \"%d\"%(minimalDate.year)\n",
    "        if minimalDate.month == maximalDate.month:\n",
    "            month = \"{:0>2}\".format(minimalDate.month)\n",
    "            if minimalDate.day == maximalDate.day:\n",
    "                day = \"{:0>2}\".format(maximalDate.day)\n",
    "    transientFiles = glob.glob(os.path.join(inputDataFolder, \n",
    "                                            \"%s/%s/%s/*_trans_light.fits\"\n",
    "                                            %(year, month, day)))\n",
    "    if len(transientFiles)==0:\n",
    "        return []\n",
    "    \n",
    "    files2process = []\n",
    "    for transientFile in transientFiles:\n",
    "        #Parse date encoded in filename and compare with our limits\n",
    "        #(e.g. ML1_20200517_034221_red_trans_light.fits)\n",
    "        splittedFileName = os.path.basename(transientFile).split(\"_\")\n",
    "        date_obs = splittedFileName[1]\n",
    "        time_obs = splittedFileName[2]\n",
    "        observationTime = Time.strptime(date_obs+time_obs, \"%Y%m%d%H%M%S\").mjd\n",
    "        if (observationTime >= Time(minimalDate).mjd and \n",
    "            observationTime <= Time(maximalDate).mjd):\n",
    "            with fits.open(transientFile) as hdu:\n",
    "                header = hdu[1].header\n",
    "            \n",
    "            if not excludeFlagged:\n",
    "                files2process.append(transientFile)\n",
    "            else:\n",
    "                log.info(\"Excluding red-flagged (dummy) catalogues.\")\n",
    "                \n",
    "                if dummyColumn not in header.keys():\n",
    "                    log.critical(\"{} not in the header!\".format(dummyColumn))\n",
    "                    return []\n",
    "                \n",
    "                if header[dummyColumn]==False:\n",
    "                    files2process.append(transientFile)\n",
    "    \n",
    "    log.info(\"{} transient catalogues have been selected.\"\n",
    "             .format(len(files2process)))\n",
    "    if timeFunctions:\n",
    "        log_timing_memory(t0, label='getTransientFileNames')\n",
    "    return files2process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNightStartFromCatalogueName(catalogueName, tel, noonType=\"local\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the noon corresponding to the start of the\n",
    "    observation night, as a datetime object. This is either the local noon or\n",
    "    the noon in UTC, as specified. The noon is deduced from the information in\n",
    "    the catalogue name (e.g. ML1_yyyymmdd_hhmmss_red_trans_light.fits).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    catalogueName: string\n",
    "            Name of the catalogue corresponding to an observation that took \n",
    "            place on the observation night for which the noon that signifies the\n",
    "            start of the night must be determined.\n",
    "    tel: string\n",
    "            Telescope abbreviation.\n",
    "    noonType: string\n",
    "            Must be either \"local\" or \"utc\". If \"utc\", this function will return\n",
    "            the noon corresponding to the start of the night in UTC. This can be\n",
    "            different from the local noon.\n",
    "    \"\"\"\n",
    "    noonType = noonType.lower()\n",
    "    \n",
    "    #Get observation time from filename and define as being in UTC\n",
    "    splittedFileName = os.path.basename(catalogueName).split(\"_\")\n",
    "    observationDate = splittedFileName[1]\n",
    "    observationHour = splittedFileName[2]\n",
    "    observationTime = datetime.strptime(observationDate+\" \"+observationHour, \n",
    "                                   \"%Y%m%d %H%M%S\").replace(tzinfo=pytz.utc)\n",
    "    \n",
    "    if noonType == \"local\":\n",
    "        localTimeZone = timezone(get_par(settingsFile.timeZoneTelescope, tel))\n",
    "        \n",
    "        #Get local noon corresponding to the start of the observing night\n",
    "        local_noon = localTimeZone.localize(datetime.strptime(observationDate+\n",
    "                                                     \" 120000\",\"%Y%m%d %H%M%S\"))\n",
    "        #Get date of observing night\n",
    "        if observationTime < local_noon:\n",
    "            date = (observationTime - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "        else:\n",
    "            date = observationTime.strftime(\"%Y%m%d\")\n",
    "        \n",
    "        #Make local noon variable\n",
    "        startNight = localTimeZone.localize(datetime.strptime(date+\" 120000\",\n",
    "                                                              \"%Y%m%d %H%M%S\"))\n",
    "    else:\n",
    "        if noonType != \"utc\":\n",
    "            log.error(\"Noon type not understood. Assuming noon in utc.\")\n",
    "        \n",
    "        startNight = pytz.utc.localize(datetime.strptime(\n",
    "                                    observationDate+\" 120000\",\"%Y%m%d %H%M%S\"))\n",
    "        if int(observationHour[:2])<12.:\n",
    "            startNight = startNight - timedelta(days=1)\n",
    "    \n",
    "    return startNight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkInputCatalogue(catalogueName):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if the input catalogue exists and if it is a dummy (red-flagged)\n",
    "    catalogue or not. If a light version of the catalogue is available, use that\n",
    "    version. This function returns a boolean for \"does the catalogue exist?\", a\n",
    "    boolean for \"is the catalogue a dummy?\" and the catalogue name is returned,\n",
    "    as the light version might have been selected instead of the transient\n",
    "    catalogue that includes the thumbnails.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Check whether the (light) catalogue exists and ensure the use of the light\n",
    "    #version of the catalogue if it is available (better in terms of memory \n",
    "    #usage & processing speed)\n",
    "    if \"_light\" not in catalogueName:\n",
    "        lightCatalogueName = catalogueName.replace(\"_trans\", \"_trans_light\")\n",
    "        if os.path.exists(lightCatalogueName):\n",
    "            catalogueName = lightCatalogueName\n",
    "            \n",
    "    if not os.path.exists(catalogueName):\n",
    "        log.critical(\"The specified catalog does not exist:\\n{}\"\n",
    "                     .format(catalogueName))\n",
    "        return False, None, catalogueName\n",
    "            \n",
    "    #Check quality control flag of the catalogue\n",
    "    with fits.open(catalogueName) as hdu:\n",
    "        header = hdu[1].header\n",
    "            \n",
    "    if dummyColumn not in header.keys():\n",
    "        log.critical(\"{} not in the header of {}!\"\n",
    "                     .format(dummyColumn, catalogueName))\n",
    "        return False, None, catalogueName\n",
    "\n",
    "    if header[dummyColumn]:\n",
    "        log.info(\"{} is a dummy catalogue.\".format(catalogueName))\n",
    "        return True, True, catalogueName\n",
    "    \n",
    "    return True, False, catalogueName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForDatabaseProducts(runDirectory):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function checks if the database products that astcheck needs in order\n",
    "    to process transient catalogues are located in the runDirectory. These are\n",
    "    the known objects catalogue, the symbolic link to the asteroid catalogue\n",
    "    (needed for reading out the asteroid database version) and ELEMENTS.COMET,\n",
    "    which is either an empty comet database (if comets should not be included in\n",
    "    the matching) or a symbolic link to the comet database used (again needed to\n",
    "    read out the version).\n",
    "    The function also indirectly checks for the existence of the runDirectory.\n",
    "    A boolean is returned: True if all is well and False if something is\n",
    "    missing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    runDirectory: string\n",
    "            Directory in which astcheck will be run.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Check for known objects catalogue\n",
    "    if not os.path.exists(\"{}mpcorb.sof\".format(runDirectory)):\n",
    "        log.critical(\"The known objects catalogue (SOF format) could not be \"\n",
    "                     + \"found.\")\n",
    "        return False\n",
    "    \n",
    "    #Check for symbolic links pointing to the used version of the SSO databases\n",
    "    if not os.path.exists(\"{}MPCORB.DAT\".format(runDirectory)):\n",
    "        log.critical(\"MPCORB.DAT could not be found\")\n",
    "        return False\n",
    "    if not os.path.exists(\"{}ELEMENTS.COMET\".format(runDirectory)):\n",
    "        log.critical(\"ELEMENTS.COMET could not be found\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write2fitsFile(data, header, headerKeys, outputFileName, startHeader=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function formats the output data, composes the header and combines the two\n",
    "    into a hdu table. The table is then written to a fits file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : table data\n",
    "            Table data which is to be used as the data for the output fits\n",
    "            table.\n",
    "    header: header\n",
    "            Header from which certain keywords are copied to the header of the\n",
    "            output catalogue.\n",
    "    headerKeys: list of strings\n",
    "            Contains names of header keywords from the header mentioned above, \n",
    "            that will be included in the output catalogue header.\n",
    "    outputFileName: string\n",
    "            File name (including path) under which the output binary fits table \n",
    "            will be stored.\n",
    "    startHeader: header\n",
    "            Header which will be included in the header of the output catalogue.\n",
    "            startHeader can be None.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of write2fitsFile')\n",
    "    \n",
    "    #Format fits table data\n",
    "    columns = []\n",
    "    for columnName in data.columns:\n",
    "        \n",
    "        columnFormat = data[columnName].dtype\n",
    "        \n",
    "        #Converting bytestring format to fits format does not work properly for\n",
    "        #strings, as the length is not taken into account properly. Manually\n",
    "        #correct this.\n",
    "        if 'S' in columnFormat.str:\n",
    "            stringLength = columnFormat.str.split(\"S\")[-1]\n",
    "            columnFormat = \"{}A\".format(stringLength)\n",
    "        \n",
    "        columnUnit = str(data[columnName].unit)\n",
    "        if columnUnit==\"None\":\n",
    "            columnUnit = \"\"\n",
    "        \n",
    "        column = fits.Column(name=columnName, format=columnFormat,\n",
    "                             unit=columnUnit, array=data[columnName])\n",
    "        columns.append(column)\n",
    "    \n",
    "    #Compose fits table header\n",
    "    if startHeader != None:\n",
    "        header = startHeader\n",
    "    else:\n",
    "        header = fits.Header()\n",
    "        \n",
    "    for key in headerKeys:\n",
    "        header[key] = (header[key], header.comments[key])\n",
    "   \n",
    "    #Combine formatted fits columns and header into output binary fits table\n",
    "    fitsTable = fits.BinTableHDU.from_columns(columns, header=header)\n",
    "    fitsTable.writeto(outputFileName, overwrite=True)\n",
    "    \n",
    "    mem_use(label='at end of write2fitsFile')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on BlackBOX function clean_tmp\n",
    "def removeTemporaryFolder(tmp_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that removes the specified folder and its contents.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.isdir(tmp_path):\n",
    "        shutil.rmtree(tmp_path)\n",
    "        log.info ('Removing temporary folder: {}'.format(tmp_path))\n",
    "    \n",
    "    else:\n",
    "        log.warning ('tmp folder {} does not exist'.format(tmp_path))\n",
    "    \n",
    "    mem_use(label='after removing temporary folder')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run match2SSO using command line parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"User parameters\")\n",
    "    \n",
    "    parser.add_argument(\"--telescope\", type=str, default=\"ML1\", \n",
    "                        help=\"Telescope name (ML1, BG2, BG3 or BG4); \"\n",
    "                        \"default='ML1'\")\n",
    "    \n",
    "    parser.add_argument(\"--mode\", type=str, default=\"historic\", \n",
    "                        help=\"Day, night or historic mode of pipeline; \"\n",
    "                        \"default='historic'\")\n",
    "    \n",
    "    parser.add_argument(\"--catalog\", type=str, default=None, \n",
    "                        help=\"Only process this particular transient catalog. \"\n",
    "                        \"Requires full path and requires mode to be 'historic' \"\n",
    "                        \"or 'night'; default=None\")\n",
    "    \n",
    "    parser.add_argument(\"--date\", type=str, default=None,\n",
    "                        help=\"Date to process (yyyymmdd, yyyy-mm-dd, yyyy/mm/dd\"\n",
    "                        \" or yyyy.mm.dd). Mode is required to be 'historic'; \"\n",
    "                        \"default=None\")\n",
    "    \n",
    "    parser.add_argument(\"--catlist\", type=str, default=None, \n",
    "                        help=\"Process all transient catalogs in the input list.\"\n",
    "                        \" List entries require full path. Mode  must be \"\n",
    "                        \"'historic'; default=None\")\n",
    "    \n",
    "    parser.add_argument(\"--logname\", type=str, default=None,\n",
    "                        help=\"Name of log file to save. Requires full path; \"\n",
    "                        \"default of None will not create a logfile\")\n",
    "    \n",
    "    parser.add_argument(\"--keep_tmp\", type=str, default=\"False\",\n",
    "                        help=\"Boolean to indicate if temporary directories / \"\n",
    "                        \"files need to be kept; default=False\")\n",
    "    \n",
    "    parser.add_argument(\"--newdatabases\", type=str, default=\"True\",\n",
    "                        help=\"Boolean to indicate if the asteroid and comet \"\n",
    "                        \"databases need to be redownloaded. If False and \"\n",
    "                        \"downloaded versions already exist, the newest \"\n",
    "                        \"downloaded versions are used; default=True\")\n",
    "    \n",
    "    parser.add_argument(\"--includecomets\", type=str, default=\"False\",\n",
    "                        help=\"Boolean to indicate if comets are to be \"\n",
    "                        \"included in the matching; default=False\")\n",
    "    \n",
    "    parser.add_argument(\"--overwrite\", type=str, default=\"True\",\n",
    "                        help=\"Boolean to indicate if files are allowed to be \"\n",
    "                        \"overwritten; default=True\")\n",
    "    \n",
    "    parser.add_argument(\"--timing\", type=str, default=\"False\",\n",
    "                        help=\"Boolean to indicate if functions need to be \"\n",
    "                        \"(wall-)timed; default=False\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    run_match2SSO(tel=args.telescope, mode=args.mode, cat2process=args.catalog, \n",
    "                  date2process=args.date, list2process=args.catlist, \n",
    "                  logName=args.logname, keep_tmp=args.keep_tmp, \n",
    "                  redownloadDatabases=args.newdatabases, \n",
    "                  includeComets=args.includecomets, overwriteFiles=args.overwrite,\n",
    "                  timeFunctions=args.timing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
