{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# match2SSO\n",
    "* Running instructions are given at the start of function run_match2SSO\n",
    "* Originally written as a Jupyter Notebook using Python 3.8.10\n",
    "* Compatible with BlackBOX / ZOGY version 1.0.0 and up\n",
    "* Compatible with MeerLICHT / BlackGEM observations\n",
    "\n",
    "Output (SSO) catalogue columns and header keywords are listed here:\n",
    "https://www.overleaf.com/read/zrhqwcbkfqns\n",
    "\n",
    "<i>match2SSO</i> makes grateful use of the <i>lunar</i> and \n",
    "<i>jpl_eph</i> repositories that were written by Bill Gray under \n",
    "Project Pluto. The core of <i>match2SSO</i> is <i>astcheck</i>: a C++\n",
    "script in the <i>lunar</i> repository that matches detections to known\n",
    "solar system objects. More information on <i>astcheck</i> can be found\n",
    "at: https://www.projectpluto.com/astcheck.htm\n",
    "\n",
    "<b>Dependencies on scripts and files</b>\n",
    "* <i>lunar</i> package (https://github.com/Bill-Gray/lunar)\n",
    "* <i>jpl_eph</i> package (https://github.com/Bill-Gray/jpl_eph)\n",
    "* JPL DE ephemeris file (ftp://ssd.jpl.nasa.gov/pub/eph/planets/Linux/)\n",
    "* MPC's Observatory codes list\n",
    "  (https://www.minorplanetcenter.net/iau/lists/ObsCodes.html)\n",
    "\n",
    "In addition, match2SSO uses MPCORB.DAT (MPC's asteroid database) and\n",
    "COMET.ELEMENTS (JPL's comet database), but these are downloaded when\n",
    "running the script and hence do not need to be pre-downloaded.\n",
    "\n",
    "if KEEP_TMP is False, all temporary files and folders are removed\n",
    "except for the most recent, unintegrated version of the asteroid &\n",
    "comet databases in the databaseFolder.\n",
    "For the night mode, we'll need to implement the cleaning function in\n",
    "BlackBOX that is commented out at the end of the night mode section in\n",
    "run_match2SSO. (As the night mode will be run in parallel on different\n",
    "catalogues and the individual night mode processes don't communicate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python packages and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__version__ = \"1.0.7\"\n",
    "__author__ = \"Danielle Pieterse\"\n",
    "KEYWORDS_VERSION = \"1.0.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import resource\n",
    "import platform\n",
    "import shutil\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from string import ascii_lowercase, ascii_uppercase\n",
    "import psutil\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Third party imports\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table, Column\n",
    "from astropy.time import Time\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "\n",
    "# Local imports\n",
    "import set_match2SSO as settingsFile # Load match2SSO settings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set log format and global logging constants\n",
    "LOG_FORMAT = ('%(asctime)s.%(msecs)03d [%(levelname)s, %(process)s] '\n",
    "              '%(message)s [%(funcName)s, line %(lineno)d]')\n",
    "DATE_FORMAT = '%Y-%m-%dT%H:%M:%S'\n",
    "logging.basicConfig(level='INFO', format=LOG_FORMAT, datefmt=DATE_FORMAT)\n",
    "LOG_FORMATTER = logging.Formatter(LOG_FORMAT, DATE_FORMAT)\n",
    "logging.Formatter.converter = time.gmtime # Convert time in logger to UTC\n",
    "LOG = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global constants\n",
    "\n",
    "# Relevant detection catalogue column names\n",
    "NUMBER_COLUMN = \"NUMBER\"  # Detection number, unique within the catalogue\n",
    "RA_COLUMN = \"RA_PSF_D\"   # RA in deg\n",
    "DEC_COLUMN = \"DEC_PSF_D\" # Dec in deg\n",
    "MAG_COLUMN = \"MAG_ZOGY\"\n",
    "SNR_COLUMN = \"SNR_ZOGY\" # Negative values correspond to negative transients\n",
    "\n",
    "# Relevant header keywords of detection catalogue\n",
    "DUMMY_KEYWORD = \"TDUMCAT\" # Boolean. If True, the catalogue is empty.\n",
    "DATE_KEYWORD = \"DATE-OBS\" # Observation date & time in isot format\n",
    "MPC_CODE_KEYWORD = \"MPC-CODE\" # MPC observatory code of telescope\n",
    "CENTRAL_RA_KEYWORD = \"RA-CNTR\" # Right ascension of the field center, in deg\n",
    "CENTRAL_DEC_KEYWORD = \"DEC-CNTR\" # Declination of the field center, in deg\n",
    "LIMMAG_KEYWORD = \"T-LMAG\" # Transient limiting magnitude\n",
    "\n",
    "# Default header for the MPC submission file\n",
    "DEFAULT_SUBMISSION_HEADER = \"\".join([\n",
    "    \"CON Radboud University, Houtlaan 4, 6525XZ, Nijmegen, The Netherlands\\n\",\n",
    "    \"CON [p.groot@astro.ru.nl]\\n\",\n",
    "    \"OBS P. J. Groot, S. L. D. Bloemen, L. Townsend\\n\",\n",
    "    \"MEA P. M. Vreeswijk, D. L. A. Pieterse, K. Paterson\\n\",\n",
    "    \"TEL 0.65-m reflector + CCD\\n\",\n",
    "    \"NET Gaia-DR2\\n\",\n",
    "    \"AC2 mpc-response@blackgem.org\\n\"\n",
    "    ])\n",
    "\n",
    "# Load switches\n",
    "REDOWNLOAD_DATABASES = bool(settingsFile.redownload_databases)\n",
    "INCLUDE_COMETS = bool(settingsFile.include_comets)\n",
    "KEEP_TMP = bool(settingsFile.keep_tmp)\n",
    "OVERWRITE_FILES = bool(settingsFile.overwrite_files)\n",
    "TIME_FUNCTIONS = bool(settingsFile.time_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions to run match2SSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_match2SSO(tel, mode, cat2process, date2process, list2process,\n",
    "                  logname):\n",
    "    \"\"\"\n",
    "    Run match2SSO on the input catalogue(s)/date. match2SSO can be run in\n",
    "    different mode / date2process / cat2process / list2process combinations.\n",
    "    Allowed combinations are: (if not mentioned, the variable is None)\n",
    "      * Day mode\n",
    "      * Day mode + date2process\n",
    "      * Night mode + cat2process\n",
    "      * Historic mode + cat2process\n",
    "      * Historic mode + date2process\n",
    "      * Historic mode + list2process\n",
    "    \n",
    "    Day mode:\n",
    "    Create known objects database & CHK files for the upcoming night, or - in\n",
    "    case date2process is specified - for the specified night.\n",
    "      0) Creates a run directory in preparation of the nightly processing\n",
    "      1) Downloads asteroid and comet databases to the database folder.\n",
    "      2) Integrates the asteroid database to midnight of the observation night\n",
    "      3) Combines the comet and integrated asteroid databases into a SOF-\n",
    "         formatted known objects database.\n",
    "      4) Creates symbolic links to the used databases and the observatory codes\n",
    "         list in the run directory.\n",
    "      5) Runs astcheck on a fake detection in order to create the CHK files\n",
    "         that astcheck will need for faster processing when running on\n",
    "         observations.\n",
    "      6) Removes the fake detection in- & output, excluding the CHK files!\n",
    "    Products of steps 1-2 are saved to the database folder, those of steps 3-5\n",
    "    to the run directory.\n",
    "    \n",
    "    Night mode:\n",
    "    Run match2SSO on a single transient catalogue. The day mode should have\n",
    "    been run once before the night mode. This allows the night mode to run in\n",
    "    parallel on multiple transient catalogues of the same night, as steps that\n",
    "    cannot be parallelised (making known objects database and CHK files) have\n",
    "    already been executed in the day mode. The night mode:\n",
    "      1) Converts the transient catalogue into an MPC-formatted text file.\n",
    "      2) Runs astcheck on that file, to find matches between the transient\n",
    "         detections and known solar system objects.\n",
    "      3) Makes an SSO catalogue containing the matches.\n",
    "      4) Makes an MPC submission file of the matches.\n",
    "    \n",
    "    Historic mode:\n",
    "    The historic mode does the entire processing - executing steps 0-3 & 5 of\n",
    "    the day mode, followed by all steps of the night mode. It can be run on a\n",
    "    single transient catalogue, an entire night of observations or a list of\n",
    "    observations (possibly spanning multiple nights). For the first catalogue\n",
    "    that is processed of each observation night, a new known objects catalogue\n",
    "    is created that is integrated to the observation midnight. The asteroid and\n",
    "    comet databases used for this are only downloaded once per historic mode\n",
    "    run (and only if REDOWNLOAD_DATABASES is True). For the remaining files (or\n",
    "    if REDOWNLOAD_DATABASES is False), the most recently downloaded versions\n",
    "    are used.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tel: string\n",
    "        Abbreviated telescope name. Can be either ML1, BG2, BG3 or BG4.\n",
    "    mode: string\n",
    "        Mode in which match2SSO is run. This can be 'day', 'night' or \n",
    "        'historic'.\n",
    "    cat2process: string\n",
    "        Path to and name of the transient catalogue that is to be processed.\n",
    "    date2process: string\n",
    "        Formatted as yyyymmdd, yyyy-mm-dd, yyyy/mm/dd or yyyy.mm.dd. When used\n",
    "        with day mode: date for which the known objects database needs to be\n",
    "        prepared. When used with historic mode: date for which all light\n",
    "        transient catalogues need to be processed.\n",
    "    list2process: string\n",
    "        Path to and name of the text file that contains the paths to and names\n",
    "        of transient catalogues (one per line) that need to be processed.\n",
    "    logname: string\n",
    "        Path to and name of the log file in which comments about the run are\n",
    "        stored.\n",
    "    \n",
    "    \"\"\"\n",
    "    t_glob = time.time()\n",
    "    \n",
    "    # Format input parameters\n",
    "    mode = mode.lower()\n",
    "    local_timezone = timezone(get_par(settingsFile.timeZoneTelescope, tel))\n",
    "    if date2process is not None:\n",
    "        date2process = date2process.replace(\".\", \"\").replace(\n",
    "            \"/\", \"\").replace(\"-\", \"\")\n",
    "    \n",
    "    # Perform checks on input parameter combinations and setting file\n",
    "    # parameters\n",
    "    if not check_input_parameters(mode, cat2process, date2process,\n",
    "                                  list2process):\n",
    "        return\n",
    "    if not check_settings():\n",
    "        return\n",
    "    folders = load_and_check_folders(tel)\n",
    "    if not folders: # Empty list\n",
    "        return\n",
    "    input_folder, software_folder, database_folder, log_folder, \\\n",
    "        submission_folder = folders\n",
    "    \n",
    "    # Logging\n",
    "    setup_logfile(logname, log_folder)\n",
    "    LOG.info(\"Mode: %s\", mode)\n",
    "    mem_use(label='at start of run_match2SSO')\n",
    "    \n",
    "    # Get local noon corresponding to the night start in case date2process or\n",
    "    # cat2process are specified. night_start is a datetime object (incl.\n",
    "    # timezone information).\n",
    "    if cat2process is not None:\n",
    "        night_start = get_night_start_from_date(cat2process, tel)\n",
    "    \n",
    "    elif date2process is not None:\n",
    "        night_start = local_timezone.localize(datetime.strptime(\" \".join([\n",
    "            date2process, \"120000\"]), \"%Y%m%d %H%M%S\"))\n",
    "    \n",
    "    \n",
    "    if mode == \"day\":\n",
    "        LOG.info(\"Running the day mode.\")\n",
    "        \n",
    "        # If no observation night is specified, use the upcoming local night\n",
    "        if date2process is None:\n",
    "            night_start = (datetime.now(local_timezone)).strftime(\n",
    "                \"%Y%m%d 120000\")\n",
    "            # Add timezone info to datetime object\n",
    "            night_start = local_timezone.localize(datetime.strptime(\n",
    "                night_start, \"%Y%m%d %H%M%S\"))\n",
    "        \n",
    "        # Create a run directory corresponding to the observation night\n",
    "        rundir = (\"{}{}/\".format(database_folder,\n",
    "                                 night_start.strftime(\"%Y%m%d\")))\n",
    "        LOG.info(\"Run directory: %s\", rundir)\n",
    "        if not os.path.isdir(rundir):\n",
    "            os.makedirs(rundir)\n",
    "        \n",
    "        # Create symbolic link to the observatory codes list\n",
    "        if not os.path.exists(\"{}ObsCodes.html\".format(rundir)):\n",
    "            LOG.info(\"Creating symbolic link to ObsCodes.html\")\n",
    "            os.symlink(\"{}ObsCodes.html\".format(software_folder),\n",
    "                       \"{}ObsCodes.html\".format(rundir))\n",
    "        \n",
    "        # Download and integrate known object databases\n",
    "        midnight = night_start + timedelta(days=0.5)\n",
    "        create_known_objects_database(midnight, rundir, database_folder,\n",
    "                                      REDOWNLOAD_DATABASES)\n",
    "        \n",
    "        # Create CHK files that astcheck needs in advance, to allow\n",
    "        # parallelisation in the night mode\n",
    "        create_chk_files(night_start, \"night_start\", rundir)\n",
    "        create_chk_files(night_start + timedelta(days=1), \"night_end\", rundir)\n",
    "        \n",
    "        # Check for known object database products\n",
    "        if not find_database_products(rundir):\n",
    "            log_timing_memory(t_glob, label='run_match2SSO')\n",
    "            logging.shutdown()\n",
    "            return\n",
    "        \n",
    "        LOG.info(\"Day mode finished.\")\n",
    "    \n",
    "    \n",
    "    elif mode == \"night\":\n",
    "        \n",
    "        LOG.info(\"Running the night mode on transient catalogue: \\n%s\",\n",
    "                 cat2process)\n",
    "        rundir = \"{}{}/\".format(database_folder,\n",
    "                                night_start.strftime(\"%Y%m%d\"))\n",
    "        LOG.info(\"Run directory: %s\", rundir)\n",
    "        \n",
    "        # Check for known object database products. Stop processing if it \n",
    "        # doesn't exist.\n",
    "        if not find_database_products(rundir):\n",
    "            logging.shutdown()\n",
    "            return\n",
    "        \n",
    "        # Check for CHK files\n",
    "        night_start_utc = get_night_start_from_date(cat2process, tel, \"utc\")\n",
    "        night_end_utc = night_start_utc + timedelta(days=1)\n",
    "        night_start_utc = night_start_utc.strftime(\"%Y%m%d\")\n",
    "        night_end_utc = night_end_utc.strftime(\"%Y%m%d\")\n",
    "        if not os.path.exists(\"{}{}.chk\".format(rundir, night_start_utc)):\n",
    "            LOG.critical(\"Missing %s.chk!\", night_start_utc)\n",
    "            logging.shutdown()\n",
    "            return\n",
    "        if not os.path.exists(\"{}{}.chk\".format(rundir, night_end_utc)):\n",
    "            LOG.critical(\"Missing %s.chk!\", night_end_utc)\n",
    "            logging.shutdown()\n",
    "            return\n",
    "        del night_start_utc\n",
    "        del night_end_utc\n",
    "        \n",
    "        # Check for observatory codes list. Stop processing if it doesn't exist\n",
    "        if not os.path.exists(\"{}ObsCodes.html\".format(rundir)):\n",
    "            LOG.critical(\"%sObsCodes.html doesn't exist.\", rundir)\n",
    "            logging.shutdown()\n",
    "            return\n",
    "        \n",
    "        _ = match_single_catalogue(cat2process, rundir, software_folder,\n",
    "                                   database_folder, submission_folder,\n",
    "                                   night_start, make_kod=False,\n",
    "                                   redownload_db=False)\n",
    "        \n",
    "        # Beware that the run directory created for the processing of the\n",
    "        # catalogue is not removed. This is the case because a single parallel\n",
    "        # process does not know about the rest. A cleaning function should be\n",
    "        # run at the end of the nightly processing if one wants to remove the\n",
    "        # run directory. Also delete the integrated asteroid database that was\n",
    "        # made during the day mode at this time. See code below.\n",
    "        # if not KEEP_TMP:\n",
    "        #     if os.path.exists(\"{}MPCORB.DAT\".format(rundir)):\n",
    "        #         asteroid_database = os.readlink(\"{}MPCORB.DAT\"\n",
    "        #                                             .format(rundir))\n",
    "        #         if \"epoch\" in asteroid_database:\n",
    "        #             os.remove(asteroid_database)\n",
    "        #             LOG.info(\"Removed %s\", asteroid_database)\n",
    "        #     remove_tmp_folder(rundir)\n",
    "    \n",
    "    \n",
    "    elif mode == \"historic\":\n",
    "        \n",
    "        if cat2process is not None:\n",
    "            LOG.info(\"Running historic mode on transient catalogue: \\n%s\",\n",
    "                     cat2process)\n",
    "            match_catalogues_single_night(\n",
    "                [cat2process], night_start, REDOWNLOAD_DATABASES,\n",
    "                software_folder, database_folder, submission_folder)\n",
    "        \n",
    "        elif date2process is not None:\n",
    "            LOG.info(\"Running historic mode on night %s\", date2process)\n",
    "            catalogues2process = get_transient_filenames(\n",
    "                input_folder, night_start, night_start+timedelta(days=1), tel)\n",
    "            if not catalogues2process:\n",
    "                LOG.critical(\"No light transient catalogues exist for night \"\n",
    "                             \"%s\", date2process)\n",
    "                log_timing_memory(t_glob, label='run_match2SSO')\n",
    "                logging.shutdown()\n",
    "                return\n",
    "            \n",
    "            match_catalogues_single_night(\n",
    "                catalogues2process, night_start, REDOWNLOAD_DATABASES,\n",
    "                software_folder, database_folder, submission_folder)\n",
    "        \n",
    "        elif list2process is not None:\n",
    "            LOG.info(\"Running historic mode on catalogue list: \\n%s\",\n",
    "                     list2process)\n",
    "            with open(list2process, \"r\") as catalogue_list:\n",
    "                listed_catalogues = [name.strip() for name in catalogue_list \\\n",
    "                                     if name[0] != '#']\n",
    "            \n",
    "            # Order by observation date (noon that equals the start of the\n",
    "            # observation day)\n",
    "            noons = []\n",
    "            catalogues2process = []\n",
    "            for cat_name in listed_catalogues:\n",
    "                noon = get_night_start_from_date(cat_name, tel)\n",
    "                noons.append(noon.strftime(\"%Y%m%d %H%M%S\"))\n",
    "                catalogues2process.append(cat_name)\n",
    "            \n",
    "            # Process files per night\n",
    "            LOG.info(\"Catalogue list spans %i nights\", len(np.unique(noons)))\n",
    "            first_night = True\n",
    "            for noon in np.unique(noons):\n",
    "                LOG.info(\"Processing night that starts at %s\", noon)\n",
    "                night_index = np.where(np.array(noons) == noon)[0]\n",
    "                catalogues2process_1night = np.array(\n",
    "                    catalogues2process)[night_index]\n",
    "                \n",
    "                # Use the same version of the asteroid and comet databases for\n",
    "                # all data to be processed. If REDOWNLOAD_DATABASES, this\n",
    "                # version corresponds to the time of processing the first image\n",
    "                # from the list. If REDOWNLOAD_DATABASES is False, the latest\n",
    "                # existing downloaded version of the databases is used.\n",
    "                noon = local_timezone.localize(datetime.strptime(\n",
    "                    noon, \"%Y%m%d %H%M%S\"))\n",
    "                if first_night:\n",
    "                    match_catalogues_single_night(\n",
    "                        catalogues2process_1night, noon, REDOWNLOAD_DATABASES,\n",
    "                        software_folder, database_folder, submission_folder)\n",
    "                else:\n",
    "                    match_catalogues_single_night(\n",
    "                        catalogues2process_1night, noon, False,\n",
    "                        software_folder, database_folder, submission_folder)\n",
    "                first_night = False\n",
    "    \n",
    "    LOG.info(\"Finished running match2SSO.\")\n",
    "    log_timing_memory(t_glob, label='run_match2SSO')\n",
    "    logging.shutdown()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_catalogues_single_night(catalogues_single_night, night_start,\n",
    "                                  redownload_db, software_folder,\n",
    "                                  database_folder, submission_folder):\n",
    "    \"\"\"\n",
    "    Wrapper function that calls the match_single_catalogue function for\n",
    "    each catalogue in a list that contains catalogues corresponding to\n",
    "    observations taken on the same night.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    catalogues_single_night: list of strings\n",
    "        Names of the catalogues that will need to be processed by match2SSO.\n",
    "        The catalogues are expected to correspond to observations taken on the\n",
    "        same night.\n",
    "    night_start: datetime object\n",
    "        Start of the observing night.\n",
    "    redownload_db: boolean\n",
    "        Boolean indicating whether the known object databases need to be\n",
    "        redownloaded, or alternatively if existing downloaded versions of the\n",
    "        databases can be used for the matching.\n",
    "    software_folder: string\n",
    "        Name of the folder in which the MPC observatory codes list\n",
    "        (Obscodes.html) is stored.\n",
    "    database_folder: string\n",
    "        Name of the folder containing the known objects databases, or where\n",
    "        these databases can be downloaded.\n",
    "    submission_folder: string\n",
    "        Name of the folder in which the MPC submission files will be stored.\n",
    "    \"\"\"\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    LOG.info(\"%i catalogues to process for the night around %s.\",\n",
    "             len(catalogues_single_night),\n",
    "             night_start.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    #Create run directory\n",
    "    rundir = \"{}{}/\".format(database_folder, night_start.strftime(\"%Y%m%d\"))\n",
    "    LOG.info(\"Run directory: %s\", rundir)\n",
    "    if not os.path.exists(rundir):\n",
    "        os.makedirs(rundir)\n",
    "    \n",
    "    #Run matching per catalogue\n",
    "    make_kod = True\n",
    "    for cat_name in catalogues_single_night:\n",
    "        made_kod = match_single_catalogue(\n",
    "            cat_name, rundir, software_folder, database_folder,\n",
    "            submission_folder, night_start, make_kod, redownload_db)\n",
    "        if made_kod:\n",
    "            make_kod = False #Only make known objects database once\n",
    "    \n",
    "    #Remove the run directory after processing the last catalogue of the night\n",
    "    if not KEEP_TMP:\n",
    "        #Remove integrated database made for this night\n",
    "        if os.path.exists(\"{}MPCORB.DAT\".format(rundir)):\n",
    "            asteroid_database = os.readlink(\"{}MPCORB.DAT\".format(rundir))\n",
    "            if \"epoch\" in asteroid_database:\n",
    "                os.remove(asteroid_database)\n",
    "                LOG.info(\"Removed %s\", asteroid_database)\n",
    "        \n",
    "        #Remove temporary folder made for this night\n",
    "        remove_tmp_folder(rundir)\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='match_catalogues_single_night')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_single_catalogue(cat_name, rundir, software_folder, database_folder,\n",
    "                           submission_folder, night_start, make_kod,\n",
    "                           redownload_db):\n",
    "    \"\"\"\n",
    "    Run matching routine on a single transient catalogue. Optionally, a new\n",
    "    known objects database is created where the reference epoch corresponds to\n",
    "    midnight on the observation night. The detections in the transient \n",
    "    catalogue are then matched to the positions of the solar system bodies in\n",
    "    the known objects catalogue. Matches are saved to an SSO catalogue.\n",
    "    Function returns a boolean indicating whether a known objects catalogue was\n",
    "    (successfully) made.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cat_name: string\n",
    "        Name of the transient catalogue of which the detections are to be\n",
    "        matched to known solar system objects.\n",
    "    rundir: string\n",
    "        Directory corresponding to observation night where all temporary\n",
    "        products will be stored during the running of match2SSO.\n",
    "    software_folder: string\n",
    "        Name of the folder in which the MPC observatory codes list\n",
    "        (Obscodes.html) is stored.\n",
    "    database_folder: string\n",
    "        Name of the folder containing the known objects databases, or where\n",
    "        these databases can be downloaded.\n",
    "    submission_folder: string\n",
    "        Name of the folder in which the MPC submission files will be stored.\n",
    "    night_start: datetime object, including time zone\n",
    "        Noon corresponding to the start of the local night during which the\n",
    "        observation corresponding to the transient catalogue was made.\n",
    "    make_kod: boolean\n",
    "        Boolean indicating whether a new known objects database needs to be\n",
    "        made. In night mode, this should be False. In historic mode, this is\n",
    "        only True for the first catalogue that is processed per observation\n",
    "        night, as the database will need to be integrated to that observation\n",
    "        night.\n",
    "    redownload_db: boolean\n",
    "        Only used when make_kod is True. This boolean indicates whether the\n",
    "        asteroid and comet databases will need to be redownloaded before making\n",
    "        the known objects database. Alternatively, the most recent, previously\n",
    "        downloaded version of the databases are used.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of match_single_catalogue')\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    LOG.info(\"Running match2SSO on %s\", cat_name)\n",
    "    \n",
    "    # Check if input catalogue exists and is not flagged red\n",
    "    is_existing, is_dummy, cat_name = check_input_catalogue(cat_name)\n",
    "    if not is_existing:\n",
    "        return made_kod\n",
    "    del is_existing\n",
    "    \n",
    "    # File names\n",
    "    mpcformat_file = \"{}{}\".format(rundir, os.path.basename(cat_name).replace(\n",
    "        \"_light.fits\", \".fits\").replace(\".fits\", \"_MPCformat.txt\"))\n",
    "    sso_cat = cat_name.replace(\"_light.fits\", \".fits\").replace(\".fits\",\n",
    "                                                               \"_sso.fits\")\n",
    "    predict_cat = sso_cat.replace(\"_trans\", \"\").replace(\"_sso.fits\",\n",
    "                                                        \"_sso_predict.fits\")\n",
    "    submission_file = \"{}{}.txt\".format(\n",
    "        submission_folder, os.path.basename(sso_cat).replace(\".fits\",\n",
    "                                                             \"_submit\"))\n",
    "    \n",
    "    # Keep track of whether known object database has been made\n",
    "    made_kod = False\n",
    "    \n",
    "    # Create predictions and SSO catalogues in case of a dummy (empty) detection\n",
    "    # catalogue\n",
    "    if is_dummy:\n",
    "        _ = predictions(None, rundir, software_folder, predict_cat, \"\")\n",
    "        create_sso_catalogue(None, rundir, software_folder, sso_cat, 0)\n",
    "        return made_kod\n",
    "    \n",
    "    # Check if output catalogues exist, in which case the known objects database\n",
    "    # will not need to be made\n",
    "    if os.path.exists(predict_cat) and (os.path.exists(sso_cat) and not\n",
    "                                            OVERWRITE_FILES):\n",
    "        LOG.info(\"Prediction and SSO catalogues exist and won't be remade.\\n\")\n",
    "        \n",
    "        # Check for any version of a submission file for this observation\n",
    "        submission_files = glob.glob(\n",
    "            \"{}*.txt\".format(submission_file.replace(\".txt\", \"\")))\n",
    "        if submission_files:\n",
    "            LOG.info(\"There is at least one version of an MPC submission file \"\n",
    "                     \"for this observation. No new one will be made.\\n\")\n",
    "            return made_kod\n",
    "        \n",
    "        # Check if MPC-formatted file that the submission creation function\n",
    "        # needs exists and get the MPC code. If it doesn't exist yet /\n",
    "        # anymore, remake this file first.\n",
    "        mpc_code = convert_fits2mpc(cat_name, mpcformat_file, software_folder)\n",
    "        if mpc_code is None:\n",
    "            LOG.critical(\"Stop running match2SSO on catalogue because of \"\n",
    "                         \"unknown MPC code.\")\n",
    "            return made_kod\n",
    "    \n",
    "        # Create a submission file that can be used to submit the detections\n",
    "        # that were matched to known solar system objects to the MPC\n",
    "        create_submission_file(sso_cat, mpcformat_file, submission_file,\n",
    "                               mpc_code)\n",
    "        \n",
    "        if not KEEP_TMP:\n",
    "            os.remove(mpcformat_file)\n",
    "            LOG.info(\"Removed %s\", mpcformat_file)\n",
    "        return made_kod\n",
    "    \n",
    "    # If make_kod, create a new known objects database with a reference epoch\n",
    "    # corresponding to midnight of the observation night. Also create a\n",
    "    # symbolic link to the MPC observatory codes list.\n",
    "    if make_kod:\n",
    "        midnight = night_start + timedelta(days=0.5)\n",
    "        create_known_objects_database(midnight, rundir, database_folder,\n",
    "                                      redownload_db)\n",
    "        \n",
    "        # Check if the run directory contains the proper known objects database\n",
    "        # files for further processing\n",
    "        if not find_database_products(rundir):\n",
    "            return made_kod\n",
    "        \n",
    "        made_kod = True\n",
    "        \n",
    "        # Make symbolic link to observatory codes list if it doesn't exist yet\n",
    "        if not os.path.exists(\"{}ObsCodes.html\".format(rundir)):\n",
    "            LOG.info(\"Creating symbolic link to ObsCodes.html\")\n",
    "            os.symlink(\"{}ObsCodes.html\".format(software_folder),\n",
    "                       \"{}ObsCodes.html\".format(rundir))\n",
    "    \n",
    "    # Convert the transient catalogue to an MPC-formatted text file\n",
    "    mpc_code = convert_fits2mpc(cat_name, mpcformat_file, software_folder)\n",
    "    if mpc_code is None:\n",
    "        LOG.critical(\"Stop running match2SSO on catalogue because of unknown \"\n",
    "                     \"MPC code.\")\n",
    "        return made_kod\n",
    "    \n",
    "    # Make predictions catalogue\n",
    "    N_sso = predictions(cat_name, rundir, software_folder, predict_cat, mpc_code)\n",
    "    \n",
    "    # Check if SSO catalogue and submission file already exist\n",
    "    submission_files = glob.glob(\n",
    "        \"{}*.txt\".format(submission_file.replace(\".txt\", \"\")))\n",
    "    if submission_files and os.path.exists(sso_cat) and not OVERWRITE_FILES:\n",
    "        LOG.info(\"SSO catalogue and submission file exist and won't be remade.\\n\")\n",
    "        if not KEEP_TMP:\n",
    "            os.remove(mpcformat_file)\n",
    "            LOG.info(\"Removed %s\", mpcformat_file)\n",
    "        return made_kod\n",
    "    \n",
    "    # Run astcheck on the MPC-formatted transient file\n",
    "    astcheck_file = mpcformat_file.replace(\"_MPCformat.txt\",\n",
    "                                           \"_astcheckMatches.txt\")\n",
    "    run_astcheck(mpcformat_file, rundir, astcheck_file)\n",
    "    \n",
    "    # Save matches found by astcheck to an SSO catalogue\n",
    "    create_sso_catalogue(astcheck_file, rundir, software_folder, sso_cat, N_sso)\n",
    "    \n",
    "    # Create a submission file that can be used to submit the detections that\n",
    "    # were matched to known solar system objects to the MPC\n",
    "    create_submission_file(sso_cat, mpcformat_file, submission_file, mpc_code)\n",
    "    \n",
    "    # Delete temporary files corresponding to the processed transient\n",
    "    # catalogue. The other temporary files (the CHK files, the SOF file and the\n",
    "    # symbolic links) in the run directory are not (yet) removed, as they\n",
    "    # might be needed for processing of other data from the same night.\n",
    "    if not KEEP_TMP:\n",
    "        os.remove(mpcformat_file)\n",
    "        LOG.info(\"Removed %s\", mpcformat_file)\n",
    "        os.remove(astcheck_file)\n",
    "        LOG.info(\"Removed %s\", astcheck_file)\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='match_single_catalogue')\n",
    "    \n",
    "    return made_kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core functions in match2SSO\n",
    "* Create known objects catalogue (with asteroids with a max. orbital\n",
    "  uncertainty)\n",
    "* Create catalogue with predictions of asteroids in the FOV\n",
    "* Convert transient catalogue to MPC format\n",
    "* Run matching algorithm (astcheck) on file made in previous step\n",
    "* Save solar system object detections (matches) to SSO catalogue\n",
    "* Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chk_files(noon, noon_type, rundir):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that creates the CHK files that astcheck uses (and produces if\n",
    "    they don't exist yet) when matching, by running astcheck on a fake\n",
    "    detection and subsequently removing the fake data products excluding the\n",
    "    CHK files. These files describe the positions of all asteroids at the start\n",
    "    and end of the night (both at noon) in UTC. This function is only needed\n",
    "    when running match2SSO in the day mode, as this will then allow\n",
    "    parallelisation in the night mode.\n",
    "    \n",
    "    As the 24-hour local observing night (between local noons) can overlap with\n",
    "    two UTC nights (between UTC noons) if there is a large time difference\n",
    "    between the timezone of the telescope and UTC, we will have to try\n",
    "    producing CHK files both for a time close to the start of the local night\n",
    "    (we take 1 min after) as well as for a time close to the end of it (1 min\n",
    "    before). This function should hence be run twice. This will produce a total\n",
    "    of 2 or 3 CHK files that astcheck will subsequently use to match any\n",
    "    observation taken during the local observing night to the known solar\n",
    "    system objects.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    noon: datetime object\n",
    "        Noon that corresponds to either the start or the end of the observing\n",
    "        night.\n",
    "    noon_type: string\n",
    "        Should be either \"night_start\" or \"night_end\", depending on which noon\n",
    "        was given as input.\n",
    "    rundir: string\n",
    "        Directory in which astcheck is run. This directory should contain\n",
    "        the mpc2sof catalogue that contains the known solar system objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check noon_type parameter and set observation time for fake\n",
    "    # detection\n",
    "    noon_type = noon_type.lower()\n",
    "    if noon_type not in [\"night_start\", \"night_end\"]:\n",
    "        LOG.error(\"Unknown noon type!\")\n",
    "        return\n",
    "    if noon_type == \"night_start\":\n",
    "        obstime = noon + timedelta(minutes=1)\n",
    "    elif noon_type == \"night_end\":\n",
    "        obstime = noon - timedelta(minutes=1)\n",
    "    \n",
    "    # Convert observation time of fake detection to UTC\n",
    "    obstime = obstime.astimezone(pytz.utc)\n",
    "    \n",
    "    # Create MPC-formatted file with fake detection\n",
    "    mpcformat_file_fake = \"{}fakedetection_{}_MPCformat.txt\".format(rundir,\n",
    "                                                                    noon_type)\n",
    "    LOG.info(\"Creating fake detection: %s\", mpcformat_file_fake)\n",
    "    mpcformat_file_fake_content = open(mpcformat_file_fake, \"w\")\n",
    "    fake_detection = \"\".join([\n",
    "        \"     0000001  C{} {:0>2} {:08.5f} \"\n",
    "        .format(obstime.year, obstime.month, obstime.day),\n",
    "        \"00 00 00.00 +00 00 00.0          0.00 G      L66\"\n",
    "        ])\n",
    "    mpcformat_file_fake_content.write(fake_detection)\n",
    "    mpcformat_file_fake_content.close()\n",
    "    \n",
    "    # Run astcheck on fake observation to create CHK files\n",
    "    LOG.info(\"Running astcheck on fake detection\")\n",
    "    astcheck_file_fake = mpcformat_file_fake.replace(\"_MPCformat.txt\",\n",
    "                                                     \"_astcheckMatches.txt\")\n",
    "    run_astcheck(mpcformat_file_fake, rundir, astcheck_file_fake,\n",
    "                 matching_radius=0)\n",
    "    \n",
    "    # Remove MPC-formatted file and astcheck output related to the fake\n",
    "    # detection\n",
    "    os.remove(mpcformat_file_fake)\n",
    "    LOG.info(\"Removed %s\", mpcformat_file_fake)\n",
    "    os.remove(astcheck_file_fake)\n",
    "    LOG.info(\"Removed %s\", astcheck_file_fake)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_database(sso_type, redownload_db, database_folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function downloads the asteroid or comet database if desired. After\n",
    "    downloading, asteroids with too large uncertainties will be removed from\n",
    "    the downloaded asteroid database copy.\n",
    "    Alternatively, the function will load the latest downloaded version of the\n",
    "    database. The function returns the database name and version number.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sso_type: string\n",
    "        Solar system object type that is in the database. Can be either\n",
    "        'asteroid' or 'comet'. Capitals are allowed as well.\n",
    "    redownload_db: boolean\n",
    "        If False, the databases will not be redownloaded. Instead, the name and\n",
    "        version number of the latest downloaded database version are returned.\n",
    "    database_folder: string\n",
    "        Folder to save the asteroid and comet databases to that are downloaded\n",
    "        in this function.\n",
    "    \"\"\"\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_subfunc = time.time()\n",
    "        \n",
    "    sso_type = sso_type.lower()\n",
    "    if sso_type == \"asteroid\":\n",
    "        database_url = settingsFile.URL_asteroidDatabase\n",
    "    elif sso_type == \"comet\":\n",
    "        database_url = settingsFile.URL_cometDatabase\n",
    "    else:\n",
    "        error_string = \"Database type unknown. Cannot be downloaded.\"\n",
    "        LOG.critical(error_string)\n",
    "        raise ValueError(error_string)\n",
    "    \n",
    "    #Determine whether database needs to be downloaded\n",
    "    existing_databases = glob.glob(\"{}{}DB_*.dat\".format(database_folder,\n",
    "                                                         sso_type))\n",
    "    existing_unintegrated_databases = [DB for DB in existing_databases \\\n",
    "                                       if \"epoch\" not in DB]\n",
    "    download = True\n",
    "    if not redownload_db and len(existing_databases) > 0:\n",
    "        download = False\n",
    "        \n",
    "    #Download database if desired and get database version\n",
    "    if download:\n",
    "        LOG.info(\"Downloading %s database...\", sso_type)\n",
    "        database_version = datetime.utcnow().strftime(\"%Y%m%dT%H%M\")\n",
    "        database_name = \"{}{}DB_version{}.dat\".format(\n",
    "            database_folder, sso_type, database_version)\n",
    "        req = requests.get(database_url, allow_redirects=True)\n",
    "        open(database_name, \"wb\").write(req.content)\n",
    "        LOG.info(\"%s database version: %s\", sso_type, database_version)\n",
    "        \n",
    "        #Remove asteroids with large orbital uncertainties from database\n",
    "        if sso_type == \"asteroid\":\n",
    "            select_asteroids_on_uncertainty(database_name)\n",
    "            \n",
    "        if not KEEP_TMP and len(existing_databases) > 0:\n",
    "            LOG.info(\"Removing older %s database versions.\", sso_type)\n",
    "            for old_database in existing_databases:\n",
    "                os.remove(old_database)\n",
    "                LOG.info('Removed %s.', old_database)\n",
    "    else:\n",
    "        #Retrieve most recent (unintegrated) database version. If there is no\n",
    "        #unintegrated database, retrieve the most recent integrated one. Empty\n",
    "        #databases (created when INCLUDE_COMETS = False) are not taken into\n",
    "        #account, as these are in a different folder.\n",
    "        databases_sorted = sorted(existing_unintegrated_databases)\n",
    "        if not databases_sorted:\n",
    "            databases_sorted = sorted(existing_databases)\n",
    "        database_name = databases_sorted[-1]\n",
    "        database_version = os.path.splitext(\n",
    "            os.path.basename(database_name))[0].split(\"_\")[1].replace(\n",
    "                \"version\", \"\")\n",
    "        LOG.info(\"%s database version %s\", sso_type, database_version)\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_subfunc, label='downloadDatabase ({})'\n",
    "                          .format(sso_type))\n",
    "    \n",
    "    return database_name, database_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_known_objects_database(midnight, rundir, database_folder,\n",
    "                                  redownload_db):\n",
    "    \"\"\"\n",
    "    Function downloads the most recent versions of the asteroid database and\n",
    "    the comet database. It then uses integrat.cpp from the lunar repository to\n",
    "    integrate the asteroid orbits to midnight of the observation night, in\n",
    "    order to optimize the predicted positions of known objects.\n",
    "    \n",
    "    Beware: the current version of integrat.cpp cannot be used on JPL's comet\n",
    "    file, as it is not compatible with its format. As a consequence, there\n",
    "    might be an offset in the predictions of the comet positions, perhaps\n",
    "    causing us to miss these objects in the linking routine. Note that the\n",
    "    MPC's comet file could be integrated to the right epoch using integrat.cpp,\n",
    "    but as this file is not compatible with astcheck, it cannot be used here.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    midnight: datetime object, including time zone\n",
    "        Local midnight during the observation night.\n",
    "    rundir: string\n",
    "        Directory in which mpc2sof is run and which the known objects catalogue\n",
    "        is saved to.\n",
    "    database_folder: string\n",
    "        Folder to save the known objects databases to that are created in this\n",
    "        function.\n",
    "    redownload_db: boolean\n",
    "        If False, the databases will not be redownloaded. They will only be\n",
    "        integrated to the observation epoch (midnight on the observation\n",
    "        night).\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of create_known_objects_database')\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    # Download asteroid database\n",
    "    asteroid_database, asteroid_database_version = download_database(\n",
    "        \"asteroid\", redownload_db, database_folder)\n",
    "    \n",
    "    # Download comet database if requested\n",
    "    if INCLUDE_COMETS:\n",
    "        _, comet_database_version = download_database(\"comet\", redownload_db,\n",
    "                                                      database_folder)\n",
    "    else:\n",
    "        LOG.info(\"Do not download comet database. Instead, create an empty \"\n",
    "                 \"comet database so that there's no matching to comets.\")\n",
    "        open(\"{}ELEMENTS.COMET\".format(rundir), \"w\").write(\"\".join([\n",
    "            \"Num  Name                                     Epoch      q      \",\n",
    "            \"     e        i         w        Node          Tp       Ref\\n---\",\n",
    "            \"---------------------------------------- ------- ----------- ---\",\n",
    "            \"------- --------- --------- --------- -------------- ------------\"\n",
    "            ]))\n",
    "    \n",
    "    # Integrat only accepts UTC midnights. Choose the one closest to local\n",
    "    # midnight.\n",
    "    date_midnight = midnight.date()\n",
    "    if midnight.hour >= 12.:\n",
    "        date_midnight = date_midnight + timedelta(days=1)\n",
    "    midnight_utc = pytz.utc.localize(datetime.strptime(\" \".join([\n",
    "        date_midnight.strftime(\"%Y%m%d\"), \"000000\"]), \"%Y%m%d %H%M%S\"))\n",
    "    midnight_utc_str = midnight_utc.strftime(\"%Y%m%dT%H%M\")\n",
    "    \n",
    "    # Integrate the asteroid database to the observation date\n",
    "    integrated_asteroid_database = (\n",
    "        \"{}asteroidDB_version{}_epoch{}.dat\"\n",
    "        .format(database_folder, asteroid_database_version, midnight_utc_str))\n",
    "    if not os.path.exists(integrated_asteroid_database):\n",
    "        LOG.info(\"Integrating asteroid database to epoch %s...\",\n",
    "                 midnight_utc_str)\n",
    "        if TIME_FUNCTIONS:\n",
    "            t_subtiming = time.time()\n",
    "        subprocess.run([\"integrat\", asteroid_database,\n",
    "                        integrated_asteroid_database,\n",
    "                        str(Time(midnight_utc).jd),\n",
    "                        \"-f{}\".format(settingsFile.JPL_ephemerisFile)],\n",
    "                       cwd=database_folder, check=True)\n",
    "        if TIME_FUNCTIONS:\n",
    "            log_timing_memory(t_subtiming, label='integrat')\n",
    "        \n",
    "        # Remove temporary file created by integrat\n",
    "        if os.path.exists(\"{}ickywax.ugh\".format(database_folder)):\n",
    "            os.remove(\"{}ickywax.ugh\".format(database_folder))\n",
    "    \n",
    "    # Create the symbolic links in the run directory that mpc2sof needs\n",
    "    symlink_asteroid_database = \"{}MPCORB.DAT\".format(rundir)\n",
    "    if os.path.exists(symlink_asteroid_database):\n",
    "        LOG.info(\"Removing the old MPCORB.DAT symbolic link\")\n",
    "        os.unlink(symlink_asteroid_database)\n",
    "    os.symlink(integrated_asteroid_database, symlink_asteroid_database)\n",
    "    LOG.info(\"Created symbolic link %s\", symlink_asteroid_database)\n",
    "    \n",
    "    if INCLUDE_COMETS:\n",
    "        symlink_comet_database = \"{}ELEMENTS.COMET\".format(rundir)\n",
    "        if os.path.exists(symlink_comet_database):\n",
    "            LOG.info(\"Removing the old ELEMENTS.COMET symbolic link\")\n",
    "            os.unlink(symlink_comet_database)\n",
    "        os.symlink(\"{}cometDB_version{}.dat\".format(\n",
    "            database_folder, comet_database_version), symlink_comet_database)\n",
    "        LOG.info(\"Created symbolic link %s\", symlink_comet_database)\n",
    "    mem_use(label='after creating symbolic links to the databases')\n",
    "    \n",
    "    # Combine the known comets and asteroids into a SOF file, which astcheck\n",
    "    # will then use as input\n",
    "    LOG.info(\"Combining asteroids and comets into SOF file.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_subtiming = time.time()\n",
    "    \n",
    "    subprocess.run(\"mpc2sof\", cwd=rundir, check=True)\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_subtiming, label='mpc2sof')\n",
    "        log_timing_memory(t_func, label='create_known_objects_database')\n",
    "    LOG.info(\"Finished loading and formatting external databases.\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_asteroids_on_uncertainty(asteroid_database):\n",
    "    \n",
    "    \"\"\"\n",
    "    Go through the asteroid database (MPCORB format) and select the asteroids\n",
    "    which have orbital uncertainty parameters smaller than maxUncertainty.\n",
    "    The MPC uncertainty parameters that we consider are explained here:\n",
    "    https://www.minorplanetcenter.net/iau/info/UValue.html\n",
    "    \n",
    "    Overwrite the database with just the asteroids selected on their orbital\n",
    "    uncertainties, so that there will be no matching with poorly known objects.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    asteroid_database: string\n",
    "        Name of the full asteroid database (MPCORB-formatted text-file).\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of select_asteroids_on_uncertainty')\n",
    "    \n",
    "    if settingsFile.maxUncertainty is None:\n",
    "        LOG.info(\"All known solar system bodies are used in the matching, \"\n",
    "                 \"irrespective of their uncertainty parameter.\")\n",
    "        return\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    LOG.info(\"Removing asteroids with too large uncertainties...\\n\")\n",
    "    \n",
    "    #Open asteroid database\n",
    "    asteroid_database_content = open(asteroid_database, \"r\").readlines()\n",
    "    \n",
    "    #Find the size of the header of the asteroid database, assuming that the\n",
    "    #header ends with a line of dashes.\n",
    "    header_end_index = 0\n",
    "    line_index = 0\n",
    "    for line in asteroid_database_content:\n",
    "        if line.startswith(\"-----\"):\n",
    "            header_end_index = line_index\n",
    "            break\n",
    "        line_index += 1\n",
    "    \n",
    "    #Re-write the asteroid database file, including only the header and the\n",
    "    #lines corresponding to asteroids that have small orbital uncertainties.\n",
    "    number_asteroids_pre_selection = 0\n",
    "    number_asteroids_post_selection = 0\n",
    "    with open(asteroid_database, \"w\") as asteroid_database_content_new:\n",
    "        for line_index in range(len(asteroid_database_content)-1):\n",
    "            \n",
    "            #Copy header to file\n",
    "            if line_index <= header_end_index:\n",
    "                asteroid_database_content_new.write(\n",
    "                    asteroid_database_content[line_index])\n",
    "                continue\n",
    "            \n",
    "            line = asteroid_database_content[line_index]\n",
    "            \n",
    "            #Copy empty lines\n",
    "            if line == \"\\n\":\n",
    "                asteroid_database_content_new.write(line)\n",
    "                continue\n",
    "            \n",
    "            number_asteroids_pre_selection += 1\n",
    "            \n",
    "            #Filter on uncertainty parameter. Copy lines of asteroids for\n",
    "            #which orbits are determined reasonably well.\n",
    "            uncertainty = line[105]\n",
    "            if uncertainty.isdigit():\n",
    "                if float(uncertainty) <= settingsFile.maxUncertainty:\n",
    "                    asteroid_database_content_new.write(line)\n",
    "                    number_asteroids_post_selection += 1\n",
    "    \n",
    "    LOG.info(\"%i out of %i asteroids have U <= %s\", \n",
    "             number_asteroids_post_selection, number_asteroids_pre_selection,\n",
    "             settingsFile.maxUncertainty)\n",
    "    LOG.info(\"Asteroid database now only includes sources with U <= %s\",\n",
    "             settingsFile.maxUncertainty)\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='select_asteroids_on_uncertainty')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(transient_cat, rundir, software_folder, predict_cat, mpc_code,\n",
    "                is_FOV_circle=False):\n",
    "    \"\"\"\n",
    "    Use astcheck to predict which asteroids are in the FOV during the\n",
    "    observation. Predictions can be made for a circular or square FOV. The\n",
    "    function returns the number of asteroids that are estimated to be bright\n",
    "    enough to be detected (V mag <= limiting AB magnitude). This is a crude\n",
    "    estimate, as no correction for the different type of magnitudes is taken\n",
    "    into account.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    transient_cat: string or None\n",
    "        Path to and name of the transient catalogue. If None, we will make a\n",
    "        dummy (empty) predictions catalogue.\n",
    "    rundir: string\n",
    "        Directory corresponding to observation night where the temporary output\n",
    "        file that astcheck makes will be stored.\n",
    "    software_folder: string\n",
    "        Folder in which the lunar and jpl_eph packages that are called by\n",
    "        match2SSO are stored.\n",
    "    predict_cat: string\n",
    "        Path to and name of the output catalogue to which the predictions will\n",
    "        be saved.\n",
    "    mpc_code: string\n",
    "        MPC code corresponding to the telescope.\n",
    "    is_FOV_circle: boolean\n",
    "        Boolean indicating if the FOV is circular. If False, a square FOV is\n",
    "        assumed where the sides are aligned with RA & Dec.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of predictions')\n",
    "    \n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    if not OVERWRITE_FILES and os.path.exists(predict_cat):\n",
    "        LOG.info(\"Prediction catalogue already exists and won't be re-made.\")\n",
    "        with fits.open(predict_cat) as hdu:\n",
    "            hdr = hdu[1].header\n",
    "        return hdr[\"N-SSO\"]\n",
    "    \n",
    "    # If the transient catalogue was red-flagged, we will not make a predictions\n",
    "    # catalogue as there is little use for it.\n",
    "    if transient_cat is None:\n",
    "        LOG.info(\"Creating a dummy predictions catalogue.\")\n",
    "        sso_header = create_sso_header(rundir, software_folder, 0, 0, True,\n",
    "                                       False)\n",
    "        write2fits(Table(), None, [], predict_cat, start_header=sso_header)\n",
    "        if TIME_FUNCTIONS:\n",
    "            log_timing_memory(t_func, label='predictions')\n",
    "        return 0\n",
    "    \n",
    "    LOG.info(\"Making predictions...\")\n",
    "    \n",
    "    # Open header of transient catalogue\n",
    "    with fits.open(transient_cat) as hdu:\n",
    "        hdr = hdu[1].header\n",
    "    \n",
    "    # Get the observation date (isot format) and central field coordinates (in\n",
    "    # deg) from the header\n",
    "    date = hdr[DATE_KEYWORD]\n",
    "    if CENTRAL_RA_KEYWORD in hdr.keys() and CENTRAL_DEC_KEYWORD in hdr.keys():\n",
    "        ra_field = hdr[CENTRAL_RA_KEYWORD]\n",
    "        dec_field = hdr[CENTRAL_DEC_KEYWORD]\n",
    "    else:\n",
    "        ra_field = hdr[\"RA\"]\n",
    "        dec_field = hdr[\"DEC\"]\n",
    "    limmag = hdr[LIMMAG_KEYWORD]\n",
    "    \n",
    "    # Create temporary output file for astcheck results\n",
    "    output_file = \"{}{}\".format(rundir, os.path.basename(transient_cat).replace(\n",
    "        \"_light\", \"\").replace(\"_trans.fits\", \"_sso_predictions.txt\"))\n",
    "    output_file_content = open(output_file, 'w')\n",
    "    \n",
    "    if is_FOV_circle:\n",
    "        field_radius = 3600.*settingsFile.FOV_width/2.\n",
    "    else:\n",
    "        #Square FOV. Use the half diagonal of the FOV as the radius.\n",
    "        field_radius = 3600.*np.sqrt(2)*settingsFile.FOV_width/2.\n",
    "    \n",
    "    # Run astcheck from folder containing .sof-file\n",
    "    subprocess.call([\"astcheck\", \"-c\", str(date), str(ra_field), str(dec_field),\n",
    "                     str(mpc_code), \"-r{}\".format(field_radius), \"-h\",\n",
    "                     \"-m{}\".format(settingsFile.limitingMagnitude),\n",
    "                     \"-M{}\".format(settingsFile.maximalNumberOfAsteroids)],\n",
    "                    stdout=output_file_content, cwd=rundir)\n",
    "    output_file_content.close()\n",
    "    \n",
    "    # Read in astcheck output\n",
    "    astcheck_file_content = open(output_file, \"r\").readlines()\n",
    "    astcheck_file_content = remove_astcheck_header_and_footer(\n",
    "        astcheck_file_content)\n",
    "    \n",
    "    # Create table to store solar system bodies and their properties\n",
    "    output_columns = {\n",
    "        \"ID_SSO\":       [\"12a\", \"\", \"\"],\n",
    "        \"RA_SSO\":       [\"f4\", \"deg\", \"%.6f\"],\n",
    "        \"DEC_SSO\":      [\"f4\", \"deg\", \"%.6f\"],\n",
    "        \"V_RA_SSO\":     [\"f4\", \"arcsec/hour\", \"%.4f\"],\n",
    "        \"V_DEC_SSO\":    [\"f4\", \"arcsec/hour\", \"%.4f\"],\n",
    "        \"MAG_V_SSO\":    [\"f4\", \"\", \"%.2f\"]\n",
    "        }\n",
    "    output_table = Table()\n",
    "    for key in output_columns.keys():\n",
    "        output_table.add_column(Column(name=key, dtype=output_columns[key][0],\n",
    "                                       unit=output_columns[key][1]))\n",
    "        if output_columns[key][2]:\n",
    "            output_table[key].format = output_columns[key][2]\n",
    "        \n",
    "    # Loop through SSOs and store their properties in the output table\n",
    "    for source in astcheck_file_content:\n",
    "        source = re.sub('\\n$', '', source) # Remove line end character\n",
    "        source_properties = re.split(' +', source)\n",
    "        if source_properties[0]:\n",
    "            identifier = \" \".join([source_properties[0], source_properties[1]])\n",
    "        else:\n",
    "            identifier = source_properties[1]\n",
    "        ra_source, dec_source, magnitude, v_ra, v_dec = source_properties[2:]\n",
    "        \n",
    "        output_row = [identifier, float(ra_source), float(dec_source),\n",
    "                      float(v_ra), float(v_dec), float(magnitude)]\n",
    "        output_table.add_row(output_row)\n",
    "    \n",
    "    # Remove temporary astcheck output file\n",
    "    if not KEEP_TMP:\n",
    "        os.remove(output_file)\n",
    "        LOG.info(\"Removed %s\", output_file)\n",
    "    \n",
    "    # In case of a square FOV, disregard SSOs outside the FOV\n",
    "    if not is_FOV_circle and len(output_table)>0:\n",
    "        center = SkyCoord(ra_field, dec_field, unit=\"deg\", frame=\"icrs\")\n",
    "        sources = SkyCoord(output_table[\"RA_SSO\"],\n",
    "                           output_table[\"DEC_SSO\"], unit=\"deg\", frame=\"icrs\")\n",
    "        dra, ddec = center.spherical_offsets_to(sources)\n",
    "        mask_dist = ((abs(dra.deg) <= settingsFile.FOV_width/2.) &\n",
    "                     (abs(ddec.deg) <= settingsFile.FOV_width/2.))\n",
    "        output_table = output_table[mask_dist]\n",
    "    \n",
    "    # Create header for predicted asteroids in FOV\n",
    "    i_bright = np.where(output_table[\"MAG_V_SSO\"] <= limmag)[0]\n",
    "    N_sso = len(i_bright)\n",
    "    if N_sso > 0:\n",
    "        dummy = False\n",
    "    else:\n",
    "        dummy = True\n",
    "    sso_header = create_sso_header(rundir, software_folder, 0, N_sso, dummy,\n",
    "                                   False)\n",
    "    \n",
    "    # Save to table\n",
    "    write2fits(output_table, None, [], predict_cat, start_header=sso_header)\n",
    "    \n",
    "    LOG.info(\"Predictions saved to %s.\", predict_cat)\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='predictions')\n",
    "    \n",
    "    return N_sso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sso_header(rundir, software_folder, N_det, N_sso, dummy,\n",
    "                      incl_detections):\n",
    "    \"\"\"\n",
    "    Function creates the headers for the SSO catalogue and the SSO predictions\n",
    "    catalogue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rundir: string\n",
    "        Name of the folder in which the symbolic links to the databases are\n",
    "        stored. These are used to get the version numbers of the databases.\n",
    "    software_folder: string\n",
    "        Folder in which the lunar and jpl_eph repositories are located.\n",
    "    N_det: int\n",
    "        Number of detected solar system objects. Only one matched object is\n",
    "        counted per transient detection and if an object is matched to multiple\n",
    "        transients, it is also only counted once.\n",
    "    N_sso: int\n",
    "        Number of solar system objects in the FOV that are supposedly bright\n",
    "        enough for a detection (V magnitude < T-LMAG). The difference between V\n",
    "        and AB magnitudes is ignored here. This number is therefore a rough\n",
    "        prediction for the number of detections.\n",
    "    dummy: boolean\n",
    "        Boolean indicating whether the catalogue is a dummy catalogue without\n",
    "        sources (dummy=True). If False, there are sources in the catalogue.\n",
    "    incl_detections: boolean\n",
    "        Boolean indicating whether the catalogue is the SSO catalogue -\n",
    "        corresponding to detected solar system objects - or not. If not, it is\n",
    "        the catalogue containing the predicted objects in the FOV and some of\n",
    "        the header keywords will not be included in the header.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of create_sso_header')\n",
    "    LOG.info(\"Creating SSO header.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    # Create empty SSO header\n",
    "    header = fits.Header()\n",
    "    \n",
    "    # Add Python version to SSO header\n",
    "    header['PYTHON-V'] = (platform.python_version(), \"Python version used\")\n",
    "    \n",
    "    # Get C++ version and add to the SSO header. Based on \n",
    "    # [https://stackoverflow.com/questions/44734397/which-c-standard-is-the-\n",
    "    # default-when-compiling-with-g/44735016#44735016]\n",
    "    proc = subprocess.run(\"g++ -dM -E -x c++  /dev/null | grep -F __cplusplus\",\n",
    "                          capture_output=True, shell=True, check=True)\n",
    "    cpp_macro = proc.stdout.decode(\"utf-8\").replace(\"\\n\", \"\").split()[-1]\n",
    "    \n",
    "    if cpp_macro not in settingsFile.CPPmacro2version.keys():\n",
    "        LOG.error(\"C++ macro unknown: %s\", cpp_macro)\n",
    "        cpp_version = \"None\"\n",
    "    else:\n",
    "        cpp_version = settingsFile.CPPmacro2version[cpp_macro]\n",
    "    header['CPP-V'] = (cpp_version, \"C++ version used\")\n",
    "    \n",
    "    # Get G++ version and add to SSO header\n",
    "    proc = subprocess.run(\"g++ --version\", capture_output=True, shell=True,\n",
    "                          check=True)\n",
    "    gpp_version = proc.stdout.decode(\"utf-8\").split(\"\\n\")[0].split()[-1]\n",
    "    header['GPP-V'] = (gpp_version, \"G++ version used\")\n",
    "    \n",
    "    # Add match2SSO & header keyword versions to the SSO header\n",
    "    header['SSO-V'] = (__version__, \"match2SSO version used\")\n",
    "    header['SSOKW-V'] = (KEYWORDS_VERSION,\n",
    "                         \"SSO header keywords version used\")\n",
    "    \n",
    "    # Get unique strings with git, signifying the latest commit that was made\n",
    "    # to the lunar & jpl_eph repositories and hence signifynig the versions of\n",
    "    # these repositories. Save the strings to the SSO header.\n",
    "    proc = subprocess.run(\"git rev-parse --short=4 HEAD\", capture_output=True,\n",
    "                          shell=True, cwd=\"{}lunar/\".format(software_folder),\n",
    "                          check=True)\n",
    "    lunar_version = proc.stdout.decode(\"utf-8\").replace(\"\\n\", \"\")\n",
    "    header['LUNAR-V'] = (lunar_version, \"lunar repository version used\")\n",
    "    \n",
    "    proc = subprocess.run(\"git rev-parse --short=4 HEAD\", capture_output=True,\n",
    "                          shell=True, cwd=\"{}jpl_eph/\".format(software_folder),\n",
    "                          check=True)\n",
    "    jpl_eph_version = proc.stdout.decode(\"utf-8\").replace(\"\\n\", \"\")\n",
    "    header['JPLEPH-V'] = (jpl_eph_version, \"jpl_eph repository version used\")\n",
    "    \n",
    "    # Add version of JPL lunar & planetary ephemerides file to SSO header\n",
    "    header['JPLDE-V'] = (\"DE{}\"\n",
    "                         .format(settingsFile.JPL_ephemerisFile.split(\n",
    "                             \".\")[-1]), \"JPL ephemeris file version used\")\n",
    "    \n",
    "    # Add asteroid database version & reference epoch to the SSO header.\n",
    "    # The MPCORB.DAT symbolic link in the run directory refers to the\n",
    "    # asteroid database version that was used. The name structure of this\n",
    "    # database is: \n",
    "    # asteroid_database_version[yyyymmddThhmm]_epoch[yyyymmddThhmm].dat\n",
    "    asteroid_database_version = \"None\"\n",
    "    asteroid_database_epoch = \"None\"\n",
    "    if os.path.exists(\"{}MPCORB.DAT\".format(rundir)):\n",
    "        asteroid_database = os.readlink(\"{}MPCORB.DAT\".format(rundir))\n",
    "        asteroid_database_date = os.path.basename(\n",
    "            asteroid_database).split(\"_\")[1].replace(\"version\", \"\")\n",
    "        asteroid_database_version = \"{}-{}-{}T{}:{}\".format(\n",
    "            asteroid_database_date[0:4], asteroid_database_date[4:6],\n",
    "            asteroid_database_date[6:8], asteroid_database_date[9:11],\n",
    "            asteroid_database_date[11:13])\n",
    "        reference_epoch = os.path.basename(asteroid_database).split(\n",
    "            \"_\")[2].replace(\"epoch\", \"\")\n",
    "        asteroid_database_epoch = (\"{}-{}-{}T{}:{}\".format(\n",
    "            reference_epoch[0:4], reference_epoch[4:6], reference_epoch[6:8],\n",
    "            reference_epoch[9:11], reference_epoch[11:13]))\n",
    "    \n",
    "    header['ASTDB-V'] = (asteroid_database_version,\n",
    "                         \"asteroid database version (date in UTC)\")\n",
    "    header['ASTDB-EP'] = (asteroid_database_epoch,\n",
    "                          \"asteroid database epoch in UTC\")\n",
    "    \n",
    "    # Add comet database version to the SSO header.\n",
    "    # The ELEMENTS.COMET symbolic link in the run directory refers to the\n",
    "    # comet database version that was used. The name structure of this\n",
    "    # database is: cometDB_version[yyyymmddThhmm].dat\n",
    "    comet_database_version = \"None\"\n",
    "    if INCLUDE_COMETS:\n",
    "        comet_database = os.readlink(\"{}ELEMENTS.COMET\".format(rundir))\n",
    "        comet_database_date = os.path.basename(comet_database).split(\n",
    "            \"_\")[1].replace(\"version\", \"\")\n",
    "        comet_database_version = \"{}-{}-{}T{}:{}\".format(\n",
    "            comet_database_date[0:4], comet_database_date[4:6],\n",
    "            comet_database_date[6:8], comet_database_date[9:11],\n",
    "            comet_database_date[11:13])\n",
    "    \n",
    "    header['COMDB-V'] = (comet_database_version,\n",
    "                         \"comet database version (date in UTC)\")\n",
    "    \n",
    "    # Add matching radius and maximum orbital uncertainty parameter to header\n",
    "    if incl_detections:\n",
    "        header['RADIUS'] = (float(settingsFile.matchingRadius),\n",
    "                            \"matching radius in arcsec\")\n",
    "    header['U-MAX'] = (settingsFile.maxUncertainty,\n",
    "                       \"maximum orbital uncertainty parameter\")\n",
    "    \n",
    "    # Add number of (predicted) detections to header\n",
    "    header['N-SSO'] = (N_sso, \"predicted number of bright solar system objects \"\n",
    "                       + \"in the FOV\")\n",
    "    if incl_detections:\n",
    "        header['N-SSODET'] = (N_det, \"number of detected solar system objects\")\n",
    "    \n",
    "    # Add keyword indicating whether there is\n",
    "    header['SDUMCAT'] = (bool(dummy), \"dummy SSO catalogue without sources?\")\n",
    "    \n",
    "    LOG.info(\"SSO header complete.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='create_sso_header')\n",
    "    \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fits2mpc(transient_cat, mpcformat_file, software_folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function converts the transient catalogue to a text file of the MPC\n",
    "    80-column format, so that astcheck can run on it. For the asteroid / comet\n",
    "    identifier used in the MPC file, the transient number is used. This\n",
    "    transient number cannot be used for MPC submissions as it is not all-time\n",
    "    unique (per telescope). But it is a straight-forward way to link detections\n",
    "    to known solar system objects within match2SSO.\n",
    "    \n",
    "    Negative transients are excluded from the MPC-formatted file. In the case of\n",
    "    moving objects, these are sources that are present in the reference image\n",
    "    but not in the new image. They can be recognised as having a negative\n",
    "    signal-to-noise ratio value in the significance (Scorr) image for MeerLICHT\n",
    "    and BlackGEM. As reference images can be stacked images (that are not\n",
    "    centered on the asteroid) for which the observation date and time is\n",
    "    unclear, and as we use the date of the new image as the observation date,\n",
    "    asteroids in the reference images are not useful.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    transient_cat: string\n",
    "        Path to and name of the transient catalogue.\n",
    "    mpcformat_file: string\n",
    "        Path to and name of the MPC-formatted text file that is made in this\n",
    "        function.\n",
    "    software_folder: string\n",
    "        Folder in which the MPC observatory codes list (Obscodes.html) is\n",
    "        stored.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of convert_fits2mpc')\n",
    "    LOG.info(\"Converting transient catalogue to MPC-format.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    # Load transient catalogue header\n",
    "    with fits.open(transient_cat) as hdu:\n",
    "        transient_header = hdu[1].header\n",
    "    \n",
    "    # Get the MPC observatory code from the header\n",
    "    mpc_code = transient_header[MPC_CODE_KEYWORD].strip()\n",
    "    if mpc_code not in list(pd.read_fwf(\"{}ObsCodes.html\"\n",
    "                                        .format(software_folder),\n",
    "                                        widths=[4, 2000], \n",
    "                                        skiprows=1)['Code'])[:-1]:\n",
    "        LOG.critical(\"MPC code %s is not in the MPC list of observatory codes\",\n",
    "                     mpc_code)\n",
    "        return None\n",
    "    \n",
    "    # Check if MPC-formatted file exists and if it should be overwritten or not\n",
    "    if not OVERWRITE_FILES and os.path.exists(mpcformat_file):\n",
    "        LOG.info(\"MPC-formatted file already exists and will not re-made.\")\n",
    "        return mpc_code\n",
    "    \n",
    "    # Get observation date in the right format\n",
    "    date_obs = Time(transient_header[DATE_KEYWORD], format='isot').datetime\n",
    "    decimal_day = date_obs.day + 1./24.*(\n",
    "        date_obs.hour + 1./60.*(date_obs.minute + 1./60.*(\n",
    "            date_obs.second + date_obs.microsecond/10.**6)))\n",
    "    mpc_char16to32 = \"{} {:0>2} {:08.5f} \".format(date_obs.year, \n",
    "                                                  date_obs.month, decimal_day)\n",
    "    # Load transient catalogue data\n",
    "    with fits.open(transient_cat) as hdu:\n",
    "        detections = Table(hdu[1].data)\n",
    "    \n",
    "    # Remove negative transients\n",
    "    index_positives = np.where(detections[SNR_COLUMN]>=0)[0]\n",
    "    detections = detections[index_positives]\n",
    "    \n",
    "    # Create output file\n",
    "    mpcformat_file_content = open(mpcformat_file, \"w\")\n",
    "    \n",
    "    # Loop over the detections and add data to the MPC-formatted file\n",
    "    for detection_index in range(len(detections)):\n",
    "        # Use the source numbers as \"temporary designations\" in the MPC format.\n",
    "        # In this way, we will be able to link the known objects to the right\n",
    "        # source.\n",
    "        line = (\"     {:0>7}  C\"\n",
    "                .format(detections[NUMBER_COLUMN][detection_index]))\n",
    "        line = \"\".join([line, mpc_char16to32])\n",
    "        \n",
    "        # Get the coordinates and magnitude of the source\n",
    "        coord = SkyCoord(detections[RA_COLUMN][detection_index],\n",
    "                         detections[DEC_COLUMN][detection_index],\n",
    "                         unit='deg', frame='icrs') \n",
    "        mag = \"{:.1f}\".format(detections[MAG_COLUMN][detection_index])\n",
    "        \n",
    "        line = \"\".join([line, \"{} {}          {} G      {}\"\n",
    "                        .format(coord.to_string('hmsdms', sep=' ',\n",
    "                                                precision=2)[:11],\n",
    "                                coord.to_string('hmsdms', sep=' ',\n",
    "                                                precision=1)[-11:],\n",
    "                                mag.rjust(4), mpc_code)])\n",
    "        \n",
    "        # Write the data to the MPC-formatted file\n",
    "        mpcformat_file_content.write(\"{}\\n\".format(line))\n",
    "    \n",
    "    mpcformat_file_content.close()\n",
    "    \n",
    "    LOG.info(\"MPC-formatted file saved to %s.\", mpcformat_file)\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='convert_fits2mpc')\n",
    "    \n",
    "    return mpc_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_astcheck(mpcformat_file, rundir, output_file,\n",
    "                 matching_radius=settingsFile.matchingRadius):\n",
    "    \"\"\"\n",
    "    Run astcheck on the input transient catalogue to find matches between\n",
    "    transient detections and known solar system objects. Per detection, all\n",
    "    matches within the matching_radius are selected and saved to the output text\n",
    "    file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mpcformat_file: string\n",
    "        Name of the input text-file that is formatted according to the MPC's\n",
    "        80-column MPC format. This file can list detections / tracklets for\n",
    "        astcheck to match, but it can also contain just a single row\n",
    "        representing the observation. In the latter use case, the specified\n",
    "        coordinates should correspond to the centre of the observation.\n",
    "    rundir: string\n",
    "        Directory in which astcheck is run. This directory should contain\n",
    "        the mpc2sof catalogue that contains the known solar system objects.\n",
    "    output_file: string\n",
    "        Path to and name of the output text file in which the matches found\n",
    "        by astcheck are stored.\n",
    "    matching_radius: int or float\n",
    "        Matching radius in arcsec. The default value is the one specified in\n",
    "        the settings file [set_match2SSO.py].\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of run_astcheck')\n",
    "    LOG.info(\"Running astcheck: matching detections to known solar system \"\n",
    "             \"bodies.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    if not OVERWRITE_FILES and os.path.exists(output_file):\n",
    "        LOG.info(\"Astcheck output file already exists and won't be re-made.\")\n",
    "        return\n",
    "    \n",
    "    # Create a file for storing the output of the astcheck run\n",
    "    output_file_content = open(output_file, 'w')\n",
    "    \n",
    "    # Run astcheck from folder containing .sof-file\n",
    "    subprocess.call([\"astcheck\", mpcformat_file, \"-h\",\n",
    "                     \"-r{}\".format(matching_radius),\n",
    "                     \"-m{}\".format(settingsFile.limitingMagnitude),\n",
    "                     \"-M{}\".format(settingsFile.maximalNumberOfAsteroids)],\n",
    "                    stdout=output_file_content, cwd=rundir)\n",
    "    output_file_content.close()\n",
    "    \n",
    "    LOG.info(\"Matches saved to %s.\", output_file)\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='run_astcheck')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sso_catalogue(astcheck_file, rundir, software_folder, sso_cat,\n",
    "                         N_sso):\n",
    "    \"\"\"\n",
    "    Open the text-file that was produced when running astcheck [astcheck_file]\n",
    "    and save the information to an SSO catalogue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    astcheck_file: string\n",
    "        Name of the file containing astcheck's output (the matches). This can\n",
    "        be None, in which case we will create a dummy SSO catalogue without\n",
    "        matches.\n",
    "    rundir: string\n",
    "        Directory in which astcheck was run. This directory also contains\n",
    "        symbolic links to the asteroid and comet databases that were used to\n",
    "        create the known objects catalogue that astcheck used.\n",
    "    software_folder: string\n",
    "        Folder in which the lunar and jpl_eph packages that are called by\n",
    "        match2SSO are stored.\n",
    "    sso_cat: string\n",
    "        Name of the SSO catalogue to be created in which the matches\n",
    "        are stored.\n",
    "    N_sso: int\n",
    "        Number of solar system objects in the FOV that are supposedly bright\n",
    "        enough for a detection (V magnitude < T-LMAG). The difference between V\n",
    "        and AB magnitudes is ignored here. This number is therefore a rough\n",
    "        prediction for the number of detections.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of create_sso_catalogue')\n",
    "    LOG.info(\"Converting astcheck output into an SSO catalogue.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    if not OVERWRITE_FILES and os.path.exists(sso_cat):\n",
    "        LOG.info(\"SSO catalogue already exists and will not be re-made.\")\n",
    "        return\n",
    "    \n",
    "    # If the transient catalogue was red-flagged, matching was not performed\n",
    "    # and an empty SSO catalogue needs to be created.\n",
    "    if astcheck_file is None:\n",
    "        LOG.info(\"Creating a dummy SSO catalogue.\")\n",
    "        sso_header = create_sso_header(rundir, software_folder, 0, N_sso, True,\n",
    "                                       True)\n",
    "        write2fits(Table(), None, [], sso_cat, start_header=sso_header)\n",
    "        if TIME_FUNCTIONS:\n",
    "            log_timing_memory(t_func, label='create_sso_catalogue')\n",
    "        return\n",
    "    \n",
    "    # Remove astcheck header and footer if needed\n",
    "    astcheck_file_content = open(astcheck_file, \"r\").readlines()\n",
    "    astcheck_file_content = remove_astcheck_header_and_footer(\n",
    "        astcheck_file_content)\n",
    "    \n",
    "    # Find empty lines\n",
    "    separator = \"\\n\"\n",
    "    indices_separator = np.where(np.array(astcheck_file_content)\n",
    "                                 == separator)[0]\n",
    "    indices_separator = np.append(-1, indices_separator)\n",
    "    indices_separator = np.append(indices_separator,\n",
    "                                  len(astcheck_file_content))\n",
    "    \n",
    "    # Create table to store match information in\n",
    "    output_columns = {\n",
    "        NUMBER_COLUMN:  [\"i4\", \"\"],\n",
    "        \"ID_SSO\":       [\"12a\", \"\"],\n",
    "        \"DIST_RA_SSO\":  [\"i2\", \"arcsec\"],\n",
    "        \"DIST_DEC_SSO\": [\"i2\", \"arcsec\"],\n",
    "        \"DIST_SSO\":     [\"i2\", \"arcsec\"],\n",
    "        \"MAG_V_SSO\":    [\"f4\", \"\"],\n",
    "        \"FLAGS_SSO\":    [\"i2\", \"\"]\n",
    "        }\n",
    "    output_table = Table()\n",
    "    for key in output_columns.keys():\n",
    "        output_table.add_column(Column(name=key, dtype=output_columns[key][0],\n",
    "                                       unit=output_columns[key][1]))\n",
    "    # Loop over sources\n",
    "    N_det = 0\n",
    "    for index in range(len(indices_separator)-1):\n",
    "        minimal_index = indices_separator[index]+1\n",
    "        maximal_index = indices_separator[index+1]\n",
    "        \n",
    "        if minimal_index == maximal_index:\n",
    "            continue\n",
    "        \n",
    "        # Name of the source in the MPC-formatted input file (transient number)\n",
    "        transient_number = astcheck_file_content[minimal_index:maximal_index][\n",
    "            0].split(':')[0].split()\n",
    "        # Lines corresponding to matches in the astcheck output file\n",
    "        matches = astcheck_file_content[minimal_index:maximal_index][1:]\n",
    "        \n",
    "        if not matches: #Empty list\n",
    "            continue\n",
    "        \n",
    "        N_det += 1\n",
    "        \n",
    "        # If a source is matched to multiple solar system objects, assign the\n",
    "        # matches a flag of 1\n",
    "        if len(matches)>1:\n",
    "            initial_flag = 1\n",
    "        else:\n",
    "            initial_flag = 0\n",
    "        \n",
    "        # Get properties of matches\n",
    "        for i_match in range(len(matches)):\n",
    "            match_properties = re.split(' +', matches[i_match])\n",
    "            match_properties = [x for x in match_properties if len(x) > 0]\n",
    "            identifier = match_properties[0]\n",
    "            try:\n",
    "                int(match_properties[1])\n",
    "                offset_ra, offset_dec, offset, magnitude = match_properties[1:5]\n",
    "            except ValueError:\n",
    "                identifier = \" \".join([identifier, match_properties[1]])\n",
    "                offset_ra, offset_dec, offset, magnitude = match_properties[2:6]\n",
    "            \n",
    "            try:\n",
    "                magnitude = float(magnitude)\n",
    "            except ValueError:\n",
    "                LOG.warning(\"Magnitude '%s' could not be converted to float.\",\n",
    "                            magnitude)\n",
    "                magnitude = None\n",
    "        \n",
    "            # Add match to output table\n",
    "            output_row = [transient_number, str(identifier), float(offset_ra),\n",
    "                          float(offset_dec), float(offset), magnitude, initial_flag]\n",
    "            output_table.add_row(output_row)\n",
    "    \n",
    "    # If a solar system object was matched to multiple transient sources in the\n",
    "    # image, assign it a flag of 2\n",
    "    unique_objects = np.unique(output_table['ID_SSO'])\n",
    "    if len(unique_objects) != len(output_table):\n",
    "        for obj in unique_objects:\n",
    "            obj_indices = np.where(output_table['ID_SSO'] == obj)[0]\n",
    "            if len(obj_indices) > 1:\n",
    "                output_table[\"FLAGS_SSO\"][obj_indices] += 2\n",
    "    \n",
    "    # Set dummy parameter (dummy means that there were no matches found)\n",
    "    dummy = False\n",
    "    if not output_table: #Empty table\n",
    "        dummy = True\n",
    "    \n",
    "    # Create header for SSO catalogue\n",
    "    sso_header = create_sso_header(rundir, software_folder, N_det, N_sso, dummy,\n",
    "                                   True)\n",
    "    \n",
    "    write2fits(output_table, None, [], sso_cat, start_header=sso_header)\n",
    "    \n",
    "    LOG.info(\"Matches saved to SSO catalogue: %s\", sso_cat)\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='create_sso_catalogue')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(sso_cat, mpcformat_file, submission_file,\n",
    "                           mpc_code):\n",
    "    \"\"\"\n",
    "    Make an MPC submission file using the SSO catalogue and the MPC-formatted\n",
    "    file that were created within match2SSO to link the transient detections\n",
    "    from a single catalogue to known solar system objects. The detections\n",
    "    corresponding to matches are grouped in a 'known objects submission file'.\n",
    "    The identifiers used in the submission file are the packed designation of\n",
    "    the matching objects. These are the packed permanent designations if\n",
    "    available. Otherwise, the packed provisional designations are used.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sso_cat: string\n",
    "        Path to and name of the SSO catalogue of which the matches need to be\n",
    "        converted to a submission file.\n",
    "    mpcformat_file: string\n",
    "        Name of the MPC formatted file that was made in match2SSO for the\n",
    "        matching (but does not contain the correct SSO identifiers yet for\n",
    "        submission to the MPC).\n",
    "    submission_file: string\n",
    "        Name of the MPC submission file that will be made from the SSO\n",
    "        catalogue. The submission file should have the extension \".txt\".\n",
    "    mpc_code: string\n",
    "        MPC code corresponding to the telescope.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of create_submission_file')\n",
    "    LOG.info(\"Creating MPC submission file.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    # Compose submission file name\n",
    "    submission_file = submission_file.replace(\n",
    "        \".txt\", \"_{}.txt\".format(Time.now().strftime(\"%Y%m%dT%H%M%S\")))\n",
    "    \n",
    "    # Check if file already exists (will only happen when running this function\n",
    "    # multiple times in close succession, as the production time is used in the\n",
    "    # file name)\n",
    "    if not OVERWRITE_FILES and os.path.exists(submission_file):\n",
    "        LOG.info(\"Submission file already exists and will not be re-made.\")\n",
    "        return\n",
    "    \n",
    "    # Open SSO catalogue\n",
    "    with fits.open(sso_cat) as hdu:\n",
    "        sso_cat_content = Table(hdu[1].data)\n",
    "    \n",
    "    # Check SSO catalogue for matches\n",
    "    if not sso_cat_content:\n",
    "        LOG.info(\"No matches found. Submission file will not be made.\")\n",
    "        return\n",
    "    \n",
    "    # Create submission file\n",
    "    LOG.info(\"Making submission file %s.\", submission_file)\n",
    "    if os.path.exists(submission_file):\n",
    "        LOG.warning(\"MPC submission file %s is overwritten.\", submission_file)\n",
    "    submission_file_content = open(submission_file, 'w')\n",
    "    \n",
    "    # Write header to the submission file\n",
    "    submission_file_content.write(create_submission_header(submission_file,\n",
    "                                                           mpc_code))\n",
    "    \n",
    "    # Open MPC-formatted file as the submission file will be very similar, only\n",
    "    # with MPC designations rather than transient numbers as the first column.\n",
    "    # For a large part, the MPC-formatted file content will therefore be copied\n",
    "    # over.\n",
    "    detections_mpcformat = pd.read_fwf(mpcformat_file, widths=[14, 66],\n",
    "                                       names=[\"char1to14\", \"char15to80\"],\n",
    "                                       dtype={'char1to14':np.int32, \n",
    "                                              'char15to80':str})\n",
    "    \n",
    "    # For each detection that was matched to a known solar system object,\n",
    "    # get the packed designation of the matching object and write the detection\n",
    "    # to the submission file\n",
    "    for match_index in range(len(sso_cat_content[NUMBER_COLUMN])):\n",
    "        designation = sso_cat_content[\"ID_SSO\"][match_index].strip()\n",
    "        \n",
    "        # Start creating the line of the submission file corresponding to the\n",
    "        # detection, by adding the packed designation of the object to the line\n",
    "        if re.match(r\"^[0-9]{4}\\s[A-Z]\", designation) or \"/\" in designation:\n",
    "            # Provisional or survey designation\n",
    "            packed_designation = wrapper_pack_provisional_designation(\n",
    "                designation)\n",
    "            if packed_designation is None:\n",
    "                continue\n",
    "            detection_line = \"    {}  \".format(packed_designation)\n",
    "        \n",
    "        else:\n",
    "            # Asteroid or comet with permanent designation\n",
    "            packed_designation, fragment = pack_permanent_designation(\n",
    "                designation)\n",
    "            if packed_designation is None:\n",
    "                continue\n",
    "            detection_line = \"{}{}  \".format(packed_designation,\n",
    "                                             fragment.rjust(7))\n",
    "        \n",
    "        # Get the detection details from the MPC-formatted file and add to the\n",
    "        # line of the submission file corresponding to the detection\n",
    "        detection_index = np.where(\n",
    "            np.array(detections_mpcformat[\"char1to14\"])\n",
    "            == int(sso_cat_content[NUMBER_COLUMN][match_index]))[0]\n",
    "        if len(detection_index) != 1:\n",
    "            LOG.error(\"%i detections found that correspond to transient number\"\n",
    "                      \" %i. Should be only one.\", len(detection_index),\n",
    "                      int(sso_cat_content[NUMBER_COLUMN][match_index]))\n",
    "            continue\n",
    "        detection_index = detection_index[0]\n",
    "        detection_line = \"\".join([detection_line, detections_mpcformat[\n",
    "            \"char15to80\"][detection_index]])\n",
    "        \n",
    "        # Check line corresponding to detection and write to submission file if\n",
    "        # all is well\n",
    "        if len(detection_line) != 80:\n",
    "            LOG.error(\"Detection not formatted correctly in 80 columns:\\n%s\",\n",
    "                      detection_line)\n",
    "        submission_file_content.write(detection_line+\"\\n\")\n",
    "    \n",
    "    submission_file_content.close()\n",
    "    LOG.info(\"MPC submission file saved to %s\", submission_file)\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='create_submission_file')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_header(submission_file, mpc_code, comment=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function composes the header of the MPC submission file corresponding to a\n",
    "    single transient catalogue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    submission_file: string\n",
    "        Name of the submission file for which the header is composed.\n",
    "    mpc_code: string\n",
    "        MPC code of the telescope with which the observation was made.\n",
    "    comment: string\n",
    "        Comment to be added to the header in the COM line. By default, this is\n",
    "        None, meaning that the COM line is not added to the header.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of create_submission_header')\n",
    "    LOG.info(\"Creating header for submission file.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    firstline = \"COD {}\\n\".format(mpc_code)\n",
    "    \n",
    "    # Special cases for which a phrase needs to be included in the ACK line\n",
    "    # of the header of the MPC submission file:\n",
    "    # neocand = \"NEO CANDIDATE\" #submitting new NEO candidate\n",
    "    # neocp = \"NEOCP\"           #submitting observations of NEOCP objects\n",
    "    # Add ACK line to the header of the MPC submission file.\n",
    "    \n",
    "    ack_line = \"ACK {}\\n\".format(Path(submission_file).stem)\n",
    "    if len(ack_line) > 82:\n",
    "        LOG.error(\"ACK line in submission file %s is too long!\",\n",
    "                  submission_file)\n",
    "    \n",
    "    # Add COM line to the header\n",
    "    com_line = ''\n",
    "    if comment is not None:\n",
    "        if len(comment) > 76:\n",
    "            LOG.warning(\"COM line is too long and therefore not used. \"\n",
    "                        \"Use at most 76 characters!\")\n",
    "        else:\n",
    "            com_line = \"COM {}\\n\".format(comment)\n",
    "    \n",
    "    LOG.info(\"Submission file header complete.\")\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='create_submission_header')\n",
    "    \n",
    "    return \"\".join([firstline, DEFAULT_SUBMISSION_HEADER, ack_line, com_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to make an MPC submission file\n",
    "Convert asteroid designations to their packed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate_number(num):\n",
    "    \n",
    "    \"\"\"\n",
    "    Number packing function needed to pack MPC designations.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_dict = {str(index): letter for index, letter in \n",
    "                enumerate(\"\".join([ascii_uppercase, ascii_lowercase]), start=10)}\n",
    "    \n",
    "    if int(num) > 9:\n",
    "        return num_dict[str(num)]\n",
    "    \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_cycle_number(number_of_cycles):\n",
    "    \n",
    "    \"\"\"Input parameter number_of_cycles is a string of 0-3 digits.\"\"\"\n",
    "    \n",
    "    if not number_of_cycles: #Empty string\n",
    "        return \"00\"\n",
    "    \n",
    "    if int(number_of_cycles) > 99:\n",
    "        return \"{}{}\".format(abbreviate_number(number_of_cycles[0:2]),\n",
    "                             number_of_cycles[2])\n",
    "    \n",
    "    return \"{:0>2}\".format(number_of_cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_provisional_designation_comet(full_designation, pack_year):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function converts the provisional comet designation into its packed form\n",
    "    using the definitions given in\n",
    "    https://www.minorplanetcenter.net/iau/info/PackedDes.html#prov\n",
    "    As described in https://www.minorplanetcenter.net/iau/info/OpticalObs.html,\n",
    "    for comets a character is added in front of the provisional designation (at\n",
    "    column 5), describing the comet type.\n",
    "    The function returns an 8-character long string, spanning columns 5-12 in\n",
    "    the submission file, or None in case of an issue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    full_designation: string\n",
    "        Unpacked provisional designation assigned to the comet by the MPC.\n",
    "    pack_year: dict\n",
    "        Dictionary needed to pack the first two numbers of the year (indicating\n",
    "        the century) into a single letter, according to the MPC standard.\n",
    "    \"\"\"\n",
    "    \n",
    "    comet_type, designation = full_designation.split(\"/\")\n",
    "    \n",
    "    # In case of a comet fragment, the last character of the packed designation\n",
    "    # is the fragment letter. Otherwise, it is zero.\n",
    "    fragment = \"0\"\n",
    "    if '-' in designation:\n",
    "        designation, fragment = designation.split(\"-\")\n",
    "        fragment = fragment.lower()\n",
    "    year, remainder = designation.split(\" \")\n",
    "        \n",
    "    if int(year[:2]) not in pack_year.keys():\n",
    "        LOG.error(\"Provisional designation of comet %s cannot be packed. \"\n",
    "                  \"Skipping it.\", full_designation)\n",
    "        return None\n",
    "    \n",
    "    packed_year = \"{}{}\".format(pack_year[int(year[:2])], year[2:])\n",
    "    \n",
    "    # In case there are two letters after the space in the designation. This\n",
    "    # can be the case if the object was thought to be an asteroid early on.\n",
    "    if remainder[1].isalpha():\n",
    "        \n",
    "        if fragment != \"0\":\n",
    "            # A comet with two letters in its provisional designation after\n",
    "            # the space and a fragment letter cannot be submitted in the old\n",
    "            # submission format. It can in the new ADES format, but we are\n",
    "            # not yet using this. Skip detection.\n",
    "            LOG.error(\"Provisional designation of comet %s cannot be packed.\"\n",
    "                      \"Skipping it.\", full_designation)\n",
    "            return None\n",
    "        \n",
    "        # Although this object is not a fragment, its provisional designation\n",
    "        # does contain a second letter after the space which should be written\n",
    "        # to the same position as the fragment letter.\n",
    "        fragment = remainder[1]\n",
    "        remainder = \"{}{}\".format(remainder[0], remainder[2:])\n",
    "    \n",
    "    # There should be at most three digits after the space-letter combination\n",
    "    # in the provisional designation.\n",
    "    if len(remainder) > 4:\n",
    "        LOG.error(\"Unclear how to pack provisional designation of comet %s. \"\n",
    "                  \"Skipping it.\", full_designation)\n",
    "        return None\n",
    "    \n",
    "    if int(year[:2]) not in pack_year.keys():\n",
    "        LOG.error(\"Data from before 1800 or after 2099 cannot be assigned a \"\n",
    "                  \"packed provisional designation.\")\n",
    "        return None\n",
    "    \n",
    "    packed_designation = (\"{}{}{}{}{}\"\n",
    "                          .format(comet_type, packed_year, remainder[0],\n",
    "                                  pack_cycle_number(remainder[1:]), fragment))\n",
    "    # Final check\n",
    "    if len(packed_designation) != 8:\n",
    "        LOG.error(\"Packed provisional designation is of incorrect length: \"\n",
    "                  \"'%s'\", packed_designation)\n",
    "        return None\n",
    "    \n",
    "    return packed_designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_provisional_designation_asteroid(full_designation, pack_year):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function converts the provisional asteroid designation into its packed form\n",
    "    using the definitions given in\n",
    "    https://www.minorplanetcenter.net/iau/info/PackedDes.html#prov\n",
    "    We add a space in front so that the returned string is 8 characters long,\n",
    "    spanning columns 5-12 in the submission file. The function returns None in\n",
    "    case of an issue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    full_designation: string\n",
    "        Unpacked provisional designation assigned to the asteroid by the MPC.\n",
    "    pack_year: dict\n",
    "        Dictionary needed to pack the first two numbers of the year (indicating\n",
    "        the century) into a single letter, according to the MPC standard.\n",
    "    \"\"\"\n",
    "    \n",
    "    if int(full_designation[:2]) not in pack_year.keys():\n",
    "        LOG.error(\"Provisional designation of asteroid %s cannot be packed. \"\n",
    "                  \"Skipping it.\", full_designation)\n",
    "        return None\n",
    "    \n",
    "    packed_year = \"{}{}\".format(pack_year[int(full_designation[:2])],\n",
    "                                full_designation[2:4])\n",
    "    packed_designation = (\" {}{}{}{}\"\n",
    "                          .format(packed_year, full_designation[5],\n",
    "                                  pack_cycle_number(full_designation[7:]),\n",
    "                                  full_designation[6]))\n",
    "    # Final check\n",
    "    if len(packed_designation) != 8:\n",
    "        LOG.error(\"Packed provisional designation is of incorrect length: \"\n",
    "                  \"'%s'\", packed_designation)\n",
    "        return None\n",
    "    \n",
    "    return packed_designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_pack_provisional_designation(full_designation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Wrapper function for packing provisional minor planet designations into\n",
    "    their packed forms. The function first checks whether [full_designation] is\n",
    "    a survey designation. If so, it is packed using the definitions in\n",
    "    https://www.minorplanetcenter.net/iau/info/PackedDes.html#prov\n",
    "    \n",
    "    If the designation is not a survey designation, the function determines\n",
    "    whether it belongs to a comet or an asteroid. For comets, we call the\n",
    "    function pack_provisional_designation_comet. For asteroids, we call\n",
    "    pack_provisional_designation_asteroid.\n",
    "    \n",
    "    The function returns an 8-character long string, spanning columns 5-12 in\n",
    "    the submission file, or None in case of an issue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    full_designation: string\n",
    "        Unpacked provisional designation assigned to the object by the MPC.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove space before or after designation\n",
    "    full_designation = full_designation.strip()\n",
    "    \n",
    "    # Their are four special survey designation forms (for surveys that were\n",
    "    # undertaken between 1960 and 1977)that should be packed differently\n",
    "    survey_strings = [\"P-L\", \"T-1\", \"T-2\", \"T-3\"]\n",
    "    for survey_string in survey_strings:\n",
    "        if (re.match(r\"^[0-9]{4}\\s[A-Z]\", full_designation)\n",
    "                and full_designation.endswith(survey_string)):\n",
    "            packed_designation = \"{}S{}\".format(survey_string.replace(\"-\", \"\"),\n",
    "                                                full_designation[:4])\n",
    "            return packed_designation\n",
    "    \n",
    "    pack_year = {18: \"I\", 19: \"J\", 20: \"K\"}\n",
    "    \n",
    "    # For a comet\n",
    "    if \"/\" in full_designation:\n",
    "        packed_designation = pack_provisional_designation_comet(\n",
    "            full_designation, pack_year)\n",
    "        return packed_designation\n",
    "    \n",
    "    # For an asteroid\n",
    "    packed_designation = pack_provisional_designation_asteroid(\n",
    "        full_designation, pack_year)\n",
    "    \n",
    "    return packed_designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_permanent_designation(full_designation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function converts the permanent minor planet designation into its packed\n",
    "    form (5 characters), using the definitions given in\n",
    "    https://www.minorplanetcenter.net/iau/info/PackedDes.html#perm\n",
    "    Return the packed designation and - if applicable - the letter\n",
    "    corresponding to the comet fragment. If the object is not a comet fragment,\n",
    "    an empty string will be returned for the fragment letter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    full_designation: string\n",
    "        Unpacked permanent designation assigned to the object by the MPC.\n",
    "    \"\"\"\n",
    "    fragment = ''\n",
    "    \n",
    "    if not full_designation.isdigit():\n",
    "        # Object is a comet\n",
    "        if len(full_designation.split(\"-\")) == 2:\n",
    "            designation, fragment = full_designation.split(\"-\")\n",
    "        else:\n",
    "            designation = full_designation\n",
    "            fragment = ''\n",
    "        packed_designation = designation.zfill(5)\n",
    "        fragment = fragment.lower()\n",
    "    \n",
    "    elif int(full_designation) < 99999:\n",
    "        packed_designation = \"{:0>5}\".format(int(full_designation))\n",
    "    \n",
    "    elif int(full_designation) < 620000:\n",
    "        quotient = int(full_designation)//10000\n",
    "        packed_designation = \"{}{:0>4}\".format(abbreviate_number(quotient),\n",
    "                                               int(full_designation)%10000)\n",
    "    else:\n",
    "        remainder = int(full_designation) - 620000\n",
    "        quotient3 = remainder//62**3\n",
    "        remainder -= quotient3*62**3\n",
    "        quotient2 = remainder//62**2\n",
    "        remainder -= quotient2*62**2\n",
    "        quotient1 = remainder//62\n",
    "        remainder -= quotient1*62\n",
    "        packed_designation = \"~{}{}{}{}\".format(abbreviate_number(quotient3),\n",
    "                                                abbreviate_number(quotient2),\n",
    "                                                abbreviate_number(quotient1),\n",
    "                                                abbreviate_number(remainder))\n",
    "    # Final check\n",
    "    if len(packed_designation) != 5:\n",
    "        LOG.error(\"Packed permanent designation is of incorrect length: '%s'\",\n",
    "                  packed_designation)\n",
    "        return None, ''\n",
    "    \n",
    "    return packed_designation, fragment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input_parameters(mode, cat2process, date2process, list2process):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if the correct (combination of) input parameters was/were defined for\n",
    "    run_match2SSO. If so, this function returns True. Otherwise, False is\n",
    "    returned.\n",
    "    \"\"\"\n",
    "    all_good = True\n",
    "    \n",
    "    # General checks on the input parameters\n",
    "    param_list = [date2process, cat2process, list2process]\n",
    "    param_num_none = sum([isinstance(par, type(None)) for par in param_list])\n",
    "    \n",
    "    if  param_num_none < len(param_list)-1:\n",
    "        print(\"CRITICAL: either specify --date, --catalog OR --catlist. A \"\n",
    "              \"combination is not allowed.\")\n",
    "        all_good = False\n",
    "    \n",
    "    if mode not in [\"day\", \"night\", \"historic\"]:\n",
    "        print(\"CRITICAL: unknown mode.\")\n",
    "        all_good = False\n",
    "    \n",
    "    # Checks per mode\n",
    "    if mode == \"day\":\n",
    "        if cat2process is not None or list2process is not None:\n",
    "            print(\"CRITICAL: the day mode cannot be combined with the \"\n",
    "                  +\"--catalog or --catlist arguments.\")\n",
    "            all_good = False\n",
    "    \n",
    "    elif mode == \"night\":\n",
    "        if cat2process is None:\n",
    "            print(\"CRITICAL: --catalog needs to be specified when running \"\n",
    "                  \"match2SSO in night mode.\")\n",
    "            all_good = False\n",
    "        if date2process is not None or list2process is not None:\n",
    "            print(\"CRITICAL: the night mode cannot be combined with the \"\n",
    "                  \"--date or --catlist arguments.\")\n",
    "            all_good = False\n",
    "    \n",
    "    elif mode == \"historic\" and param_num_none == len(param_list):\n",
    "        print(\"CRITICAL: --date, --catalog and --catlist are all None. Nothing\"\n",
    "              \" to process.\")\n",
    "        all_good = False\n",
    "    \n",
    "    # Check on the existence of the specified input\n",
    "    if cat2process is not None and not os.path.exists(cat2process):\n",
    "        print(\"CRITICAL: the specified catalog does not exist.\")\n",
    "        all_good = False\n",
    "    \n",
    "    if list2process is not None and not os.path.exists(list2process):\n",
    "        print(\"CRITICAL: the specified catalog list does not exist.\")\n",
    "        all_good = False\n",
    "    \n",
    "    return all_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder_name(directory_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function checks if directory name ends with a slash. If not, it is added.\n",
    "    \"\"\"\n",
    "    \n",
    "    if directory_name[-1] != \"/\":\n",
    "        directory_name = \"\".join([directory_name, \"/\"])\n",
    "    \n",
    "    return directory_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_check_folders(tel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function loads the folders specified in the settings file and checks\n",
    "    whether they end with a slash. In addition, checks on the existence of the\n",
    "    folders are performed. A list of the folder names is returned. The returned\n",
    "    list is empty if there was an issue.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tel: string\n",
    "        Telescope abbreviation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load folders\n",
    "    input_folder = get_par(settingsFile.inputFolder, tel)\n",
    "    software_folder = get_par(settingsFile.softwareFolder, tel)\n",
    "    database_folder = get_par(settingsFile.databaseFolder, tel)\n",
    "    log_folder = get_par(settingsFile.logFolder, tel)\n",
    "    submission_folder = get_par(settingsFile.submissionFolder, tel)\n",
    "    \n",
    "    # Check if folder names end with a slash\n",
    "    input_folder = check_folder_name(input_folder)\n",
    "    software_folder = check_folder_name(software_folder)\n",
    "    database_folder = check_folder_name(database_folder)\n",
    "    log_folder = check_folder_name(log_folder)\n",
    "    submission_folder = check_folder_name(submission_folder)\n",
    "    \n",
    "    # Check if critical folders exists. If not, return an empty list.\n",
    "    if not os.path.isdir(input_folder):\n",
    "        print(\"CRITICAL: input folder given in settings file doesn't exist\")\n",
    "        return []\n",
    "    \n",
    "    if not os.path.isdir(software_folder):\n",
    "        print(\"CRITICAL: software folder given in settings file doesn't exist\")\n",
    "        return []\n",
    "    \n",
    "    # Create the other folders if they don't exist, except for the log folder\n",
    "    # as that is taken care of in the setup_logfile function.\n",
    "    if not os.path.isdir(database_folder):\n",
    "        os.makedirs(database_folder)\n",
    "    if not os.path.isdir(submission_folder):\n",
    "        os.makedirs(submission_folder)\n",
    "    \n",
    "    folders = [input_folder, software_folder, database_folder, log_folder,\n",
    "               submission_folder]\n",
    "    \n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_settings():\n",
    "    \n",
    "    \"\"\"Function checks the parameters in the settings file for validity.\"\"\"\n",
    "    \n",
    "    # Check that astcheck parameters are numbers\n",
    "    if not isinstance(settingsFile.matchingRadius, (float, int)):\n",
    "        print(\"CRITICAL: incorrectly specified matching radius in settings \"\n",
    "              +\"file. Must be float or integer.\")\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(settingsFile.limitingMagnitude, (float, int)):\n",
    "        print(\"CRITICAL: incorrectly specified limiting mag. in settings \"\n",
    "              \"file.\")\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(settingsFile.maximalNumberOfAsteroids, int):\n",
    "        print(\"CRITICAL: incorrectly specified max. number of asteroids in \"\n",
    "              +\"settings file.\")\n",
    "        return False\n",
    "    \n",
    "    if (not isinstance(settingsFile.maxUncertainty, int) and\n",
    "            settingsFile.maxUncertainty is not None):\n",
    "        print(\"CRITICAL: incorrectly specified max. uncertainty in settings \"\n",
    "              +\"file. Must be 0-9 or None.\")\n",
    "        return False\n",
    "    \n",
    "    # Check if JPL ephemeris file exists\n",
    "    if not os.path.exists(settingsFile.JPL_ephemerisFile):\n",
    "        print(\"CRITICAL: JPL ephemeris file specified in settings file doesn't\"\n",
    "              \" exist.\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logfile(logname, log_folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function creates log file and configures the log handler.\n",
    "    \"\"\"\n",
    "    \n",
    "    if logname is None:\n",
    "        return\n",
    "    \n",
    "    log_dir, log_filename = os.path.split(logname)\n",
    "    \n",
    "    # If no folder is specified, use the log folder from the settings file\n",
    "    if not log_dir:\n",
    "        log_dir = log_folder\n",
    "    \n",
    "    # Create folder to store log in, if it does not yet exist\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    # Configure log handling\n",
    "    log_file = \"{}/{}\" .format(log_dir, log_filename)\n",
    "    if os.path.exists(log_file):\n",
    "        file_path_and_name, extension = os.path.splitext(log_file)\n",
    "        log_file = \"{}_{}{}\".format(file_path_and_name,\n",
    "                                    Time.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "                                    extension)\n",
    "        print(\"Log file already exists. Creating a new log named {}\"\n",
    "              .format(log_file))\n",
    "    \n",
    "    file_handler = logging.FileHandler(log_file, 'a')\n",
    "    file_handler.setFormatter(LOG_FORMATTER)\n",
    "    file_handler.setLevel('INFO')\n",
    "    LOG.addHandler(file_handler)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function\n",
    "def get_par(par, tel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to check if [par] is a dictionary with one of the keys being [tel]\n",
    "    or the alphabetic part of [tel] (e.g. 'BG'), and if so, return the\n",
    "    corresponding value. Otherwise just return the parameter value.\n",
    "    \"\"\"\n",
    "    \n",
    "    par_val = par\n",
    "    if isinstance(par, dict):\n",
    "        if tel in par:\n",
    "            par_val = par[tel]\n",
    "        else:\n",
    "            # cut off digits from [tel]\n",
    "            tel_base = ''.join([char for char in tel if char.isalpha()])\n",
    "            if tel_base in par:\n",
    "                par_val = par[tel_base]\n",
    "    \n",
    "    return par_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function\n",
    "def mem_use(label=''):\n",
    "    \n",
    "    \"\"\"Function keeps track of the memory usage.\"\"\"\n",
    "    \n",
    "    # ru_maxrss is in units of kilobytes on Linux; however, this seems\n",
    "    # to be OS dependent as on mac os it is in units of bytes; see\n",
    "    # manpages of \"getrusage\"\n",
    "    if sys.platform == 'darwin':\n",
    "        norm = 1024**3\n",
    "    else:\n",
    "        norm = 1024**2\n",
    "    \n",
    "    mem_max = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/norm\n",
    "    mem_now = psutil.Process().memory_info().rss / 1024**3\n",
    "    mem_virt = psutil.Process().memory_info().vms / 1024**3\n",
    "    \n",
    "    LOG.info('memory use [GB]: rss=%.3f, maxrss=%.3f, vms=%.3f in %s', mem_now,\n",
    "             mem_max, mem_virt, label)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOGY function\n",
    "def log_timing_memory(t_in, label=''):\n",
    "    \n",
    "    \"\"\"Function to report the time spent in a function.\"\"\"\n",
    "    \n",
    "    LOG.info('wall-time spent in %s: %.3f s', label, time.time()-t_in)\n",
    "    mem_use(label=label)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transient_filenames(input_folder, minimal_date, maximal_date, tel,\n",
    "                            exclude_flagged=False):\n",
    "    \"\"\"\n",
    "    Function returns a list with the transient file names that were taken\n",
    "    between the minimal and maximal specified dates. The input dates should be\n",
    "    Time objects. If exclude_flagged is True, the dummy transient catalogues\n",
    "    (which are red-flagged) are excluded.\n",
    "    \n",
    "    Function assumes a directory and filename structure and hence might not be\n",
    "    applicable to other telescopes than MeerLICHT & BlackGEM.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_folder: string\n",
    "        Folder which contains the yyyy/mm/dd/ folders in which the transient\n",
    "        catalogues are stored.\n",
    "    minimal_date: datetime object, incl time zone\n",
    "        Minimal observation date of the time block for which the observations\n",
    "        are selected.\n",
    "    maximal_date: datetime object, incl time zone\n",
    "        Maximal observation date of the time block for which the observations\n",
    "        are selected.\n",
    "    tel: string\n",
    "        Telescope abbreviation.\n",
    "    exclude_flagged: boolean\n",
    "        Boolean indicating whether red-flagged (dummy) catalogues should be\n",
    "        excluded or not.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of get_transient_filenames')\n",
    "    LOG.info(\"Selecting transient catalogues between %s and %s.\",\n",
    "             minimal_date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "             maximal_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    if TIME_FUNCTIONS:\n",
    "        t_func = time.time()\n",
    "    \n",
    "    # Convert to local time\n",
    "    local_timezone = timezone(get_par(settingsFile.timeZoneTelescope, tel))\n",
    "    minimal_date = minimal_date.astimezone(local_timezone)\n",
    "    maximal_date = maximal_date.astimezone(local_timezone)\n",
    "    \n",
    "    # Select the transient files by observation date\n",
    "    year, month, day = \"*\", \"*\", \"*\"\n",
    "    if minimal_date.year == maximal_date.year:\n",
    "        year = \"%d\"%(minimal_date.year)\n",
    "        if minimal_date.month == maximal_date.month:\n",
    "            month = \"{:0>2}\".format(minimal_date.month)\n",
    "            if minimal_date.day == maximal_date.day:\n",
    "                day = \"{:0>2}\".format(maximal_date.day)\n",
    "    \n",
    "    transient_files = glob.glob(os.path.join(input_folder,\n",
    "                                            \"%s/%s/%s/*_trans_light.fits\"\n",
    "                                            %(year, month, day)))\n",
    "    if not transient_files:\n",
    "        return []\n",
    "    \n",
    "    files2process = []\n",
    "    for transient_cat in transient_files:\n",
    "        # Parse date encoded in filename and compare with our limits\n",
    "        # (e.g. ML1_20200517_034221_red_trans_light.fits)\n",
    "        splitted_filename = os.path.basename(transient_cat).split(\"_\")\n",
    "        date_obs = splitted_filename[1]\n",
    "        time_obs = splitted_filename[2]\n",
    "        observation_time = Time.strptime(date_obs+time_obs, \"%Y%m%d%H%M%S\").mjd\n",
    "        if (observation_time >= Time(minimal_date).mjd and\n",
    "                observation_time <= Time(maximal_date).mjd):\n",
    "            with fits.open(transient_cat) as hdu:\n",
    "                header = hdu[1].header\n",
    "            \n",
    "            if not exclude_flagged:\n",
    "                files2process.append(transient_cat)\n",
    "            else:\n",
    "                LOG.info(\"Excluding red-flagged (dummy) catalogues.\")\n",
    "                \n",
    "                if DUMMY_KEYWORD not in header.keys():\n",
    "                    LOG.critical(\"%s not in the header!\", DUMMY_KEYWORD)\n",
    "                    return []\n",
    "                \n",
    "                if not header[DUMMY_KEYWORD]:\n",
    "                    files2process.append(transient_cat)\n",
    "    \n",
    "    LOG.info(\"%i transient catalogues have been selected.\",\n",
    "             len(files2process))\n",
    "    if TIME_FUNCTIONS:\n",
    "        log_timing_memory(t_func, label='get_transient_filenames')\n",
    "    return files2process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_night_start_from_date(cat_name, tel, noon_type=\"local\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the noon corresponding to the start of the\n",
    "    observation night, as a datetime object. This is either the local noon or\n",
    "    the noon in UTC, as specified. The noon is deduced from the catalogue\n",
    "    header.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cat_name: string\n",
    "        Name of the catalogue corresponding to an observation that took place\n",
    "        on the observation night for which the noon that signifies the start of\n",
    "        the night must be determined.\n",
    "    tel: string\n",
    "        Telescope abbreviation.\n",
    "    noon_type: string\n",
    "        Must be either \"local\" or \"utc\". If \"utc\", this function will return\n",
    "        the noon corresponding to the start of the night in UTC. This can be\n",
    "        different from the local noon.\n",
    "    \"\"\"\n",
    "    noon_type = noon_type.lower()\n",
    "    \n",
    "    # Get observation time from catalogue header and define as being in UTC\n",
    "    with fits.open(cat_name) as hdu:\n",
    "        hdr = hdu[1].header\n",
    "    \n",
    "    observation_time = pytz.utc.localize(Time(hdr[DATE_KEYWORD],\n",
    "                                              format='isot').datetime)\n",
    "    observation_date = str(observation_time.date())\n",
    "    \n",
    "    if noon_type == \"local\":\n",
    "        local_timezone = timezone(get_par(settingsFile.timeZoneTelescope, tel))\n",
    "        \n",
    "        # Get local noon corresponding to the start of the observing night\n",
    "        local_noon = local_timezone.localize(datetime.strptime(\" \".join([\n",
    "            observation_date, \"120000\"]), \"%Y-%m-%d %H%M%S\"))\n",
    "        # Get date of observing night\n",
    "        if observation_time < local_noon:\n",
    "            date = (observation_time - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "        else:\n",
    "            date = observation_time.strftime(\"%Y%m%d\")\n",
    "        \n",
    "        # Make local noon variable\n",
    "        night_start = local_timezone.localize(datetime.strptime(\" \".join([\n",
    "            date, \"120000\"]), \"%Y%m%d %H%M%S\"))\n",
    "    else:\n",
    "        if noon_type != \"utc\":\n",
    "            LOG.error(\"Noon type not understood. Assuming noon in utc.\")\n",
    "        \n",
    "        night_start = pytz.utc.localize(datetime.strptime(\" \".join([\n",
    "            observation_date, \"120000\"]), \"%Y-%m-%d %H%M%S\"))\n",
    "        if int(observation_time.hour) < 12.:\n",
    "            night_start -= timedelta(days=1)\n",
    "    \n",
    "    return night_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input_catalogue(cat_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if the input catalogue exists and if it is a dummy (red-flagged)\n",
    "    catalogue or not. If a light version of the catalogue is available, use\n",
    "    that version. This function returns a boolean for \"does the catalogue \n",
    "    exist?\", a boolean for \"is the catalogue a dummy?\" and the catalogue name\n",
    "    is returned, as the light version might have been selected instead of the\n",
    "    transient catalogue that includes the thumbnails.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check whether the (light) catalogue exists and ensure the use of the\n",
    "    # light version of the catalogue if it is available (better in terms of\n",
    "    # memory usage & processing speed)\n",
    "    if \"_light\" not in cat_name:\n",
    "        light_cat = cat_name.replace(\".fits\", \"_light.fits\")\n",
    "        if os.path.exists(light_cat):\n",
    "            cat_name = light_cat\n",
    "    \n",
    "    if not os.path.exists(cat_name):\n",
    "        LOG.critical(\"The specified catalog does not exist:\\n%s\", cat_name)\n",
    "        return False, None, cat_name\n",
    "    \n",
    "    # Check quality control flag of the catalogue\n",
    "    with fits.open(cat_name) as hdu:\n",
    "        header = hdu[1].header\n",
    "    \n",
    "    if DUMMY_KEYWORD not in header.keys():\n",
    "        LOG.critical(\"%s not in the header of %s!\", DUMMY_KEYWORD, cat_name)\n",
    "        return False, None, cat_name\n",
    "    \n",
    "    if header[DUMMY_KEYWORD]:\n",
    "        LOG.info(\"%s is a dummy catalogue.\", cat_name)\n",
    "        return True, True, cat_name\n",
    "    \n",
    "    return True, False, cat_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_database_products(rundir):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function checks if the database products that astcheck needs in order\n",
    "    to process transient catalogues are located in the run directory. These are\n",
    "    the known objects catalogue, the symbolic link to the asteroid catalogue\n",
    "    (needed for reading out the asteroid database version) and ELEMENTS.COMET,\n",
    "    which is either an empty comet database (if comets should not be included\n",
    "    in the matching) or a symbolic link to the comet database used (again \n",
    "    needed to read out the version). The function also indirectly checks for\n",
    "    the existence of the run directory. A boolean is returned: True if all is\n",
    "    well and False if something is missing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rundir: string\n",
    "        Directory in which astcheck will be run.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for known objects catalogue\n",
    "    if not os.path.exists(\"{}mpcorb.sof\".format(rundir)):\n",
    "        LOG.critical(\"The known objects catalogue (SOF format) could not be \"\n",
    "                     \"found.\")\n",
    "        return False\n",
    "    \n",
    "    # Check for symbolic links pointing to the used version of the SSO\n",
    "    # databases\n",
    "    if not os.path.exists(\"{}MPCORB.DAT\".format(rundir)):\n",
    "        LOG.critical(\"MPCORB.DAT could not be found\")\n",
    "        return False\n",
    "    if not os.path.exists(\"{}ELEMENTS.COMET\".format(rundir)):\n",
    "        LOG.critical(\"ELEMENTS.COMET could not be found\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_astcheck_header_and_footer(astcheck_file_content):\n",
    "    \n",
    "    \"\"\"\n",
    "    Before the -h switch was implemented in astcheck, the header and footer\n",
    "    needed to be removed manually. Check if astcheck has done this already, or\n",
    "    if manual removal is required. Return the content of the astcheck file\n",
    "    excluding the header and footer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The footer is variable in terms of the number of lines it spans, but it\n",
    "    # always starts with the footer_string as defined below and can hence be\n",
    "    # recognized by this string.\n",
    "    footer_string = \"The apparent motion and arc length\"\n",
    "    header_size = 5 # Number of header lines\n",
    "    footer_index = [index for index in range(len(astcheck_file_content)) if \\\n",
    "                   footer_string in astcheck_file_content[index]]\n",
    "    if footer_index:\n",
    "        return astcheck_file_content[header_size:footer_index[0]]\n",
    "    \n",
    "    else:\n",
    "        return astcheck_file_content[1:] # Just remove first empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write2fits(data, header, header_keys, output_file, start_header=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function formats the output data, composes the header and combines the two\n",
    "    into a hdu table. The table is then written to a fits file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : table data\n",
    "        Table data which is to be used as the data for the output fits table.\n",
    "    header: header\n",
    "        Header from which certain keywords are copied to the header of the\n",
    "        output catalogue.\n",
    "    header_keys: list of strings\n",
    "        Contains names of header keywords from the header mentioned above, that\n",
    "        will be included in the output catalogue header.\n",
    "    output_file: string\n",
    "        File name (including path) under which the output binary fits table\n",
    "        will be stored.\n",
    "    start_header: header\n",
    "        Header which will be included in the header of the output catalogue.\n",
    "        start_header can be None.\n",
    "    \"\"\"\n",
    "    mem_use(label='at start of write2fits')\n",
    "    \n",
    "    # Format fits table data\n",
    "    columns = []\n",
    "    for column_name in data.columns:\n",
    "        \n",
    "        column_format = data[column_name].dtype\n",
    "        \n",
    "        # Converting bytestring format to fits format does not work properly\n",
    "        # for strings, as the length is not taken into account properly.\n",
    "        # Manually correct this.\n",
    "        if 'S' in column_format.str:\n",
    "            string_length = column_format.str.split(\"S\")[-1]\n",
    "            column_format = \"{}A\".format(string_length)\n",
    "        \n",
    "        column_unit = str(data[column_name].unit)\n",
    "        if column_unit == \"None\":\n",
    "            column_unit = \"\"\n",
    "        \n",
    "        column = fits.Column(name=column_name, format=column_format,\n",
    "                             unit=column_unit, array=data[column_name])\n",
    "        columns.append(column)\n",
    "    \n",
    "    # Compose fits table header\n",
    "    if start_header is not None:\n",
    "        header = start_header\n",
    "    else:\n",
    "        header = fits.Header()\n",
    "    \n",
    "    for key in header_keys:\n",
    "        header[key] = (header[key], header.comments[key])\n",
    "   \n",
    "    # Combine formatted fits columns and header into output binary fits table\n",
    "    fitstable = fits.BinTableHDU.from_columns(columns, header=header)\n",
    "    fitstable.writeto(output_file, overwrite=True)\n",
    "    \n",
    "    mem_use(label='at end of write2fits')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on BlackBOX function clean_tmp\n",
    "def remove_tmp_folder(tmp_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that removes the specified folder and its contents.\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.isdir(tmp_path):\n",
    "        shutil.rmtree(tmp_path)\n",
    "        LOG.info('Removing temporary folder: %s', tmp_path)\n",
    "    else:\n",
    "        LOG.warning('tmp folder %s does not exist', tmp_path)\n",
    "    \n",
    "    mem_use(label='after removing temporary folder')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run match2SSO using command line parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    PARSER = argparse.ArgumentParser(description=\"User parameters\")\n",
    "    \n",
    "    PARSER.add_argument(\"--telescope\", type=str, default=\"ML1\",\n",
    "                        help=\"Telescope name (ML1, BG2, BG3 or BG4); \"\n",
    "                        \"default='ML1'\")\n",
    "    \n",
    "    PARSER.add_argument(\"--mode\", type=str, default=\"historic\",\n",
    "                        help=\"Day, night or historic mode of pipeline; \"\n",
    "                        \"default='historic'\")\n",
    "    \n",
    "    PARSER.add_argument(\"--catalog\", type=str, default=None,\n",
    "                        help=\"Only process this particular transient catalog. \"\n",
    "                        \"Requires full path and requires mode to be 'historic'\"\n",
    "                        \" or 'night'; default=None\")\n",
    "    \n",
    "    PARSER.add_argument(\"--date\", type=str, default=None,\n",
    "                        help=\"Date to process (yyyymmdd, yyyy-mm-dd, yyyy/mm/d\"\n",
    "                        \"d or yyyy.mm.dd). Mode is required to be 'historic'; \"\n",
    "                        \"default=None\")\n",
    "    \n",
    "    PARSER.add_argument(\"--catlist\", type=str, default=None,\n",
    "                        help=\"Process all transient catalogs in the input list\"\n",
    "                        \". List entries require full path. Mode  must be \"\n",
    "                        \"'historic'; default=None\")\n",
    "    \n",
    "    PARSER.add_argument(\"--logname\", type=str, default=None,\n",
    "                        help=\"Name of log file to save. Requires full path; \"\n",
    "                        \"default of None will not create a log file\")\n",
    "    \n",
    "    ARGS = PARSER.parse_args()\n",
    "    run_match2SSO(tel=ARGS.telescope, mode=ARGS.mode, cat2process=ARGS.catalog,\n",
    "                  date2process=ARGS.date, list2process=ARGS.catlist,\n",
    "                  logname=ARGS.logname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
